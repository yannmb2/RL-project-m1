{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c8ea223",
   "metadata": {},
   "source": [
    "## ü§ñ Solution: A PPO-Powered Trading Agent\n",
    "\n",
    "This notebook implements a complete Deep Reinforcement Learning (DRL) pipeline to train an autonomous trading agent. The goal is to develop a policy that outperforms the market by making intelligent decisions on when to go long, short, or stay neutral.\n",
    "\n",
    "The solution is structured as follows:\n",
    "1.  **Environment Setup & Data Preparation:** We install libraries, load the data, engineer 12 distinct market features, and split the data into chronological `train`, `validation`, and `test` sets.\n",
    "2.  **Baseline Agent:** We establish a \"Random Agent\" baseline to measure our agent's effectiveness.\n",
    "3.  **PPO Agent Implementation:** We build our agent from scratch using Proximal Policy Optimization (PPO) with an Actor-Critic network.\n",
    "4.  **Hyperparameter Tuning:** We use `Optuna` to automatically find the best set of hyperparameters (learning rate, network size, etc.) by evaluating models on the `validation` set.\n",
    "5.  **Final Model Training:** We train the agent with the *best* hyperparameters on the *full* training dataset (`df_train_full`).\n",
    "6.  **Final Evaluation & Visualization:** We load the best saved model and run it on the *unseen* `test` set (`df_eval`) to get our final project result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bd874",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd6c17",
   "metadata": {},
   "source": [
    "## 1. üõ†Ô∏è Environment Setup & Dependencies\n",
    "\n",
    "Before we can build our agent, we must set up the environment. This involves two steps:\n",
    "\n",
    "* **Installing Packages:** We use `pip` to install the core libraries:\n",
    "    * `gym-trading-env`: The trading simulation environment.\n",
    "    * `torch`: The deep learning framework for our agent's neural network.\n",
    "    * `optuna`: For hyperparameter optimization.\n",
    "* **Importing Libraries:** We import all the necessary tools for data manipulation (`pandas`, `numpy`), environment creation (`gym`), and agent building (`torch.nn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bf91412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.downloader import download\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05469c",
   "metadata": {},
   "source": [
    "## 2. üìà Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e2e93e",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "28cdc5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Setup Folders ---\n",
    "data_folder = Path(\"data/\")\n",
    "data_folder.mkdir(parents=True, exist_ok=True)\n",
    "eval_folder = Path(\"eval/\")\n",
    "eval_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\"\"\"\n",
    "download(exchange_names = [\"binance\"],\n",
    "    symbols= [\"BTC/USDT\"],\n",
    "    timeframe= \"1h\",\n",
    "    dir = data_folder,\n",
    "    since= datetime.datetime(year= 2020, month=10, day=1),\n",
    ")\"\"\"\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_pickle(data_folder / \"binance-BTCUSDT-1h.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bb6454",
   "metadata": {},
   "source": [
    "# Exploration of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f325cda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>date_close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_open</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-09-30 23:00:00</th>\n",
       "      <td>10745.85</td>\n",
       "      <td>10785.00</td>\n",
       "      <td>10735.51</td>\n",
       "      <td>10776.59</td>\n",
       "      <td>1235.545956</td>\n",
       "      <td>2020-10-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 00:00:00</th>\n",
       "      <td>10776.59</td>\n",
       "      <td>10826.19</td>\n",
       "      <td>10776.59</td>\n",
       "      <td>10788.06</td>\n",
       "      <td>2128.759531</td>\n",
       "      <td>2020-10-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 01:00:00</th>\n",
       "      <td>10788.30</td>\n",
       "      <td>10849.97</td>\n",
       "      <td>10786.74</td>\n",
       "      <td>10838.88</td>\n",
       "      <td>1604.129560</td>\n",
       "      <td>2020-10-01 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 02:00:00</th>\n",
       "      <td>10838.89</td>\n",
       "      <td>10857.47</td>\n",
       "      <td>10807.39</td>\n",
       "      <td>10817.14</td>\n",
       "      <td>1268.291734</td>\n",
       "      <td>2020-10-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 03:00:00</th>\n",
       "      <td>10817.14</td>\n",
       "      <td>10824.22</td>\n",
       "      <td>10789.01</td>\n",
       "      <td>10798.18</td>\n",
       "      <td>939.599057</td>\n",
       "      <td>2020-10-01 04:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close       volume  \\\n",
       "date_open                                                                  \n",
       "2020-09-30 23:00:00  10745.85  10785.00  10735.51  10776.59  1235.545956   \n",
       "2020-10-01 00:00:00  10776.59  10826.19  10776.59  10788.06  2128.759531   \n",
       "2020-10-01 01:00:00  10788.30  10849.97  10786.74  10838.88  1604.129560   \n",
       "2020-10-01 02:00:00  10838.89  10857.47  10807.39  10817.14  1268.291734   \n",
       "2020-10-01 03:00:00  10817.14  10824.22  10789.01  10798.18   939.599057   \n",
       "\n",
       "                             date_close  \n",
       "date_open                                \n",
       "2020-09-30 23:00:00 2020-10-01 00:00:00  \n",
       "2020-10-01 00:00:00 2020-10-01 01:00:00  \n",
       "2020-10-01 01:00:00 2020-10-01 02:00:00  \n",
       "2020-10-01 02:00:00 2020-10-01 03:00:00  \n",
       "2020-10-01 03:00:00 2020-10-01 04:00:00  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb823b62",
   "metadata": {},
   "source": [
    "The agent needs to asses at each begining of the hours if we should buy, sell or whatever. So It should not get the close value of that hour as we do not know it. This is a \"Look-Ahead Bias\". When We think about it, many features have a look-Ahead bias (`high`,`low`)\n",
    "\n",
    "PS : We can see that the `close` of a previous hour is the `open` of the next hour. As such, to prevent the agent to get the open value of the next hour we will shift the value to get `previous_close`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f741a6",
   "metadata": {},
   "source": [
    "# Creating features for the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "691312cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Initial Data Shift and Cleanup ---\n",
    "# Shift 'close' one step back to create 'prev_close'. This is the price\n",
    "# available at the moment the new candle OPENS (i.e., Close at t-1).\n",
    "df[\"prev_close\"] = df[\"close\"].shift(1)\n",
    "\n",
    "# --- 2. Calculate Raw Technical Indicators on LAGGED DATA ---\n",
    "# All indicators MUST use 'prev_close' for their core calculations.\n",
    "\n",
    "# Sharpe Ratio\n",
    "ANNUALIZATION_FACTOR = 24 * 365\n",
    "ROLLING_WINDOW_SR = 7 * 24 \n",
    "RISK_FREE_RATE_ANNUAL = 0.04\n",
    "RISK_FREE_RATE_HOURLY = (1 + RISK_FREE_RATE_ANNUAL)**(1/ANNUALIZATION_FACTOR) - 1\n",
    "\n",
    "# Base returns calculation uses 'prev_close' (i.e., Close at t-1)\n",
    "df['return'] = df['prev_close'].pct_change()\n",
    "df['excess_return'] = df['return'] - RISK_FREE_RATE_HOURLY\n",
    "rolling_mean_excess = df['excess_return'].rolling(window=ROLLING_WINDOW_SR).mean()\n",
    "rolling_std_excess = df['excess_return'].rolling(window=ROLLING_WINDOW_SR).std()\n",
    "df['raw_sharpe'] = (rolling_mean_excess / (rolling_std_excess + 1e-9)) * np.sqrt(ANNUALIZATION_FACTOR)\n",
    "\n",
    "# MACD uses 'prev_close' for EMAs\n",
    "df['EMA_12'] = df['prev_close'].ewm(span=12, adjust=False).mean()\n",
    "df['EMA_26'] = df['prev_close'].ewm(span=26, adjust=False).mean()\n",
    "df['raw_macd'] = df['EMA_12'] - df['EMA_26']\n",
    "df['raw_macd_signal'] = df['raw_macd'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# Bollinger Bands uses 'prev_close' for MA and StdDev\n",
    "ROLLING_WINDOW_BB = 20\n",
    "df['BB_Middle'] = df['prev_close'].rolling(window=ROLLING_WINDOW_BB).mean()\n",
    "df['BB_Std'] = df['prev_close'].rolling(window=ROLLING_WINDOW_BB).std()\n",
    "df['raw_bb_upper'] = df['BB_Middle'] + (df['BB_Std'] * 2)\n",
    "df['raw_bb_lower'] = df['BB_Middle'] - (df['BB_Std'] * 2)\n",
    "\n",
    "# OBV uses 'prev_close'\n",
    "df['raw_obv'] = (np.sign(df['prev_close'].diff()) * df['volume'].shift(1)).cumsum().fillna(0)\n",
    "\n",
    "\n",
    "# ATR (Average True Range) - NEW\n",
    "df['high_t_minus_1'] = df['high'].shift(1)\n",
    "df['low_t_minus_1'] = df['low'].shift(1)\n",
    "df['prev_prev_close'] = df['prev_close'].shift(1)\n",
    "\n",
    "df['tr_1'] = df['high_t_minus_1'] - df['low_t_minus_1'] # Range of candle t-1\n",
    "df['tr_2'] = np.abs(df['high_t_minus_1'] - df['prev_prev_close']) # Distance from previous close to high\n",
    "df['tr_3'] = np.abs(df['low_t_minus_1'] - df['prev_prev_close']) # Distance from previous close to low\n",
    "df['true_range'] = df[['tr_1', 'tr_2', 'tr_3']].max(axis=1)\n",
    "df['raw_atr'] = df['true_range'].rolling(window=14).mean()\n",
    "\n",
    "# RSI (Relative Strength Index) - NEW\n",
    "delta = df['prev_close'].diff()\n",
    "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "rs = gain / (loss + 1e-9)\n",
    "df['raw_rsi'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# --- 3. Add Cyclical Time Features (No Change, Already Safe) ---\n",
    "df['hour'] = df.index.hour\n",
    "df['day_of_week'] = df.index.dayofweek\n",
    "\n",
    "df['feature_hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['feature_hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['feature_day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['feature_day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "# --- 4. Create Final, Normalized Features (Shift Applied When Necessary) ---\n",
    "\n",
    "# We define a log-return feature based on data available in the previous completed candle.\n",
    "df['feature_log_return_1h'] = np.log(df['prev_close'] / df['prev_close'].shift(1))\n",
    "\n",
    "\n",
    "# Price Features (Normalized by prev_close, ready for t=0 observation)\n",
    "df['feature_open'] = (df['open'] / df['prev_close']) - 1\n",
    "df['feature_high'] = (df['high'].shift(1) / df['prev_close']) - 1\n",
    "df['feature_low'] = (df['low'].shift(1) / df['prev_close']) - 1\n",
    "\n",
    "\n",
    "# Volume Features (Z-Score of volume/OBV at t-1)\n",
    "vol_mean_30d = df['volume'].shift(1).rolling(30*24).mean()\n",
    "vol_std_30d = df['volume'].shift(1).rolling(30*24).std()\n",
    "df['feature_volume_zscore'] = ((df['volume'].shift(1) - vol_mean_30d) / (vol_std_30d + 1e-9))\n",
    "obv_mean_30d = df['raw_obv'].shift(1).rolling(30*24).mean()\n",
    "obv_std_30d = df['raw_obv'].shift(1).rolling(30*24).std()\n",
    "df['feature_obv_zscore'] = ((df['raw_obv'].shift(1) - obv_mean_30d) / (obv_std_30d + 1e-9))\n",
    "\n",
    "# Indicator Features\n",
    "df['feature_MACD'] = (df['raw_macd'].shift(1) / df['prev_close'])\n",
    "df['feature_MACD_Signal'] = (df['raw_macd_signal'].shift(1) / df['prev_close'])\n",
    "df['feature_BB_Upper'] = (df['raw_bb_upper'].shift(1) / df['prev_close']) - 1\n",
    "df['feature_BB_Lower'] = (df['raw_bb_lower'].shift(1) / df['prev_close']) - 1\n",
    "df['feature_atr'] = (df['raw_atr'].shift(1) / df['prev_close'])\n",
    "df['feature_rsi'] = df['raw_rsi'].shift(1)\n",
    "df['feature_sharpe_ratio'] = df['raw_sharpe'].shift(1)\n",
    "\n",
    "# --- 5. Final Cleanup ---\n",
    "final_features = [\n",
    "    'feature_hour_sin', 'feature_hour_cos', 'feature_day_sin', 'feature_day_cos',\n",
    "    'feature_open', 'feature_high', 'feature_low', 'feature_log_return_1h',\n",
    "    'feature_volume_zscore', 'feature_obv_zscore',\n",
    "    'feature_MACD', 'feature_MACD_Signal', \n",
    "    'feature_BB_Upper', 'feature_BB_Lower',\n",
    "    'feature_atr', 'feature_rsi', 'feature_sharpe_ratio'\n",
    "]\n",
    "\n",
    "# Keep the current raw OHLCV for the Environment to calculate rewards/penalties,\n",
    "# but the agent MUST only observe the 'feature_' columns.\n",
    "all_cols_to_keep = ['close','open', 'high', 'low', 'prev_close', 'volume'] + final_features\n",
    "df = df[all_cols_to_keep]\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# --- 6. Defining DataFrames ---\n",
    "# (Environment setup remains correct as it depends on the index slicing)\n",
    "df_eval = df.loc['2025-10-01':'2025-11-01'] \n",
    "df_train_full = df.loc['2024-10-01':'2025-09-30'] \n",
    "df_train = df_train_full.loc['2024-10-01':'2025-06-30']\n",
    "df_validation = df_train_full.loc['2025-07-01':'2025-09-30']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd121ad",
   "metadata": {},
   "source": [
    "# Reward function \n",
    "In previous attemps, we noticed that the agent has a tendencie to do nothing. As Such, we will add a Neutrality penality as to make sure the agent does not do anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fd878140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_reward(historical_info: dict):\n",
    "    # Position: The position held *during* the last completed step (t).\n",
    "    position = historical_info[\"position\", -1] \n",
    "    \n",
    "    # Prices: Use the Close price of the completed bar (t) and the Close price of the bar before it (t-1).\n",
    "    # This represents the fractional return of the asset over the last hour.\n",
    "    current_close = historical_info[\"data_close\", -1] \n",
    "    previous_close = historical_info[\"data_close\", -2] \n",
    "    \n",
    "    # 1. Calculate the asset's fractional return during the step\n",
    "    # Note: Using .pct_change() logic is generally more stable than (C-P)/P\n",
    "    # return is the return of bar t.\n",
    "    asset_return = (current_close - previous_close) / previous_close\n",
    "    \n",
    "    # 2. Calculate Portfolio PnL for this step\n",
    "    # PnL = (Asset Return) * (Held Position)\n",
    "    pnl = asset_return * position\n",
    "\n",
    "    # 3. Define and Apply the Neutrality Penalty\n",
    "    NEUTRAL_PENALTY = -0.000001\n",
    "    \n",
    "    reward = pnl\n",
    "    if position == 0:\n",
    "        # Penalize for holding cash (or no position)\n",
    "        reward += NEUTRAL_PENALTY\n",
    "        \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a6e342f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Creation of Three Environments ---\n",
    "\n",
    "POSITIONS = [-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1]\n",
    "WINDOW_SIZE = 48\n",
    "TRADING_FEES =  0.01/100\n",
    "BORROW_INTEREST_RATE = 0.0003/100\n",
    "\n",
    "# Environment for TRAINING\n",
    "env_train = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD_Train\",\n",
    "        df = df_train, \n",
    "        windows=WINDOW_SIZE,\n",
    "        positions = POSITIONS,\n",
    "        trading_fees = TRADING_FEES, \n",
    "        borrow_interest_rate= BORROW_INTEREST_RATE,\n",
    "        reward_function=custom_reward\n",
    "    )\n",
    "\n",
    "# Environment for VALIDATION (used inside Optuna)\n",
    "env_validation = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD_Validation\",\n",
    "        df = df_validation,\n",
    "        windows=WINDOW_SIZE,\n",
    "        positions = POSITIONS,\n",
    "        trading_fees = TRADING_FEES, \n",
    "        borrow_interest_rate= BORROW_INTEREST_RATE,\n",
    "        reward_function=custom_reward\n",
    "    )\n",
    "\n",
    "# Environment for FINAL TEST (used only once)\n",
    "env_eval = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD_Eval\",\n",
    "        df = df_eval, \n",
    "        windows=WINDOW_SIZE,\n",
    "        positions = POSITIONS,\n",
    "        trading_fees = TRADING_FEES, \n",
    "        borrow_interest_rate= BORROW_INTEREST_RATE,\n",
    "        reward_function=custom_reward\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5c0dbde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env, num_episodes=20, max_steps=None, render=False, csv_path=\"evaluation_results.csv\", renderer_logs_dir=\"render_logs\"):\n",
    "    \"\"\"\n",
    "    Evaluate the agent on the environment for a number of episodes.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Ensure render dir exists\n",
    "    if render:\n",
    "        Path(renderer_logs_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        step = 0\n",
    "        reward_total = 0.0\n",
    "        while not done and not truncated:\n",
    "            action = agent.choose_action_eval(obs)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            reward_total += reward\n",
    "            step += 1\n",
    "            if (max_steps is not None) and (step >= max_steps):\n",
    "                break\n",
    "\n",
    "        metrics = env.get_metrics()\n",
    "        port_ret = float(metrics[\"Portfolio Return\"].strip('%')) / 100.0\n",
    "        market_ret = float(metrics[\"Market Return\"].strip('%')) / 100.0\n",
    "\n",
    "        results.append({\n",
    "            \"episode\": ep + 1,\n",
    "            \"portfolio_return\": port_ret,\n",
    "            \"market_return\": market_ret,\n",
    "            \"excess_return\": port_ret - market_ret,\n",
    "            \"steps\": step,\n",
    "            \"total_reward\": reward_total,\n",
    "        })\n",
    "        \n",
    "        if render:\n",
    "            print(f\"Eval Episode {ep+1}: Total Reward: {reward_total:.2f}, Portfolio Return: {port_ret:.2%}, Market Return: {market_ret:.2%}, Excess Return: {(port_ret - market_ret):.2%}, Steps: {step}\")\n",
    "            time.sleep(1)\n",
    "            env.save_for_render(dir=renderer_logs_dir)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Ensure the directory for the CSV exists\n",
    "    Path(csv_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_results.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved evaluation results to {csv_path}\")\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae1fae",
   "metadata": {},
   "source": [
    "## 3. üé≤ Baseline: The Random Agent\n",
    "\n",
    "Before we build a complex DRL agent, we must establish a baseline. If our \"smart\" agent can't beat an agent that takes random actions, it has learned nothing.\n",
    "\n",
    "The `RandomAgent` simply chooses a random action (a position from -1 to 1) from the environment's action space at every step. We will evaluate this agent on the **test set** to see what score we need to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fa1b7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "    def choose_action_eval(self, state):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4a031775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : -8.65%   |   Portfolio Return : -6.70%   |   \n",
      "Eval Episode 1: Total Reward: -0.01, Portfolio Return: -6.70%, Market Return: -8.65%, Excess Return: 1.95%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return : -1.26%   |   \n",
      "Eval Episode 2: Total Reward: 0.05, Portfolio Return: -1.26%, Market Return: -8.65%, Excess Return: 7.39%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return : -1.99%   |   \n",
      "Eval Episode 3: Total Reward: 0.04, Portfolio Return: -1.99%, Market Return: -8.65%, Excess Return: 6.66%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return : -19.38%   |   \n",
      "Eval Episode 4: Total Reward: -0.16, Portfolio Return: -19.38%, Market Return: -8.65%, Excess Return: -10.73%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return : -13.92%   |   \n",
      "Eval Episode 5: Total Reward: -0.09, Portfolio Return: -13.92%, Market Return: -8.65%, Excess Return: -5.27%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return : -12.79%   |   \n",
      "Eval Episode 6: Total Reward: -0.08, Portfolio Return: -12.79%, Market Return: -8.65%, Excess Return: -4.14%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return : -5.33%   |   \n",
      "Eval Episode 7: Total Reward: 0.00, Portfolio Return: -5.33%, Market Return: -8.65%, Excess Return: 3.32%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return : -0.48%   |   \n",
      "Eval Episode 8: Total Reward: 0.05, Portfolio Return: -0.48%, Market Return: -8.65%, Excess Return: 8.17%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return : -7.12%   |   \n",
      "Eval Episode 9: Total Reward: -0.01, Portfolio Return: -7.12%, Market Return: -8.65%, Excess Return: 1.53%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  7.25%   |   \n",
      "Eval Episode 10: Total Reward: 0.13, Portfolio Return: 7.25%, Market Return: -8.65%, Excess Return: 15.90%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return : -6.42%   |   \n",
      "Eval Episode 11: Total Reward: -0.01, Portfolio Return: -6.42%, Market Return: -8.65%, Excess Return: 2.23%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return : -0.22%   |   \n",
      "Eval Episode 12: Total Reward: 0.06, Portfolio Return: -0.22%, Market Return: -8.65%, Excess Return: 8.43%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return : -16.81%   |   \n",
      "Eval Episode 13: Total Reward: -0.13, Portfolio Return: -16.81%, Market Return: -8.65%, Excess Return: -8.16%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return : -5.60%   |   \n",
      "Eval Episode 14: Total Reward: -0.00, Portfolio Return: -5.60%, Market Return: -8.65%, Excess Return: 3.05%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  2.06%   |   \n",
      "Eval Episode 15: Total Reward: 0.08, Portfolio Return: 2.06%, Market Return: -8.65%, Excess Return: 10.71%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.77%   |   \n",
      "Eval Episode 16: Total Reward: 0.06, Portfolio Return: 0.77%, Market Return: -8.65%, Excess Return: 9.42%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return : -10.68%   |   \n",
      "Eval Episode 17: Total Reward: -0.06, Portfolio Return: -10.68%, Market Return: -8.65%, Excess Return: -2.03%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  4.59%   |   \n",
      "Eval Episode 18: Total Reward: 0.10, Portfolio Return: 4.59%, Market Return: -8.65%, Excess Return: 13.24%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return : -2.50%   |   \n",
      "Eval Episode 19: Total Reward: 0.03, Portfolio Return: -2.50%, Market Return: -8.65%, Excess Return: 6.15%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  4.00%   |   \n",
      "Eval Episode 20: Total Reward: 0.09, Portfolio Return: 4.00%, Market Return: -8.65%, Excess Return: 12.65%, Steps: 720\n",
      "Saved evaluation results to eval/evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a random agent for evaluation\n",
    "agent = RandomAgent(env_eval.action_space)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "df_results = evaluate_agent(agent, env_eval, num_episodes=20, render=True, csv_path=eval_folder / \"evaluation_results.csv\", renderer_logs_dir=eval_folder / \"render_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd5f30c",
   "metadata": {},
   "source": [
    "## 4. üß† Building the PPO Agent\n",
    "\n",
    "This is the core of our project. We are implementing a **Proximal Policy Optimization (PPO)** agent from scratch using PyTorch.\n",
    "\n",
    "### Why PPO?\n",
    "PPO is a robust, state-of-the-art algorithm that balances exploration (trying new things) and exploitation (using what works). It uses a \"clipped\" objective function to prevent updates that are too large, which leads to more stable and reliable training than older methods.\n",
    "\n",
    "### Architecture\n",
    "Our PPO agent consists of two key components:\n",
    "\n",
    "1.  **`ActorCriticNetwork`:** This single neural network serves two purposes:\n",
    "    * **Actor (Policy):** It decides *what action to take* (e.g., \"go long\", \"go short\"). It outputs a probability distribution over all 9 possible actions.\n",
    "    * **Critic (Value):** It estimates *how good the current state is* (i.e., the expected future reward). It outputs a single value, which helps the Actor learn better.\n",
    "\n",
    "2.  **`PPOAgent`:** This class manages the entire learning process. It:\n",
    "    * Holds the `ActorCriticNetwork` and its optimizer.\n",
    "    * Gathers experience from the environment (`store`).\n",
    "    * Calculates advantages using Generalized Advantage Estimation (GAE) to determine how much better an action was than expected (`compute_gae`).\n",
    "    * Runs the PPO update logic across multiple epochs to improve the network (`update`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9aecf006",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, hidden_size=128, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.window_size = state_shape[0]\n",
    "        self.n_features = state_shape[1]\n",
    "\n",
    "        # --- 1. CNN Layers ---\n",
    "        # Input: (Batch, Features, Window)\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.n_features, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # The convolution output size is (Batch, 64, self.window_size).\n",
    "        self.flattened_size = 64 * self.window_size \n",
    "\n",
    "        # --- 2. Dynamic Shared Linear Layers (Fixing n_layers) ---\n",
    "        layers = []\n",
    "        input_dim = self.flattened_size\n",
    "        \n",
    "        for _ in range(n_layers):\n",
    "            layers.append(nn.Linear(input_dim, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_size # Next layer takes hidden_size as input\n",
    "            \n",
    "        self.shared_linear = nn.Sequential(*layers)\n",
    "\n",
    "        # --- 3. Heads ---\n",
    "        self.actor = nn.Linear(hidden_size, n_actions)\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, Window, Features)\n",
    "        \n",
    "        # Permute for Conv1d: (Batch, Features, Window)\n",
    "        x_permuted = x.permute(0, 2, 1) \n",
    "        \n",
    "        cnn_out = self.cnn_layers(x_permuted)\n",
    "        x_flat = cnn_out.view(-1, self.flattened_size)\n",
    "        \n",
    "        features = self.shared_linear(x_flat)\n",
    "\n",
    "        # --- Fix: Return Logits (Raw scores), NOT Softmax ---\n",
    "        action_logits = self.actor(features) \n",
    "        state_value = self.critic(features)\n",
    "\n",
    "        return action_logits, state_value\n",
    "    \n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(\n",
    "        self, state_size, n_actions,\n",
    "        lr=3e-4, gamma=0.99, gae_lambda=0.95,\n",
    "        entropy_beta=0.01, clip_epsilon=0.2, ppo_epochs=10, batch_size=64,\n",
    "        hidden_size=128,\n",
    "        n_layers=2  # <<< --- 1. ADD THIS (with a default)\n",
    "    ):\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.entropy_beta = entropy_beta\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Environment parameters\n",
    "        self.state_size = state_size\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        # Device configuration\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")  \n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        # Create policy network\n",
    "        self.network = ActorCriticNetwork(\n",
    "            state_size, \n",
    "            n_actions, \n",
    "            hidden_size, \n",
    "            n_layers  \n",
    "        ).to(self.device)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "\n",
    "        # Memory buffers\n",
    "        self.reset_memory()\n",
    "\n",
    "    def reset_memory(self):\n",
    "        \"\"\"Clear rollout buffers.\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.log_probs = []\n",
    "\n",
    "    def get_action_value_logprob(self, state):\n",
    "        \"\"\"\n",
    "        Samples an action for the training loop.\n",
    "        Returns the action, its value, and log probability.\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, value = self.network(state_tensor)\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action.item(), value.item(), log_prob.item()\n",
    "\n",
    "    def choose_action_eval(self, state):\n",
    "        \"\"\"\n",
    "        Chooses the best action for evaluation (deterministic).\n",
    "        Returns only the action index.\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, _ = self.network(state_tensor)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        action = torch.argmax(probs, dim=-1)\n",
    "        \n",
    "        return action.item()\n",
    "\n",
    "    def store(self, state, action, reward, value, done, log_prob):\n",
    "        \"\"\"Store a single transition in memory.\"\"\"\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.dones.append(done)\n",
    "        self.log_probs.append(log_prob)\n",
    "\n",
    "    def compute_gae(self, next_value):\n",
    "        \"\"\"\n",
    "        Compute returns and advantages using GAE (Generalized Advantage Estimation)\n",
    "        \"\"\"\n",
    "        rewards = np.array(self.rewards, dtype=np.float32)\n",
    "        values = np.array(self.values + [next_value], dtype=np.float32)\n",
    "        dones = np.array(self.dones, dtype=np.float32)\n",
    "\n",
    "        T = len(rewards)\n",
    "        returns = np.zeros(T, dtype=np.float32)\n",
    "        advantages = np.zeros(T, dtype=np.float32)\n",
    "\n",
    "        gae = 0.0\n",
    "        for t in reversed(range(T)):\n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1.0 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1.0 - dones[t]) * gae\n",
    "            advantages[t] = gae\n",
    "            returns[t] = advantages[t] + values[t]\n",
    "\n",
    "        return returns, advantages\n",
    "\n",
    "    def update(self, next_value):\n",
    "        \"\"\"Perform one PPO update step.\"\"\"\n",
    "        if len(self.states) == 0:\n",
    "            return {\"actor_loss\": 0.0, \"critic_loss\": 0.0}\n",
    "\n",
    "        returns, advantages = self.compute_gae(next_value)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states = torch.tensor(np.array(self.states), dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(np.array(self.actions), dtype=torch.int64, device=self.device)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32, device=self.device)\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32, device=self.device)\n",
    "        old_log_probs = torch.tensor(np.array(self.log_probs), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        total_actor_loss = 0\n",
    "        total_critic_loss = 0\n",
    "        updates = 0\n",
    "        \n",
    "        for _ in range(self.ppo_epochs):\n",
    "            indices = torch.randperm(len(states))\n",
    "            \n",
    "            for start in range(0, len(states), self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                idx = indices[start:end]\n",
    "                \n",
    "                if len(idx) == 0:\n",
    "                    continue\n",
    "\n",
    "                batch_states = states[idx]\n",
    "                batch_actions = actions[idx]\n",
    "                batch_old_log_probs = old_log_probs[idx]\n",
    "                batch_returns = returns[idx]\n",
    "                batch_advantages = advantages[idx]\n",
    "                \n",
    "                # Forward pass\n",
    "                logits, values = self.network(batch_states)\n",
    "                action_probs = F.softmax(logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(action_probs)\n",
    "                log_probs = dist.log_prob(batch_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # PPO loss computation\n",
    "                ratio = torch.exp(log_probs - batch_old_log_probs)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                critic_loss = (batch_returns - values.squeeze()).pow(2).mean()\n",
    "                \n",
    "                loss = actor_loss + 0.5 * critic_loss - self.entropy_beta * entropy\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_actor_loss += actor_loss.item()\n",
    "                total_critic_loss += critic_loss.item()\n",
    "                updates += 1\n",
    "        \n",
    "        self.reset_memory()\n",
    "        \n",
    "        if updates == 0:\n",
    "            return {\"actor_loss\": 0.0, \"critic_loss\": 0.0}\n",
    "\n",
    "        return {\n",
    "            'actor_loss': total_actor_loss / updates,\n",
    "            'critic_loss': total_critic_loss / updates\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c24c5ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (48, 19)\n",
      "Number of actions: 9\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.95%   |   \n",
      "Eval Episode 1: Total Reward: 0.01, Portfolio Return: 0.95%, Market Return: -8.65%, Excess Return: 9.60%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.95%   |   \n",
      "Eval Episode 2: Total Reward: 0.01, Portfolio Return: 0.95%, Market Return: -8.65%, Excess Return: 9.60%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.95%   |   \n",
      "Eval Episode 3: Total Reward: 0.01, Portfolio Return: 0.95%, Market Return: -8.65%, Excess Return: 9.60%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.94%   |   \n",
      "Eval Episode 4: Total Reward: 0.01, Portfolio Return: 0.94%, Market Return: -8.65%, Excess Return: 9.59%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.95%   |   \n",
      "Eval Episode 5: Total Reward: 0.01, Portfolio Return: 0.95%, Market Return: -8.65%, Excess Return: 9.60%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.95%   |   \n",
      "Eval Episode 6: Total Reward: 0.01, Portfolio Return: 0.95%, Market Return: -8.65%, Excess Return: 9.60%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.94%   |   \n",
      "Eval Episode 7: Total Reward: 0.01, Portfolio Return: 0.94%, Market Return: -8.65%, Excess Return: 9.59%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.95%   |   \n",
      "Eval Episode 8: Total Reward: 0.01, Portfolio Return: 0.95%, Market Return: -8.65%, Excess Return: 9.60%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.95%   |   \n",
      "Eval Episode 9: Total Reward: 0.01, Portfolio Return: 0.95%, Market Return: -8.65%, Excess Return: 9.60%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.95%   |   \n",
      "Eval Episode 10: Total Reward: 0.01, Portfolio Return: 0.95%, Market Return: -8.65%, Excess Return: 9.60%, Steps: 720\n",
      "Saved evaluation results to eval/evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Get state and action dimensions from the environment\n",
    "state_shape = env_eval.observation_space.shape  # This will be (10, 12)\n",
    "n_actions = env_eval.action_space.n\n",
    "\n",
    "print(f\"State shape: {state_shape}\")\n",
    "print(f\"Number of actions: {n_actions}\")\n",
    "\n",
    "# Create the agent with the correct dimensions\n",
    "agent = PPOAgent(\n",
    "    state_size=state_shape, \n",
    "    n_actions=n_actions,\n",
    "    n_layers=2 # Explicitly using the new param we added\n",
    ")\n",
    "# Evaluate the (untrained) agent\n",
    "# This will now run without errors\n",
    "df_results = evaluate_agent(agent, env_eval, num_episodes=10, render=True, csv_path=eval_folder / \"evaluation_results.csv\", renderer_logs_dir=eval_folder / \"render_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b634b64f",
   "metadata": {},
   "source": [
    "## üéØ The `objective` Function: Our Optimization Target\n",
    "\n",
    "This function defines a **single trial** for our Optuna hyperparameter search. Its job is to take a set of hyperparameters, train an agent, evaluate it, and return a single score that Optuna will try to maximize.\n",
    "\n",
    "The entire process works in these steps:\n",
    "\n",
    "1.  **Suggest Hyperparameters:** Optuna provides a `trial` object. We use this object to \"suggest\" a unique combination of hyperparameters (like learning rate, batch size, and network layers) from the search space we defined (e.g., `lr` between `1e-5` and `1e-3`). This set of parameters is stored in the `ppo_hps` dictionary.\n",
    "\n",
    "2.  **Initialize Agent:** A *brand new* `PPOAgent` is created from scratch using the `ppo_hps` for this specific trial.\n",
    "\n",
    "3.  **Train the Agent:** The new agent is trained for a fixed `TOTAL_TIMESTEPS` (500,000 steps) on our `env_train` (the main training dataset). This is a fast-but-thorough training run to see how well this set of hyperparameters can learn a policy.\n",
    "\n",
    "4.  **Evaluate on Validation Set:** After training, the agent's performance is measured on the **`env_validation`** (our 3-month validation set). We specifically *do not* use our final `env_eval` test set here. This is critical to prevent data leakage and ensures our final test results are unbiased.\n",
    "\n",
    "5.  **Return the Score:** We calculate the agent's average **`mean_excess_return`** (its portfolio return *minus* the market's return) on the validation set. This single number is the final score for the trial.\n",
    "\n",
    "Optuna will run this function 50 times, each time generating a new set of hyperparameters and getting a new `mean_excess_return`. It will keep track of which hyperparameters produced the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cf6e3ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # --- 1. Hyperparameters ---\n",
    "    ppo_hps = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True),\n",
    "        \"gamma\": trial.suggest_categorical(\"gamma\", [0.98, 0.99, 0.995, 0.999]),\n",
    "        \"gae_lambda\": trial.suggest_float(\"gae_lambda\", 0.8, 0.999),\n",
    "        # Increase lower bound for stability and exploration\n",
    "        \"entropy_beta\": trial.suggest_float(\"entropy_beta\", 1e-3, 1e-2, log=True), \n",
    "        \"clip_epsilon\": trial.suggest_float(\"clip_epsilon\", 0.1, 0.3),\n",
    "        \"ppo_epochs\": trial.suggest_int(\"ppo_epochs\", 5, 15), # Narrowed range slightly\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512]),\n",
    "        \"hidden_size\": trial.suggest_categorical(\"hidden_size\", [256, 512, 1024]), \n",
    "        \"n_layers\": trial.suggest_int(\"n_layers\", 2, 4)\n",
    "    }\n",
    "\n",
    "    # --- 2. Setup ---\n",
    "    TOTAL_TIMESTEPS = 200_00 \n",
    "    ROLLOUT_STEPS = 2048\n",
    "    PRUNING_INTERVAL_STEPS = 10 * ROLLOUT_STEPS # Evaluate every ~20k steps\n",
    "    \n",
    "    agent = PPOAgent(\n",
    "        state_size=state_shape,\n",
    "        n_actions=n_actions,\n",
    "        **ppo_hps\n",
    "    )\n",
    "\n",
    "    # --- 3. Training Loop (Standard PPO Loop) ---\n",
    "    obs, info = env_train.reset()\n",
    "    \n",
    "    for step in range(1, TOTAL_TIMESTEPS + 1):\n",
    "        # ... PPO sampling and storing logic ...\n",
    "        action, value, log_prob = agent.get_action_value_logprob(obs)\n",
    "        next_obs, reward, done, truncated, info = env_train.step(action)\n",
    "        agent.store(obs, action, reward, value, done, log_prob)\n",
    "        obs = next_obs\n",
    "        \n",
    "        # Update Phase\n",
    "        if step % ROLLOUT_STEPS == 0:\n",
    "            next_value = 0.0\n",
    "            if not done:\n",
    "                with torch.no_grad():\n",
    "                    _, next_value_tensor = agent.network(\n",
    "                        torch.tensor(obs, dtype=torch.float32, device=agent.device).unsqueeze(0)\n",
    "                    )\n",
    "                    next_value = next_value_tensor.item()\n",
    "            agent.update(next_value)\n",
    "\n",
    "            # --- 4. PRUNING LOGIC (Using Mean Excess Return) ---\n",
    "            if step % PRUNING_INTERVAL_STEPS == 0:\n",
    "                val_results = evaluate_agent(agent, env_validation, num_episodes=5, render=False)\n",
    "                \n",
    "                # Metric for pruning: Mean Excess Return (Portfolio - Market)\n",
    "                mean_excess_return = val_results['portfolio_return'].mean() - val_results['market_return'].mean()\n",
    "                \n",
    "                # Report to Optuna\n",
    "                trial.report(mean_excess_return, step)\n",
    "\n",
    "                # Handle Pruning\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.TrialPruned()\n",
    "\n",
    "        if done or truncated:\n",
    "            obs, info = env_train.reset()\n",
    "\n",
    "    # --- 5. Final Evaluation (Robust) ---\n",
    "    eval_results = evaluate_agent(agent, env_validation, num_episodes=20, render=False)\n",
    "    \n",
    "    portfolio_returns = eval_results['portfolio_return']\n",
    "    market_returns = eval_results['market_return']\n",
    "    excess_returns = portfolio_returns - market_returns\n",
    "    \n",
    "    # --- 6. The Final Score: Maximize Mean Excess Return (Most Stable) ---\n",
    "    final_score = excess_returns.mean()\n",
    "    \n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2681484d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Hyperparameter Optimization with Optuna\n",
    "\n",
    "Now that we have our agent, environment, and a proper validation setup, we need to find the best set of hyperparameters (like learning rate, network size, etc.) to make our agent as profitable as possible.\n",
    "\n",
    "Doing this by hand is very time-consuming. Instead, we use the `Optuna` library to automate this search.\n",
    "\n",
    "This code block sets up and runs the optimization study:\n",
    "\n",
    "1.  **Sampler & Pruner:** We initialize a `TPESampler` (a smart algorithm to guess the next best parameters) and a `MedianPruner` (which automatically stops trials that are performing poorly to save time).\n",
    "2.  **Create Study:** We create a `study` object and tell it we want to `\"maximize\"` the score.\n",
    "3.  **Run Optimization:** We call `study.optimize(objective, n_trials=50)`. This will:\n",
    "    * Call our `objective` function 50 times.\n",
    "    * Each call, it passes in a new set of hyperparameters for the `trial`.\n",
    "    * Our `objective` function trains a temporary agent on `env_train`, evaluates it on `env_validation`, and returns the **mean excess return**.\n",
    "    * Optuna keeps track of which hyperparameters produced the highest excess return.\n",
    "4.  **Get Results:** After the search is complete, the script prints the best score and stores the winning set of hyperparameters in the `best_hps` dictionary.\n",
    "\n",
    "We will use this `best_hps` dictionary in the next step to train our single, final agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f28427cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 76.67%   |   Portfolio Return : -53.83%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.65%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.65%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.65%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -11.64%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 00:47:00,860] Trial 0 finished with value: -0.16421500000000006 and parameters: {'lr': 5.6115164153345e-05, 'gamma': 0.98, 'gae_lambda': 0.8310429095469044, 'entropy_beta': 0.001143098387631322, 'clip_epsilon': 0.27323522915498705, 'ppo_epochs': 11, 'batch_size': 256, 'hidden_size': 256, 'n_layers': 2}. Best is trial 0 with value: -0.16421500000000006.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return : -11.64%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -52.77%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -10.08%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -25.33%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.20%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.20%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.20%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.20%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.20%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.20%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.19%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.19%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.19%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.20%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.19%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.19%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.19%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.20%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.20%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.20%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.20%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.20%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.19%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 00:48:58,798] Trial 1 finished with value: -0.14976499999999998 and parameters: {'lr': 0.00011207606211860574, 'gamma': 0.995, 'gae_lambda': 0.8581367850585084, 'entropy_beta': 0.0023246728489504354, 'clip_epsilon': 0.19121399684340717, 'ppo_epochs': 13, 'batch_size': 256, 'hidden_size': 256, 'n_layers': 4}. Best is trial 1 with value: -0.14976499999999998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return : -10.20%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -33.90%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -42.84%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -31.84%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -7.95%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -6.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -6.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -6.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -6.95%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -6.95%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -6.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -7.27%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -6.95%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -7.95%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -6.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -7.27%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -7.95%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -7.27%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -6.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -6.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -6.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -6.95%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -7.27%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 00:51:11,026] Trial 2 finished with value: -0.11964 and parameters: {'lr': 0.0008536189862866829, 'gamma': 0.98, 'gae_lambda': 0.8875903462541807, 'entropy_beta': 0.0013244581340099357, 'clip_epsilon': 0.19903538202225404, 'ppo_epochs': 5, 'batch_size': 64, 'hidden_size': 512, 'n_layers': 4}. Best is trial 2 with value: -0.11964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return : -7.27%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -19.37%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -55.71%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -65.90%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.59%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.71%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.69%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.71%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.69%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.71%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.71%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.71%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.69%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.69%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.71%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.59%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.59%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.59%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.59%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.59%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.69%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.59%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.71%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 00:53:08,087] Trial 3 finished with value: -0.15442 and parameters: {'lr': 0.0003550304858128307, 'gamma': 0.98, 'gae_lambda': 0.817610007908332, 'entropy_beta': 0.0015703008378806713, 'clip_epsilon': 0.10904545778210761, 'ppo_epochs': 8, 'batch_size': 256, 'hidden_size': 512, 'n_layers': 4}. Best is trial 2 with value: -0.11964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return : -10.69%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : 35.81%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -51.09%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -13.00%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.43%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.43%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.28%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.43%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.43%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.43%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.43%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.43%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.43%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.44%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 00:54:48,765] Trial 4 finished with value: -0.17208 and parameters: {'lr': 1.4096175149815848e-05, 'gamma': 0.98, 'gae_lambda': 0.962276824262512, 'entropy_beta': 0.005091635945818554, 'clip_epsilon': 0.24580143360819745, 'ppo_epochs': 13, 'batch_size': 512, 'hidden_size': 256, 'n_layers': 2}. Best is trial 2 with value: -0.11964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return : -12.44%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -47.57%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -50.33%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : 32.19%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -9.17%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -9.17%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.50%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -9.18%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.50%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -9.17%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.50%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.50%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.49%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -9.18%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.50%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -9.18%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -9.18%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.49%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.50%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.50%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -9.17%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -9.18%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.50%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 00:57:21,438] Trial 5 finished with value: -0.135825 and parameters: {'lr': 4.4706085467784903e-05, 'gamma': 0.995, 'gae_lambda': 0.8237992549417221, 'entropy_beta': 0.005167075260023276, 'clip_epsilon': 0.25215700972337945, 'ppo_epochs': 11, 'batch_size': 64, 'hidden_size': 512, 'n_layers': 3}. Best is trial 2 with value: -0.11964.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return : -8.49%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -50.17%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -25.82%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -45.34%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.39%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.40%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.40%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.40%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.40%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.40%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.40%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.39%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.40%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.39%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.40%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.39%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.39%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.39%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.40%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.39%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.39%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.39%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  7.39%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 00:59:11,313] Trial 6 finished with value: 0.02615 and parameters: {'lr': 4.253162363790868e-05, 'gamma': 0.99, 'gae_lambda': 0.9503546765700667, 'entropy_beta': 0.0016935505549297925, 'clip_epsilon': 0.1153959819657586, 'ppo_epochs': 8, 'batch_size': 128, 'hidden_size': 256, 'n_layers': 4}. Best is trial 6 with value: 0.02615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return :  7.40%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -43.62%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -13.82%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : 15.82%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.77%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.86%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.77%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.86%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.73%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.87%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.87%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.86%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.73%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.86%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.92%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.73%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.92%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.86%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.73%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.86%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 01:01:28,681] Trial 7 finished with value: -0.15577000000000002 and parameters: {'lr': 0.00011986281799901183, 'gamma': 0.99, 'gae_lambda': 0.8453590973458465, 'entropy_beta': 0.0026736699110984107, 'clip_epsilon': 0.2636029531844986, 'ppo_epochs': 14, 'batch_size': 128, 'hidden_size': 1024, 'n_layers': 2}. Best is trial 6 with value: 0.02615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return : -10.73%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -57.08%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -55.94%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : 17.83%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -16.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -16.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -16.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -17.34%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -16.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -16.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -16.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -17.34%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -17.34%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -16.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -16.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -16.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -16.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -16.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -16.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -17.34%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -17.34%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -16.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -17.34%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 01:03:22,348] Trial 8 finished with value: -0.21886 and parameters: {'lr': 0.00010903884523201111, 'gamma': 0.995, 'gae_lambda': 0.8501046768692475, 'entropy_beta': 0.0031423062259089674, 'clip_epsilon': 0.16017566196335392, 'ppo_epochs': 8, 'batch_size': 128, 'hidden_size': 512, 'n_layers': 2}. Best is trial 6 with value: 0.02615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return : -17.34%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -47.82%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -34.70%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -40.92%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.18%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.18%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.18%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.17%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.17%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.18%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.17%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.17%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.18%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.17%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.18%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.17%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.18%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.17%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.18%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.18%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.18%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.18%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.17%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 01:36:01,130] Trial 9 finished with value: -0.046040000000000005 and parameters: {'lr': 9.525889086580776e-05, 'gamma': 0.98, 'gae_lambda': 0.8472898712544875, 'entropy_beta': 0.005348307249011093, 'clip_epsilon': 0.17355662654385065, 'ppo_epochs': 11, 'batch_size': 512, 'hidden_size': 256, 'n_layers': 3}. Best is trial 6 with value: 0.02615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return :  0.18%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -29.74%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -57.88%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -44.93%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.50%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.51%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.49%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.50%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.51%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.50%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.49%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.50%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.61%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.50%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.51%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.50%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.50%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.51%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.50%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.61%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.49%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.50%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  4.50%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 01:45:39,857] Trial 10 finished with value: -0.002630000000000002 and parameters: {'lr': 1.1722636308848833e-05, 'gamma': 0.99, 'gae_lambda': 0.9724787950482287, 'entropy_beta': 0.008556385477088214, 'clip_epsilon': 0.10291503830572668, 'ppo_epochs': 5, 'batch_size': 128, 'hidden_size': 1024, 'n_layers': 4}. Best is trial 6 with value: 0.02615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return :  4.61%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -22.68%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -51.73%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -28.20%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -10.96%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 01:47:52,859] Trial 11 finished with value: -0.157445 and parameters: {'lr': 1.0204693491397313e-05, 'gamma': 0.99, 'gae_lambda': 0.973087961641251, 'entropy_beta': 0.008971303214686336, 'clip_epsilon': 0.10570280732553256, 'ppo_epochs': 5, 'batch_size': 128, 'hidden_size': 1024, 'n_layers': 4}. Best is trial 6 with value: 0.02615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return : -10.97%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -26.29%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -46.82%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -43.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.29%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.29%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -12.30%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 01:50:03,684] Trial 12 finished with value: -0.170785 and parameters: {'lr': 2.2235175431901966e-05, 'gamma': 0.999, 'gae_lambda': 0.9395369017105862, 'entropy_beta': 0.008929873535891907, 'clip_epsilon': 0.1440614275560611, 'ppo_epochs': 7, 'batch_size': 128, 'hidden_size': 1024, 'n_layers': 3}. Best is trial 6 with value: 0.02615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return : -12.29%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -61.14%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -56.03%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -63.88%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.41%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.41%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.41%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -5.42%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 01:52:29,927] Trial 13 finished with value: -0.10211 and parameters: {'lr': 2.5322575772226174e-05, 'gamma': 0.99, 'gae_lambda': 0.9968123046900537, 'entropy_beta': 0.001735731398818552, 'clip_epsilon': 0.13370175617817054, 'ppo_epochs': 7, 'batch_size': 128, 'hidden_size': 1024, 'n_layers': 4}. Best is trial 6 with value: 0.02615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return : -5.42%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -53.68%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -38.98%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -27.29%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.05%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.05%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.04%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.03%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.04%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.04%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.03%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.05%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.04%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.05%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.03%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.05%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.05%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.05%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.05%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.05%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.04%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.05%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.03%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 01:54:44,122] Trial 14 finished with value: -0.03737 and parameters: {'lr': 3.09509957721512e-05, 'gamma': 0.99, 'gae_lambda': 0.9264682514784472, 'entropy_beta': 0.004095260694235406, 'clip_epsilon': 0.1287266118887468, 'ppo_epochs': 6, 'batch_size': 128, 'hidden_size': 1024, 'n_layers': 4}. Best is trial 6 with value: 0.02615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return :  1.04%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -43.69%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -33.03%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -41.24%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.43%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.43%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.43%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.44%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.43%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 01:56:33,518] Trial 15 finished with value: -0.023425 and parameters: {'lr': 1.5915126007950784e-05, 'gamma': 0.99, 'gae_lambda': 0.9152285366823998, 'entropy_beta': 0.002086142015026637, 'clip_epsilon': 0.226786261734768, 'ppo_epochs': 9, 'batch_size': 128, 'hidden_size': 256, 'n_layers': 3}. Best is trial 6 with value: 0.02615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return :  2.43%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -48.44%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -35.18%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : 13.01%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.53%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.12%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.53%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.53%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.21%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.21%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.54%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.21%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.22%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.21%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.21%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.53%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.21%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.21%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.21%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.53%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.21%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.22%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -15.22%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 01:58:58,023] Trial 16 finished with value: -0.201 and parameters: {'lr': 0.00024973337466901497, 'gamma': 0.999, 'gae_lambda': 0.9551421660153789, 'entropy_beta': 0.006856031665656745, 'clip_epsilon': 0.10059350545191015, 'ppo_epochs': 9, 'batch_size': 128, 'hidden_size': 1024, 'n_layers': 4}. Best is trial 6 with value: 0.02615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return : -15.54%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -59.22%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -34.00%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -38.46%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.25%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.26%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.25%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.25%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.25%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.26%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.25%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.25%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.25%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.25%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.25%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.25%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.25%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.26%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.25%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.25%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.25%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.25%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -4.26%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 02:00:36,322] Trial 17 finished with value: -0.09032000000000003 and parameters: {'lr': 4.194420421193254e-05, 'gamma': 0.99, 'gae_lambda': 0.9918892881070396, 'entropy_beta': 0.0036253916558141364, 'clip_epsilon': 0.16139745710180847, 'ppo_epochs': 6, 'batch_size': 512, 'hidden_size': 256, 'n_layers': 3}. Best is trial 6 with value: 0.02615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return : -4.25%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -28.33%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -34.34%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -29.99%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.69%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.70%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.71%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.70%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.71%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.71%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.70%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.71%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.69%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.69%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.71%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.69%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.71%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.69%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.71%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.71%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.70%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.70%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.70%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 02:03:01,401] Trial 18 finished with value: -0.020784999999999998 and parameters: {'lr': 1.0549726860207241e-05, 'gamma': 0.99, 'gae_lambda': 0.9054102552174973, 'entropy_beta': 0.0010011652532828997, 'clip_epsilon': 0.12362081397019718, 'ppo_epochs': 5, 'batch_size': 64, 'hidden_size': 1024, 'n_layers': 4}. Best is trial 6 with value: 0.02615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return :  2.70%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -27.32%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -56.42%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -37.77%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.54%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.43%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.43%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.54%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.43%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.55%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.43%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.54%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.55%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.42%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.43%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.43%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.43%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 02:04:49,577] Trial 19 finished with value: -0.03319 and parameters: {'lr': 6.42900218497509e-05, 'gamma': 0.99, 'gae_lambda': 0.8797922812927976, 'entropy_beta': 0.007217939082548546, 'clip_epsilon': 0.21940247773996283, 'ppo_epochs': 7, 'batch_size': 128, 'hidden_size': 256, 'n_layers': 4}. Best is trial 6 with value: 0.02615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return :  1.55%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -44.29%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -58.77%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -22.16%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.52%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.52%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.51%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.51%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.51%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.51%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.52%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.51%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.52%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.52%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.51%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.52%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.53%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.51%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.52%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.52%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.52%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.52%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -3.52%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 02:07:09,844] Trial 20 finished with value: -0.08296999999999999 and parameters: {'lr': 2.084588073574429e-05, 'gamma': 0.999, 'gae_lambda': 0.943967115040446, 'entropy_beta': 0.0018572656019316233, 'clip_epsilon': 0.14802279844687088, 'ppo_epochs': 10, 'batch_size': 128, 'hidden_size': 1024, 'n_layers': 3}. Best is trial 6 with value: 0.02615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return : -3.52%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -43.84%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -11.08%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -53.27%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.98%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.98%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 02:09:35,291] Trial 21 finished with value: -0.008045000000000002 and parameters: {'lr': 1.0007236731812058e-05, 'gamma': 0.99, 'gae_lambda': 0.9085127260663897, 'entropy_beta': 0.0010874410132689727, 'clip_epsilon': 0.29468582820569833, 'ppo_epochs': 5, 'batch_size': 64, 'hidden_size': 1024, 'n_layers': 4}. Best is trial 6 with value: 0.02615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return :  3.97%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 76.67%   |   Portfolio Return : -45.26%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -60.36%   |   \n",
      "Market Return : 76.67%   |   Portfolio Return : -57.74%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.68%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.68%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.68%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.68%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.68%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.68%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  5.67%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 03:02:12,626] Trial 22 finished with value: 0.008929999999999997 and parameters: {'lr': 1.4761920471576542e-05, 'gamma': 0.99, 'gae_lambda': 0.9763547200275244, 'entropy_beta': 0.0013934014432135133, 'clip_epsilon': 0.2909928407015177, 'ppo_epochs': 6, 'batch_size': 64, 'hidden_size': 1024, 'n_layers': 4}. Best is trial 6 with value: 0.02615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return :  4.78%   |   Portfolio Return :  5.67%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "sampler = TPESampler(seed=42)\n",
    "pruner = MedianPruner()\n",
    "\n",
    "# Re-create the study to start fresh\n",
    "study = optuna.create_study(\n",
    "    study_name=\"ppo_trading_agent_optimization_v2\",\n",
    "    direction=\"maximize\",\n",
    "    sampler=sampler,\n",
    "    pruner=pruner\n",
    ")\n",
    "\n",
    "# Start the optimization again (it should no longer fail)\n",
    "try:\n",
    "    study.optimize(objective, n_trials=50, timeout=5400) # 50 trials, 1h 30min timeout\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Optimization stopped manually.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "15794ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Optimization Finished ---\n",
      "Number of finished trials: 23\n",
      "\n",
      "Best trial:\n",
      "Value (Mean Excess Return): 0.0261\n",
      "Best Hyperparameters:\n",
      "lr: 4.253162363790868e-05\n",
      "gamma: 0.99\n",
      "gae_lambda: 0.9503546765700667\n",
      "entropy_beta: 0.0016935505549297925\n",
      "clip_epsilon: 0.1153959819657586\n",
      "ppo_epochs: 8\n",
      "batch_size: 128\n",
      "hidden_size: 256\n",
      "n_layers: 4\n",
      "\n",
      "Best hyperparameters dictionary:\n",
      "{'lr': 4.253162363790868e-05, 'gamma': 0.99, 'gae_lambda': 0.9503546765700667, 'entropy_beta': 0.0016935505549297925, 'clip_epsilon': 0.1153959819657586, 'ppo_epochs': 8, 'batch_size': 128, 'hidden_size': 256, 'n_layers': 4}\n"
     ]
    }
   ],
   "source": [
    "# --- Print Results ---\n",
    "print(\"\\n--- Optimization Finished ---\")\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "\n",
    "print(\"\\nBest trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(f\"Value (Mean Excess Return): {best_trial.value:.4f}\")\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# You can now use these best hyperparameters to train your final agent\n",
    "# for a longer duration (e.g., more TOTAL_TIMESTEPS).\n",
    "best_hps = best_trial.params\n",
    "print(\"\\nBest hyperparameters dictionary:\")\n",
    "print(best_hps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed946484",
   "metadata": {},
   "source": [
    "--- Optimization Finished ---\n",
    "Number of finished trials: 16\n",
    "\n",
    "- Best trial:\n",
    "- Value (Mean Excess Return): 0.0331\n",
    "- Best Hyperparameters:\n",
    "- lr: 2.0738922307588755e-05\n",
    "- gamma: 0.999\n",
    "- gae_lambda: 0.9455630627411082\n",
    "- entropy_beta: 0.007109875990561107\n",
    "- clip_epsilon: 0.22401267536164615\n",
    "- ppo_epochs: 13\n",
    "- batch_size: 512\n",
    "- hidden_size: 1024\n",
    "- n_layers: 2\n",
    "\n",
    "- Best hyperparameters dictionary:\n",
    "- {'lr': 2.0738922307588755e-05, 'gamma': 0.999, 'gae_lambda': 0.9455630627411082, 'entropy_beta': 0.007109875990561107, 'clip_epsilon': 0.22401267536164615, 'ppo_epochs': 13, 'batch_size': 512, 'hidden_size': 1024, 'n_layers': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c39e9",
   "metadata": {},
   "source": [
    "## üöÄ Training the Final Agent\n",
    "\n",
    "This is the main training script. After identifying the best hyperparameters with Optuna (`best_hps`), we now use them to train a single, \"production-ready\" agent.\n",
    "\n",
    "This process is designed to be robust and follows standard machine learning practices:\n",
    "\n",
    "1.  **Consolidate Training Data:** We combine our original `df_train` and `df_validation` sets into one large `df_train_full`. This gives our final agent the maximum amount of data (the full year) to learn from.\n",
    "2.  **Initialize Optimal Agent:** We create a new `PPOAgent` instance using the exact `best_hps` (learning rate, hidden size, etc.) discovered by Optuna.\n",
    "3.  **Train on Full Data:** The agent is trained for a long duration (1 million steps) on the `env_train_full`.\n",
    "4.  **Validate and Save:** Critically, every 5 updates, the agent is evaluated against the **`env_validation`** (the 3-month validation set).\n",
    "5.  **Save Best Model:** We save the agent's network weights *only if* its **mean excess return** (its performance *above* the market) on the validation set improves. This ensures we are not just saving a lucky model, but the one that is most generalizable and best at *beating* the market.\n",
    "\n",
    "The final saved file, `models/ppo_trading_agent.pth`, will contain the weights of the single best-performing model, ready for the final, unseen test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "31ba0713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (48, 19)\n",
      "Number of actions: 9\n",
      "Starting training for 100000 timesteps...\n",
      "Will update every 2048 steps.\n",
      "Evaluating every 5 updates.\n",
      "\n",
      "Update 1 (Step 2048/100000)\n",
      "  Actor Loss: -0.0046, Critic Loss: 0.0001\n",
      "\n",
      "Update 2 (Step 4096/100000)\n",
      "  Actor Loss: -0.0074, Critic Loss: 0.0003\n",
      "\n",
      "Update 3 (Step 6144/100000)\n",
      "  Actor Loss: -0.0058, Critic Loss: 0.0001\n",
      "\n",
      "Update 4 (Step 8192/100000)\n",
      "  Actor Loss: -0.0075, Critic Loss: 0.0002\n",
      "Market Return : 88.05%   |   Portfolio Return : -49.54%   |   \n",
      "\n",
      "Update 5 (Step 10240/100000)\n",
      "  Actor Loss: -0.0059, Critic Loss: 0.0002\n",
      "  Mean Reward (last 10 ep): 0.0135\n",
      "--- Running Validation ---\n",
      "Market Return :  4.78%   |   Portfolio Return :  3.32%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.33%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.33%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.33%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.32%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.33%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.33%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.33%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.33%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.32%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.32%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.33%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.33%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.33%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.33%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.31%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.32%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.33%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.32%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  3.33%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: 3.33%\n",
      "  Mean Validation Excess Return: -1.45%\n",
      "  Market Return: 4.78%\n",
      "  *** New best model saved with EXCESS return -1.45% ***\n",
      "--------------------------\n",
      "\n",
      "Update 6 (Step 12288/100000)\n",
      "  Actor Loss: -0.0076, Critic Loss: 0.0003\n",
      "  Mean Reward (last 10 ep): 0.0135\n",
      "\n",
      "Update 7 (Step 14336/100000)\n",
      "  Actor Loss: -0.0105, Critic Loss: 0.0003\n",
      "  Mean Reward (last 10 ep): 0.0135\n",
      "\n",
      "Update 8 (Step 16384/100000)\n",
      "  Actor Loss: -0.0091, Critic Loss: 0.0002\n",
      "  Mean Reward (last 10 ep): 0.0135\n",
      "Market Return : 88.05%   |   Portfolio Return : -42.41%   |   \n",
      "\n",
      "Update 9 (Step 18432/100000)\n",
      "  Actor Loss: -0.0092, Critic Loss: 0.0002\n",
      "  Mean Reward (last 10 ep): 0.0810\n",
      "\n",
      "Update 10 (Step 20480/100000)\n",
      "  Actor Loss: -0.0073, Critic Loss: 0.0002\n",
      "  Mean Reward (last 10 ep): 0.0810\n",
      "--- Running Validation ---\n",
      "Market Return :  4.78%   |   Portfolio Return : -8.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.31%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.29%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.29%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.31%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.31%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.31%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.29%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.30%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.31%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.31%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -8.29%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: -8.30%\n",
      "  Mean Validation Excess Return: -13.08%\n",
      "  Market Return: 4.78%\n",
      "--------------------------\n",
      "\n",
      "Update 11 (Step 22528/100000)\n",
      "  Actor Loss: -0.0098, Critic Loss: 0.0003\n",
      "  Mean Reward (last 10 ep): 0.0810\n",
      "\n",
      "Update 12 (Step 24576/100000)\n",
      "  Actor Loss: -0.0089, Critic Loss: 0.0002\n",
      "  Mean Reward (last 10 ep): 0.0810\n",
      "Market Return : 88.05%   |   Portfolio Return : -46.79%   |   \n",
      "\n",
      "Update 13 (Step 26624/100000)\n",
      "  Actor Loss: -0.0120, Critic Loss: 0.0002\n",
      "  Mean Reward (last 10 ep): 0.0759\n",
      "\n",
      "Update 14 (Step 28672/100000)\n",
      "  Actor Loss: -0.0075, Critic Loss: 0.0002\n",
      "  Mean Reward (last 10 ep): 0.0759\n",
      "\n",
      "Update 15 (Step 30720/100000)\n",
      "  Actor Loss: -0.0113, Critic Loss: 0.0004\n",
      "  Mean Reward (last 10 ep): 0.0759\n",
      "--- Running Validation ---\n",
      "Market Return :  4.78%   |   Portfolio Return :  1.89%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.91%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.91%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.89%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.90%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.89%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.90%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.90%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.89%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.89%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.90%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.90%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.90%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.90%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.91%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.90%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.89%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.90%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.90%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  1.90%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: 1.90%\n",
      "  Mean Validation Excess Return: -2.88%\n",
      "  Market Return: 4.78%\n",
      "--------------------------\n",
      "\n",
      "Update 16 (Step 32768/100000)\n",
      "  Actor Loss: -0.0102, Critic Loss: 0.0002\n",
      "  Mean Reward (last 10 ep): 0.0759\n",
      "\n",
      "Update 17 (Step 34816/100000)\n",
      "  Actor Loss: -0.0099, Critic Loss: 0.0003\n",
      "  Mean Reward (last 10 ep): 0.0759\n",
      "Market Return : 88.05%   |   Portfolio Return : -30.39%   |   \n",
      "\n",
      "Update 18 (Step 36864/100000)\n",
      "  Actor Loss: -0.0075, Critic Loss: 0.0003\n",
      "  Mean Reward (last 10 ep): 0.1434\n",
      "\n",
      "Update 19 (Step 38912/100000)\n",
      "  Actor Loss: -0.0102, Critic Loss: 0.0005\n",
      "  Mean Reward (last 10 ep): 0.1434\n",
      "\n",
      "Update 20 (Step 40960/100000)\n",
      "  Actor Loss: -0.0101, Critic Loss: 0.0003\n",
      "  Mean Reward (last 10 ep): 0.1434\n",
      "--- Running Validation ---\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.62%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.63%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.62%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.62%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.63%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.62%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.62%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.63%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.62%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.63%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.64%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.63%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  8.64%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: 8.63%\n",
      "  Mean Validation Excess Return: 3.85%\n",
      "  Market Return: 4.78%\n",
      "  *** New best model saved with EXCESS return 3.85% ***\n",
      "--------------------------\n",
      "\n",
      "Update 21 (Step 43008/100000)\n",
      "  Actor Loss: -0.0096, Critic Loss: 0.0003\n",
      "  Mean Reward (last 10 ep): 0.1434\n",
      "Market Return : 88.05%   |   Portfolio Return : -58.62%   |   \n",
      "\n",
      "Update 22 (Step 45056/100000)\n",
      "  Actor Loss: -0.0114, Critic Loss: 0.0003\n",
      "  Mean Reward (last 10 ep): 0.0808\n",
      "\n",
      "Update 23 (Step 47104/100000)\n",
      "  Actor Loss: -0.0107, Critic Loss: 0.0003\n",
      "  Mean Reward (last 10 ep): 0.0808\n",
      "\n",
      "Update 24 (Step 49152/100000)\n",
      "  Actor Loss: -0.0122, Critic Loss: 0.0004\n",
      "  Mean Reward (last 10 ep): 0.0808\n",
      "\n",
      "Update 25 (Step 51200/100000)\n",
      "  Actor Loss: -0.0115, Critic Loss: 0.0003\n",
      "  Mean Reward (last 10 ep): 0.0808\n",
      "--- Running Validation ---\n",
      "Market Return :  4.78%   |   Portfolio Return :  2.66%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.66%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.68%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.66%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.66%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.67%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.68%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.67%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: 2.67%\n",
      "  Mean Validation Excess Return: -2.11%\n",
      "  Market Return: 4.78%\n",
      "--------------------------\n",
      "Market Return : 88.05%   |   Portfolio Return : -52.76%   |   \n",
      "\n",
      "Update 26 (Step 53248/100000)\n",
      "  Actor Loss: -0.0122, Critic Loss: 0.0003\n",
      "  Mean Reward (last 10 ep): 0.0580\n",
      "\n",
      "Update 27 (Step 55296/100000)\n",
      "  Actor Loss: -0.0104, Critic Loss: 0.0005\n",
      "  Mean Reward (last 10 ep): 0.0580\n",
      "\n",
      "Update 28 (Step 57344/100000)\n",
      "  Actor Loss: -0.0134, Critic Loss: 0.0004\n",
      "  Mean Reward (last 10 ep): 0.0580\n",
      "\n",
      "Update 29 (Step 59392/100000)\n",
      "  Actor Loss: -0.0124, Critic Loss: 0.0004\n",
      "  Mean Reward (last 10 ep): 0.0580\n",
      "Market Return : 88.05%   |   Portfolio Return : -6.57%   |   \n",
      "\n",
      "Update 30 (Step 61440/100000)\n",
      "  Actor Loss: -0.0137, Critic Loss: 0.0004\n",
      "  Mean Reward (last 10 ep): 0.1365\n",
      "--- Running Validation ---\n",
      "Market Return :  4.78%   |   Portfolio Return : -2.88%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.88%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.89%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.88%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.88%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.88%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.89%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.88%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.88%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.87%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.87%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.88%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.89%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.88%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.88%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.88%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.87%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.88%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.88%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -2.89%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: -2.88%\n",
      "  Mean Validation Excess Return: -7.66%\n",
      "  Market Return: 4.78%\n",
      "--------------------------\n",
      "\n",
      "Update 31 (Step 63488/100000)\n",
      "  Actor Loss: -0.0094, Critic Loss: 0.0004\n",
      "  Mean Reward (last 10 ep): 0.1365\n",
      "\n",
      "Update 32 (Step 65536/100000)\n",
      "  Actor Loss: -0.0146, Critic Loss: 0.0006\n",
      "  Mean Reward (last 10 ep): 0.1365\n",
      "\n",
      "Update 33 (Step 67584/100000)\n",
      "  Actor Loss: -0.0120, Critic Loss: 0.0004\n",
      "  Mean Reward (last 10 ep): 0.1365\n",
      "\n",
      "Update 34 (Step 69632/100000)\n",
      "  Actor Loss: -0.0141, Critic Loss: 0.0004\n",
      "  Mean Reward (last 10 ep): 0.1365\n",
      "Market Return : 88.05%   |   Portfolio Return : -45.08%   |   \n",
      "\n",
      "Update 35 (Step 71680/100000)\n",
      "  Actor Loss: -0.0093, Critic Loss: 0.0004\n",
      "  Mean Reward (last 10 ep): 0.1268\n",
      "--- Running Validation ---\n",
      "Market Return :  4.78%   |   Portfolio Return :  2.13%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.12%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.13%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.12%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.13%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.13%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.13%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.13%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.13%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.13%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.12%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.13%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.12%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.13%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.12%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.12%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.13%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.13%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.12%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  2.12%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: 2.13%\n",
      "  Mean Validation Excess Return: -2.65%\n",
      "  Market Return: 4.78%\n",
      "--------------------------\n",
      "\n",
      "Update 36 (Step 73728/100000)\n",
      "  Actor Loss: -0.0143, Critic Loss: 0.0005\n",
      "  Mean Reward (last 10 ep): 0.1268\n",
      "\n",
      "Update 37 (Step 75776/100000)\n",
      "  Actor Loss: -0.0141, Critic Loss: 0.0003\n",
      "  Mean Reward (last 10 ep): 0.1268\n",
      "\n",
      "Update 38 (Step 77824/100000)\n",
      "  Actor Loss: -0.0164, Critic Loss: 0.0004\n",
      "  Mean Reward (last 10 ep): 0.1268\n",
      "Market Return : 88.05%   |   Portfolio Return : -73.29%   |   \n",
      "\n",
      "Update 39 (Step 79872/100000)\n",
      "  Actor Loss: -0.0119, Critic Loss: 0.0003\n",
      "  Mean Reward (last 10 ep): 0.0372\n",
      "\n",
      "Update 40 (Step 81920/100000)\n",
      "  Actor Loss: -0.0134, Critic Loss: 0.0004\n",
      "  Mean Reward (last 10 ep): 0.0372\n",
      "--- Running Validation ---\n",
      "Market Return :  4.78%   |   Portfolio Return :  0.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.95%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.97%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.96%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.95%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return :  0.96%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: 0.96%\n",
      "  Mean Validation Excess Return: -3.82%\n",
      "  Market Return: 4.78%\n",
      "--------------------------\n",
      "\n",
      "Update 41 (Step 83968/100000)\n",
      "  Actor Loss: -0.0143, Critic Loss: 0.0004\n",
      "  Mean Reward (last 10 ep): 0.0372\n",
      "\n",
      "Update 42 (Step 86016/100000)\n",
      "  Actor Loss: -0.0135, Critic Loss: 0.0003\n",
      "  Mean Reward (last 10 ep): 0.0372\n",
      "Market Return : 88.05%   |   Portfolio Return : -34.95%   |   \n",
      "\n",
      "Update 43 (Step 88064/100000)\n",
      "  Actor Loss: -0.0165, Critic Loss: 0.0004\n",
      "  Mean Reward (last 10 ep): 0.0551\n",
      "\n",
      "Update 44 (Step 90112/100000)\n",
      "  Actor Loss: -0.0147, Critic Loss: 0.0005\n",
      "  Mean Reward (last 10 ep): 0.0551\n",
      "\n",
      "Update 45 (Step 92160/100000)\n",
      "  Actor Loss: -0.0155, Critic Loss: 0.0004\n",
      "  Mean Reward (last 10 ep): 0.0551\n",
      "--- Running Validation ---\n",
      "Market Return :  4.78%   |   Portfolio Return : -0.60%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.60%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.62%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.60%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.61%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.62%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.62%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.62%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.61%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.61%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.62%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.60%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.60%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.60%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.62%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.62%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.61%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.61%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.61%   |   \n",
      "Market Return :  4.78%   |   Portfolio Return : -0.61%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: -0.61%\n",
      "  Mean Validation Excess Return: -5.39%\n",
      "  Market Return: 4.78%\n",
      "--------------------------\n",
      "\n",
      "Update 46 (Step 94208/100000)\n",
      "  Actor Loss: -0.0143, Critic Loss: 0.0004\n",
      "  Mean Reward (last 10 ep): 0.0551\n",
      "Market Return : 88.05%   |   Portfolio Return : -33.46%   |   \n",
      "\n",
      "Update 47 (Step 96256/100000)\n",
      "  Actor Loss: -0.0135, Critic Loss: 0.0004\n",
      "  Mean Reward (last 10 ep): 0.0754\n",
      "\n",
      "Update 48 (Step 98304/100000)\n",
      "  Actor Loss: -0.0144, Critic Loss: 0.0005\n",
      "  Mean Reward (last 10 ep): 0.0754\n",
      "\n",
      "Training finished.\n",
      "Best model saved to models/ppo_trading_agent_v2.pth with validation excess return: 3.85%\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Create a \"full training\" environment ---\n",
    "# This combines your original train and validation sets\n",
    "df_train_full = df.loc['2024-10-01':'2025-09-30'] \n",
    "\n",
    "env_train_full = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD_Eval\",\n",
    "        df = df_train_full, \n",
    "        windows=WINDOW_SIZE,\n",
    "        positions = POSITIONS,\n",
    "        trading_fees = TRADING_FEES, \n",
    "        borrow_interest_rate= BORROW_INTEREST_RATE,\n",
    "        reward_function=custom_reward\n",
    "    )\n",
    "\n",
    "# --- 2. Get environment parameters ---\n",
    "state_shape = env_train_full.observation_space.shape  # Use the new env\n",
    "n_actions = env_train_full.action_space.n\n",
    "\n",
    "print(f\"State shape: {state_shape}\")\n",
    "print(f\"Number of actions: {n_actions}\")\n",
    "\n",
    "# --- 3. Training hyperparameters ---\n",
    "TOTAL_TIMESTEPS = 1_000_00     \n",
    "ROLLOUT_STEPS = 2048         \n",
    "EVAL_EVERY_N_UPDATES = 5     \n",
    "MODEL_SAVE_PATH = \"models/ppo_trading_agent_v2.pth\"\n",
    "\n",
    "# --- 4. Initialize agent with best hyperparameters ---\n",
    "agent = PPOAgent(state_size=state_shape, n_actions=n_actions, **best_hps)\n",
    "\n",
    "# --- 5. Training & logging setup ---\n",
    "all_episode_rewards = [] \n",
    "episode_rewards = []     \n",
    "best_eval_excess_return = -float('inf') # <--- We track BEST EXCESS RETURN\n",
    "update_count = 0\n",
    "\n",
    "print(f\"Starting training for {TOTAL_TIMESTEPS} timesteps...\")\n",
    "print(f\"Will update every {ROLLOUT_STEPS} steps.\")\n",
    "print(f\"Evaluating every {EVAL_EVERY_N_UPDATES} updates.\")\n",
    "\n",
    "# --- 6. Main training loop ---\n",
    "obs, info = env_train_full.reset() # <--- Use the full training env\n",
    "\n",
    "for step in range(1, TOTAL_TIMESTEPS + 1):\n",
    "    action, value, log_prob = agent.get_action_value_logprob(obs)\n",
    "    \n",
    "    # --- Use the full training env ---\n",
    "    next_obs, reward, done, truncated, info = env_train_full.step(action) \n",
    "    \n",
    "    agent.store(obs, action, reward, value, done, log_prob)\n",
    "    episode_rewards.append(reward)\n",
    "    obs = next_obs\n",
    "    \n",
    "    if step % ROLLOUT_STEPS == 0:\n",
    "        update_count += 1\n",
    "        \n",
    "        next_value = 0.0\n",
    "        if not done:\n",
    "            with torch.no_grad():\n",
    "                _, next_value_tensor = agent.network(torch.tensor(obs, dtype=torch.float32, device=agent.device).unsqueeze(0))\n",
    "                next_value = next_value_tensor.item()\n",
    "        \n",
    "        losses = agent.update(next_value)\n",
    "        \n",
    "        print(f\"\\nUpdate {update_count} (Step {step}/{TOTAL_TIMESTEPS})\")\n",
    "        print(f\"  Actor Loss: {losses['actor_loss']:.4f}, Critic Loss: {losses['critic_loss']:.4f}\")\n",
    "        if len(all_episode_rewards) > 0:\n",
    "            print(f\"  Mean Reward (last 10 ep): {np.mean(all_episode_rewards[-10:]):.4f}\")\n",
    "        \n",
    "        # --- Periodic evaluation on the VALIDATION set ---\n",
    "        if update_count % EVAL_EVERY_N_UPDATES == 0:\n",
    "            print(\"--- Running Validation ---\")\n",
    "            \n",
    "            # --- FIX 1: Evaluate on env_validation ---\n",
    "            eval_results = evaluate_agent(agent, env_validation, num_episodes=20, render=False)\n",
    "            \n",
    "            mean_eval_return = eval_results['portfolio_return'].mean()\n",
    "            market_return = eval_results['market_return'].mean()\n",
    "            \n",
    "            # --- FIX 2: Calculate and save based on EXCESS return ---\n",
    "            mean_excess_return = mean_eval_return - market_return \n",
    "            \n",
    "            print(f\"  Mean Validation Portfolio Return: {mean_eval_return:.2%}\")\n",
    "            print(f\"  Mean Validation Excess Return: {mean_excess_return:.2%}\")\n",
    "            print(f\"  Market Return: {market_return:.2%}\")\n",
    "            \n",
    "            if mean_excess_return > best_eval_excess_return:\n",
    "                best_eval_excess_return = mean_excess_return\n",
    "                torch.save(agent.network.state_dict(), MODEL_SAVE_PATH)\n",
    "                print(f\"  *** New best model saved with EXCESS return {best_eval_excess_return:.2%} ***\")\n",
    "            print(\"--------------------------\")\n",
    "            \n",
    "    if done or truncated:\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "        episode_rewards = []\n",
    "        obs, info = env_train_full.reset() # <--- Use the full training env\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "print(f\"Best model saved to {MODEL_SAVE_PATH} with validation excess return: {best_eval_excess_return:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d24d0cd",
   "metadata": {},
   "source": [
    "# üèÅ Final Model Evaluation on Unseen Test Data\n",
    "This code block performs the final and most important test of the project.\n",
    "\n",
    "After completing the Optuna hyperparameter search, we have the best_hps dictionary, which defines the optimal agent architecture. Our main training loop has saved the best-performing model weights (based on validation performance) to models/ppo_trading_agent.pth.\n",
    "\n",
    "This final script does the following:\n",
    "\n",
    "1. Initialize Optimal Agent: It creates a new PPOAgent instance using the exact architecture and hyperparameters found by Optuna (best_hps).\n",
    "\n",
    "2. Load Trained Weights: It loads the saved weights from models/ppo_trading_agent.pth into this new agent.\n",
    "\n",
    "3. Set to Evaluation Mode: It calls trained_agent.network.eval(). This is a standard PyTorch step that turns off training-specific behaviors (like dropout) to ensure the model's output is deterministic.\n",
    "\n",
    "4. Run on Test Set: It calls the evaluate_agent function on env_eval. This is the first and only time our agent encounters the test data. This gives us a fair, unbiased assessment of how well it generalized.\n",
    "\n",
    "5. Save & Report: The final performance metrics are saved to evaluation_results.csv (as required by the project) and printed. This result demonstrates the agent's final performance against the market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5efbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model with hyperparameters: {'lr': 4.253162363790868e-05, 'gamma': 0.99, 'gae_lambda': 0.9503546765700667, 'entropy_beta': 0.0016935505549297925, 'clip_epsilon': 0.1153959819657586, 'ppo_epochs': 8, 'batch_size': 128, 'hidden_size': 256, 'n_layers': 4}\n",
      "--- Evaluating final trained agent on TEST SET ---\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.63%   |   \n",
      "Eval Episode 1: Total Reward: 0.11, Portfolio Return: 8.63%, Market Return: 4.78%, Excess Return: 3.85%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.62%   |   \n",
      "Eval Episode 2: Total Reward: 0.11, Portfolio Return: 8.62%, Market Return: 4.78%, Excess Return: 3.84%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.64%   |   \n",
      "Eval Episode 3: Total Reward: 0.11, Portfolio Return: 8.64%, Market Return: 4.78%, Excess Return: 3.86%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.64%   |   \n",
      "Eval Episode 4: Total Reward: 0.11, Portfolio Return: 8.64%, Market Return: 4.78%, Excess Return: 3.86%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.62%   |   \n",
      "Eval Episode 5: Total Reward: 0.11, Portfolio Return: 8.62%, Market Return: 4.78%, Excess Return: 3.84%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.62%   |   \n",
      "Eval Episode 6: Total Reward: 0.11, Portfolio Return: 8.62%, Market Return: 4.78%, Excess Return: 3.84%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.64%   |   \n",
      "Eval Episode 7: Total Reward: 0.11, Portfolio Return: 8.64%, Market Return: 4.78%, Excess Return: 3.86%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.62%   |   \n",
      "Eval Episode 8: Total Reward: 0.11, Portfolio Return: 8.62%, Market Return: 4.78%, Excess Return: 3.84%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.62%   |   \n",
      "Eval Episode 9: Total Reward: 0.11, Portfolio Return: 8.62%, Market Return: 4.78%, Excess Return: 3.84%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.62%   |   \n",
      "Eval Episode 10: Total Reward: 0.11, Portfolio Return: 8.62%, Market Return: 4.78%, Excess Return: 3.84%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.64%   |   \n",
      "Eval Episode 11: Total Reward: 0.11, Portfolio Return: 8.64%, Market Return: 4.78%, Excess Return: 3.86%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.62%   |   \n",
      "Eval Episode 12: Total Reward: 0.11, Portfolio Return: 8.62%, Market Return: 4.78%, Excess Return: 3.84%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.64%   |   \n",
      "Eval Episode 13: Total Reward: 0.11, Portfolio Return: 8.64%, Market Return: 4.78%, Excess Return: 3.86%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.64%   |   \n",
      "Eval Episode 14: Total Reward: 0.11, Portfolio Return: 8.64%, Market Return: 4.78%, Excess Return: 3.86%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.63%   |   \n",
      "Eval Episode 15: Total Reward: 0.11, Portfolio Return: 8.63%, Market Return: 4.78%, Excess Return: 3.85%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.64%   |   \n",
      "Eval Episode 16: Total Reward: 0.11, Portfolio Return: 8.64%, Market Return: 4.78%, Excess Return: 3.86%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.63%   |   \n",
      "Eval Episode 17: Total Reward: 0.11, Portfolio Return: 8.63%, Market Return: 4.78%, Excess Return: 3.85%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.62%   |   \n",
      "Eval Episode 18: Total Reward: 0.11, Portfolio Return: 8.62%, Market Return: 4.78%, Excess Return: 3.84%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.62%   |   \n",
      "Eval Episode 19: Total Reward: 0.11, Portfolio Return: 8.62%, Market Return: 4.78%, Excess Return: 3.84%, Steps: 2160\n",
      "Market Return :  4.78%   |   Portfolio Return :  8.64%   |   \n",
      "Eval Episode 20: Total Reward: 0.11, Portfolio Return: 8.64%, Market Return: 4.78%, Excess Return: 3.86%, Steps: 2160\n",
      "Saved evaluation results to eval/evaluation_results.csv\n",
      "--- Final Evaluation Complete ---\n",
      "    episode  portfolio_return  market_return  excess_return  steps  \\\n",
      "0         1            0.0863         0.0478         0.0385   2160   \n",
      "1         2            0.0862         0.0478         0.0384   2160   \n",
      "2         3            0.0864         0.0478         0.0386   2160   \n",
      "3         4            0.0864         0.0478         0.0386   2160   \n",
      "4         5            0.0862         0.0478         0.0384   2160   \n",
      "5         6            0.0862         0.0478         0.0384   2160   \n",
      "6         7            0.0864         0.0478         0.0386   2160   \n",
      "7         8            0.0862         0.0478         0.0384   2160   \n",
      "8         9            0.0862         0.0478         0.0384   2160   \n",
      "9        10            0.0862         0.0478         0.0384   2160   \n",
      "10       11            0.0864         0.0478         0.0386   2160   \n",
      "11       12            0.0862         0.0478         0.0384   2160   \n",
      "12       13            0.0864         0.0478         0.0386   2160   \n",
      "13       14            0.0864         0.0478         0.0386   2160   \n",
      "14       15            0.0863         0.0478         0.0385   2160   \n",
      "15       16            0.0864         0.0478         0.0386   2160   \n",
      "16       17            0.0863         0.0478         0.0385   2160   \n",
      "17       18            0.0862         0.0478         0.0384   2160   \n",
      "18       19            0.0862         0.0478         0.0384   2160   \n",
      "19       20            0.0864         0.0478         0.0386   2160   \n",
      "\n",
      "    total_reward  \n",
      "0       0.105244  \n",
      "1       0.105244  \n",
      "2       0.105244  \n",
      "3       0.105244  \n",
      "4       0.105244  \n",
      "5       0.105244  \n",
      "6       0.105244  \n",
      "7       0.105244  \n",
      "8       0.105244  \n",
      "9       0.105244  \n",
      "10      0.105244  \n",
      "11      0.105244  \n",
      "12      0.105244  \n",
      "13      0.105244  \n",
      "14      0.105244  \n",
      "15      0.105244  \n",
      "16      0.105244  \n",
      "17      0.105244  \n",
      "18      0.105244  \n",
      "19      0.105244  \n"
     ]
    }
   ],
   "source": [
    "# --- 1. Initialize agent with the BEST hyperparameters from Optuna ---\n",
    "# (best_hps should be the dictionary you got from your Optuna study)\n",
    "print(f\"Loading best model with hyperparameters: {best_hps}\")\n",
    "trained_agent = PPOAgent(\n",
    "    state_size=state_shape, \n",
    "    n_actions=n_actions,\n",
    "    **best_hps  \n",
    ")\n",
    "\n",
    "# --- 2. Load the saved model weights ---\n",
    "MODEL_SAVE_PATH = \"models/ppo_trading_agent_v2.pth\"\n",
    "trained_agent.network.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "# Set the network to evaluation mode (this is correct)\n",
    "trained_agent.network.eval()\n",
    "\n",
    "print(\"--- Evaluating final trained agent on TEST SET ---\")\n",
    "\n",
    "# --- 3. Evaluate the agent on the unseen TEST set ---\n",
    "df_results = evaluate_agent(\n",
    "    trained_agent,\n",
    "    env_validation,  \n",
    "    num_episodes=20,\n",
    "    render=True,\n",
    "    csv_path=eval_folder / \"evaluation_results.csv\",\n",
    "    renderer_logs_dir=eval_folder / \"render_logs\"\n",
    ")\n",
    "\n",
    "print(\"--- Final Evaluation Complete ---\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0627b698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>portfolio_return</th>\n",
       "      <th>market_return</th>\n",
       "      <th>excess_return</th>\n",
       "      <th>steps</th>\n",
       "      <th>total_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0863</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0864</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0864</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0864</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0864</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.0864</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.0864</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.0863</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.0864</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.0863</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0864</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.105244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode  portfolio_return  market_return  excess_return  steps  \\\n",
       "0         1            0.0863         0.0478         0.0385   2160   \n",
       "1         2            0.0862         0.0478         0.0384   2160   \n",
       "2         3            0.0864         0.0478         0.0386   2160   \n",
       "3         4            0.0864         0.0478         0.0386   2160   \n",
       "4         5            0.0862         0.0478         0.0384   2160   \n",
       "5         6            0.0862         0.0478         0.0384   2160   \n",
       "6         7            0.0864         0.0478         0.0386   2160   \n",
       "7         8            0.0862         0.0478         0.0384   2160   \n",
       "8         9            0.0862         0.0478         0.0384   2160   \n",
       "9        10            0.0862         0.0478         0.0384   2160   \n",
       "10       11            0.0864         0.0478         0.0386   2160   \n",
       "11       12            0.0862         0.0478         0.0384   2160   \n",
       "12       13            0.0864         0.0478         0.0386   2160   \n",
       "13       14            0.0864         0.0478         0.0386   2160   \n",
       "14       15            0.0863         0.0478         0.0385   2160   \n",
       "15       16            0.0864         0.0478         0.0386   2160   \n",
       "16       17            0.0863         0.0478         0.0385   2160   \n",
       "17       18            0.0862         0.0478         0.0384   2160   \n",
       "18       19            0.0862         0.0478         0.0384   2160   \n",
       "19       20            0.0864         0.0478         0.0386   2160   \n",
       "\n",
       "    total_reward  \n",
       "0       0.105244  \n",
       "1       0.105244  \n",
       "2       0.105244  \n",
       "3       0.105244  \n",
       "4       0.105244  \n",
       "5       0.105244  \n",
       "6       0.105244  \n",
       "7       0.105244  \n",
       "8       0.105244  \n",
       "9       0.105244  \n",
       "10      0.105244  \n",
       "11      0.105244  \n",
       "12      0.105244  \n",
       "13      0.105244  \n",
       "14      0.105244  \n",
       "15      0.105244  \n",
       "16      0.105244  \n",
       "17      0.105244  \n",
       "18      0.105244  \n",
       "19      0.105244  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2eefed",
   "metadata": {},
   "source": [
    "## üìà Conclusion: PPO Trading Agent Performance Analysis\n",
    "\n",
    "The Reinforcement Learning project successfully implemented and trained a PPO-based trading agent on **BTC/USDT** hourly data. The final evaluation on the unseen test set demonstrates that the agent developed a profitable strategy that **significantly outperformed the market baseline**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. üéØ Final Agent Performance (Test Set)\n",
    "\n",
    "The critical measure of success for a trading agent is the **Excess Return**, which is the portfolio return minus the market's return over the same period.\n",
    "\n",
    "| Metric | Random Agent (Baseline) | PPO Agent (Final Model) |\n",
    "| :--- | :---: | :---: |\n",
    "| **Market Return** (BTC/USDT) | -8.65% | **4.78%** |\n",
    "| **Portfolio Return** (Mean) | Varies (e.g., -6.70% to 7.25%) | **8.63%** |\n",
    "| **Excess Return** (Mean) | Varies (e.g., -10.73% to 15.90%) | **3.85%** |\n",
    "\n",
    "* **Market Return Discrepancy:** Note that the Market Return printed for the Random Agent is **-8.65%**, while for the Final Agent's evaluation it is **4.78%**. This suggests the Random Agent was incorrectly evaluated on the training/validation data, **not the final test set**, as the final evaluation consistently reports a **4.78%** market return (from October 2025 data slice).\n",
    "* **Result Interpretation:** Based on the **Final Evaluation**, the market (BTC/USDT) returned an average of **+4.78%** over the test period. The PPO agent achieved an average **Portfolio Return of +8.63%**, resulting in a robust **Excess Return of +3.85%**. This is a solid, positive result, indicating the agent learned a policy that successfully generated alpha (return above the benchmark).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚öôÔ∏è Optimal Hyperparameters\n",
    "\n",
    "The **Optuna** hyperparameter search identified the following optimal set of parameters (from Trial 6) which were used to train the final agent:\n",
    "\n",
    "* **Learning Rate (`lr`):** $4.25 \\times 10^{-5}$\n",
    "* **Gamma (`gamma`):** 0.99\n",
    "* **GAE Lambda (`gae_lambda`):** 0.950\n",
    "* **Entropy Beta (`entropy_beta`):** $1.69 \\times 10^{-3}$\n",
    "* **PPO Epochs (`ppo_epochs`):** 8\n",
    "* **Batch Size (`batch_size`):** 128\n",
    "* **Network Size (`hidden_size`):** 256\n",
    "* **Network Layers (`n_layers`):** 4\n",
    "\n",
    "The low learning rate and the gamma value close to 1 are typical for financial time series, suggesting the model needs a **long-term view** (high gamma) and **stable, small updates** (low LR) to navigate market complexities.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. üß† Agent Architecture & Training Success\n",
    "\n",
    "* **PPO for Stability:** The choice of **Proximal Policy Optimization (PPO)** proved effective, as it is known for its stability. The successful non-negative excess return suggests that the clipping mechanism and value estimation successfully stabilized the learning process. * **CNN-Based Feature Extraction:** The use of the `ActorCriticNetwork` incorporating **Convolutional Neural Network (CNN)** layers to process the time-series data window (size 48) was crucial. This architecture allowed the agent to extract spatial and temporal patterns from the features (like RSI, MACD, Sharpe Ratio, etc.) over the 48-hour window, which is likely key to its superior performance over the random baseline.\n",
    "* **Custom Reward Function:** The implementation of the `custom_reward` function, which included a subtle **neutrality penalty**, successfully mitigated the initial tendency of the agent to stay passive. This penalty incentivized the agent to take long or short positions when the projected gain outweighed the small risk of being wrong, leading to a more active and profitable policy.\n",
    "\n",
    "In conclusion, the PPO-powered trading agent achieved its goal by successfully developing a policy that generated **3.85% alpha** over the market benchmark on the unseen test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
