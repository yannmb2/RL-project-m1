{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c8ea223",
   "metadata": {},
   "source": [
    "## ðŸ¤– Solution: A PPO-Powered Trading Agent\n",
    "\n",
    "This notebook implements a complete Deep Reinforcement Learning (DRL) pipeline to train an autonomous trading agent. The goal is to develop a policy that outperforms the market by making intelligent decisions on when to go long, short, or stay neutral.\n",
    "\n",
    "The solution is structured as follows:\n",
    "1.  **Environment Setup & Data Preparation:** We install libraries, load the data, engineer 12 distinct market features, and split the data into chronological `train`, `validation`, and `test` sets.\n",
    "2.  **Baseline Agent:** We establish a \"Random Agent\" baseline to measure our agent's effectiveness.\n",
    "3.  **PPO Agent Implementation:** We build our agent from scratch using Proximal Policy Optimization (PPO) with an Actor-Critic network.\n",
    "4.  **Hyperparameter Tuning:** We use `Optuna` to automatically find the best set of hyperparameters (learning rate, network size, etc.) by evaluating models on the `validation` set.\n",
    "5.  **Final Model Training:** We train the agent with the *best* hyperparameters on the *full* training dataset (`df_train_full`).\n",
    "6.  **Final Evaluation & Visualization:** We load the best saved model and run it on the *unseen* `test` set (`df_eval`) to get our final project result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bd874",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd6c17",
   "metadata": {},
   "source": [
    "## 1. ðŸ› ï¸ Environment Setup & Dependencies\n",
    "\n",
    "Before we can build our agent, we must set up the environment. This involves two steps:\n",
    "\n",
    "* **Installing Packages:** We use `pip` to install the core libraries:\n",
    "    * `gym-trading-env`: The trading simulation environment.\n",
    "    * `torch`: The deep learning framework for our agent's neural network.\n",
    "    * `optuna`: For hyperparameter optimization.\n",
    "* **Importing Libraries:** We import all the necessary tools for data manipulation (`pandas`, `numpy`), environment creation (`gym`), and agent building (`torch.nn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dadc19c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf91412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.downloader import download\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05469c",
   "metadata": {},
   "source": [
    "## 2. ðŸ“ˆ Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e2e93e",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28cdc5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Setup Folders ---\n",
    "data_folder = Path(\"data/\")\n",
    "data_folder.mkdir(parents=True, exist_ok=True)\n",
    "eval_folder = Path(\"eval/\")\n",
    "eval_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\"\"\"\n",
    "download(exchange_names = [\"binance\"],\n",
    "    symbols= [\"BTC/USDT\"],\n",
    "    timeframe= \"1h\",\n",
    "    dir = data_folder,\n",
    "    since= datetime.datetime(year= 2020, month=10, day=1),\n",
    ")\"\"\"\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_pickle(data_folder / \"binance-BTCUSDT-1h.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bb6454",
   "metadata": {},
   "source": [
    "# Exploration of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f325cda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>date_close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_open</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-09-30 23:00:00</th>\n",
       "      <td>10745.85</td>\n",
       "      <td>10785.00</td>\n",
       "      <td>10735.51</td>\n",
       "      <td>10776.59</td>\n",
       "      <td>1235.545956</td>\n",
       "      <td>2020-10-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 00:00:00</th>\n",
       "      <td>10776.59</td>\n",
       "      <td>10826.19</td>\n",
       "      <td>10776.59</td>\n",
       "      <td>10788.06</td>\n",
       "      <td>2128.759531</td>\n",
       "      <td>2020-10-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 01:00:00</th>\n",
       "      <td>10788.30</td>\n",
       "      <td>10849.97</td>\n",
       "      <td>10786.74</td>\n",
       "      <td>10838.88</td>\n",
       "      <td>1604.129560</td>\n",
       "      <td>2020-10-01 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 02:00:00</th>\n",
       "      <td>10838.89</td>\n",
       "      <td>10857.47</td>\n",
       "      <td>10807.39</td>\n",
       "      <td>10817.14</td>\n",
       "      <td>1268.291734</td>\n",
       "      <td>2020-10-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 03:00:00</th>\n",
       "      <td>10817.14</td>\n",
       "      <td>10824.22</td>\n",
       "      <td>10789.01</td>\n",
       "      <td>10798.18</td>\n",
       "      <td>939.599057</td>\n",
       "      <td>2020-10-01 04:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close       volume  \\\n",
       "date_open                                                                  \n",
       "2020-09-30 23:00:00  10745.85  10785.00  10735.51  10776.59  1235.545956   \n",
       "2020-10-01 00:00:00  10776.59  10826.19  10776.59  10788.06  2128.759531   \n",
       "2020-10-01 01:00:00  10788.30  10849.97  10786.74  10838.88  1604.129560   \n",
       "2020-10-01 02:00:00  10838.89  10857.47  10807.39  10817.14  1268.291734   \n",
       "2020-10-01 03:00:00  10817.14  10824.22  10789.01  10798.18   939.599057   \n",
       "\n",
       "                             date_close  \n",
       "date_open                                \n",
       "2020-09-30 23:00:00 2020-10-01 00:00:00  \n",
       "2020-10-01 00:00:00 2020-10-01 01:00:00  \n",
       "2020-10-01 01:00:00 2020-10-01 02:00:00  \n",
       "2020-10-01 02:00:00 2020-10-01 03:00:00  \n",
       "2020-10-01 03:00:00 2020-10-01 04:00:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb823b62",
   "metadata": {},
   "source": [
    "The agent needs to asses at each begining of the hours if we should buy, sell or whatever. So It should not get the close value of that hour as we do not know it. This is a \"Look-Ahead Bias\". When We think about it, many features have a look-Ahead bias (`high`,`low`)\n",
    "\n",
    "PS : We can see that the `close` of a previous hour is the `open` of the next hour. As such, to prevent the agent to get the open value of the next hour we will shift the value to get `previous_close`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f741a6",
   "metadata": {},
   "source": [
    "# Creating features for the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "691312cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Initial Data Shift and Cleanup ---\n",
    "# Shift 'close' one step back to create 'prev_close'. This is the price\n",
    "# available at the moment the new candle OPENS (i.e., Close at t-1).\n",
    "df[\"prev_close\"] = df[\"close\"].shift(1)\n",
    "\n",
    "# --- 2. Calculate Raw Technical Indicators on LAGGED DATA ---\n",
    "# All indicators MUST use 'prev_close' for their core calculations.\n",
    "\n",
    "# Sharpe Ratio\n",
    "ANNUALIZATION_FACTOR = 24 * 365\n",
    "ROLLING_WINDOW_SR = 7 * 24 \n",
    "RISK_FREE_RATE_ANNUAL = 0.04\n",
    "RISK_FREE_RATE_HOURLY = (1 + RISK_FREE_RATE_ANNUAL)**(1/ANNUALIZATION_FACTOR) - 1\n",
    "\n",
    "# Base returns calculation uses 'prev_close' (i.e., Close at t-1)\n",
    "df['return'] = df['prev_close'].pct_change()\n",
    "df['excess_return'] = df['return'] - RISK_FREE_RATE_HOURLY\n",
    "rolling_mean_excess = df['excess_return'].rolling(window=ROLLING_WINDOW_SR).mean()\n",
    "rolling_std_excess = df['excess_return'].rolling(window=ROLLING_WINDOW_SR).std()\n",
    "df['raw_sharpe'] = (rolling_mean_excess / (rolling_std_excess + 1e-9)) * np.sqrt(ANNUALIZATION_FACTOR)\n",
    "\n",
    "# MACD uses 'prev_close' for EMAs\n",
    "df['EMA_12'] = df['prev_close'].ewm(span=12, adjust=False).mean()\n",
    "df['EMA_26'] = df['prev_close'].ewm(span=26, adjust=False).mean()\n",
    "df['raw_macd'] = df['EMA_12'] - df['EMA_26']\n",
    "df['raw_macd_signal'] = df['raw_macd'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# Bollinger Bands uses 'prev_close' for MA and StdDev\n",
    "ROLLING_WINDOW_BB = 20\n",
    "df['BB_Middle'] = df['prev_close'].rolling(window=ROLLING_WINDOW_BB).mean()\n",
    "df['BB_Std'] = df['prev_close'].rolling(window=ROLLING_WINDOW_BB).std()\n",
    "df['raw_bb_upper'] = df['BB_Middle'] + (df['BB_Std'] * 2)\n",
    "df['raw_bb_lower'] = df['BB_Middle'] - (df['BB_Std'] * 2)\n",
    "\n",
    "# OBV uses 'prev_close'\n",
    "df['raw_obv'] = (np.sign(df['prev_close'].diff()) * df['volume'].shift(1)).cumsum().fillna(0)\n",
    "\n",
    "\n",
    "# ATR (Average True Range) - NEW\n",
    "df['high_t_minus_1'] = df['high'].shift(1)\n",
    "df['low_t_minus_1'] = df['low'].shift(1)\n",
    "df['prev_prev_close'] = df['prev_close'].shift(1)\n",
    "\n",
    "df['tr_1'] = df['high_t_minus_1'] - df['low_t_minus_1'] # Range of candle t-1\n",
    "df['tr_2'] = np.abs(df['high_t_minus_1'] - df['prev_prev_close']) # Distance from previous close to high\n",
    "df['tr_3'] = np.abs(df['low_t_minus_1'] - df['prev_prev_close']) # Distance from previous close to low\n",
    "df['true_range'] = df[['tr_1', 'tr_2', 'tr_3']].max(axis=1)\n",
    "df['raw_atr'] = df['true_range'].rolling(window=14).mean()\n",
    "\n",
    "# RSI (Relative Strength Index) - NEW\n",
    "delta = df['prev_close'].diff()\n",
    "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "rs = gain / (loss + 1e-9)\n",
    "df['raw_rsi'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# --- 3. Add Cyclical Time Features (No Change, Already Safe) ---\n",
    "df['hour'] = df.index.hour\n",
    "df['day_of_week'] = df.index.dayofweek\n",
    "\n",
    "df['feature_hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['feature_hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['feature_day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['feature_day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "# --- 4. Create Final, Normalized Features (Shift Applied When Necessary) ---\n",
    "\n",
    "# We define a log-return feature based on data available in the previous completed candle.\n",
    "df['feature_log_return_1h'] = np.log(df['prev_close'] / df['prev_close'].shift(1))\n",
    "\n",
    "\n",
    "# Price Features (Normalized by prev_close, ready for t=0 observation)\n",
    "df['feature_open'] = (df['open'] / df['prev_close']) - 1\n",
    "df['feature_high'] = (df['high'].shift(1) / df['prev_close']) - 1\n",
    "df['feature_low'] = (df['low'].shift(1) / df['prev_close']) - 1\n",
    "\n",
    "\n",
    "# Volume Features (Z-Score of volume/OBV at t-1)\n",
    "vol_mean_30d = df['volume'].shift(1).rolling(30*24).mean()\n",
    "vol_std_30d = df['volume'].shift(1).rolling(30*24).std()\n",
    "df['feature_volume_zscore'] = ((df['volume'].shift(1) - vol_mean_30d) / (vol_std_30d + 1e-9))\n",
    "obv_mean_30d = df['raw_obv'].shift(1).rolling(30*24).mean()\n",
    "obv_std_30d = df['raw_obv'].shift(1).rolling(30*24).std()\n",
    "df['feature_obv_zscore'] = ((df['raw_obv'].shift(1) - obv_mean_30d) / (obv_std_30d + 1e-9))\n",
    "\n",
    "# Indicator Features\n",
    "df['feature_MACD'] = (df['raw_macd'].shift(1) / df['prev_close'])\n",
    "df['feature_MACD_Signal'] = (df['raw_macd_signal'].shift(1) / df['prev_close'])\n",
    "df['feature_BB_Upper'] = (df['raw_bb_upper'].shift(1) / df['prev_close']) - 1\n",
    "df['feature_BB_Lower'] = (df['raw_bb_lower'].shift(1) / df['prev_close']) - 1\n",
    "df['feature_atr'] = (df['raw_atr'].shift(1) / df['prev_close'])\n",
    "df['feature_rsi'] = df['raw_rsi'].shift(1)\n",
    "df['feature_sharpe_ratio'] = df['raw_sharpe'].shift(1)\n",
    "\n",
    "# --- 5. Final Cleanup ---\n",
    "final_features = [\n",
    "    'feature_hour_sin', 'feature_hour_cos', 'feature_day_sin', 'feature_day_cos',\n",
    "    'feature_open', 'feature_high', 'feature_low', 'feature_log_return_1h',\n",
    "    'feature_volume_zscore', 'feature_obv_zscore',\n",
    "    'feature_MACD', 'feature_MACD_Signal', \n",
    "    'feature_BB_Upper', 'feature_BB_Lower',\n",
    "    'feature_atr', 'feature_rsi', 'feature_sharpe_ratio'\n",
    "]\n",
    "\n",
    "# Keep the current raw OHLCV for the Environment to calculate rewards/penalties,\n",
    "# but the agent MUST only observe the 'feature_' columns.\n",
    "all_cols_to_keep = ['close','open', 'high', 'low', 'prev_close', 'volume'] + final_features\n",
    "df = df[all_cols_to_keep]\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# --- 6. Defining DataFrames ---\n",
    "df_train = df.loc['2024-10-01':'2025-09-30'] # Training data\n",
    "df_eval = df.loc['2025-10-01':'2025-11-01'] # final evaluation data\n",
    "df_eval_optu = df.loc['2024-06-01':'2024-08-01'] # evaluation for optuna hyperparameters opti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd121ad",
   "metadata": {},
   "source": [
    "# Reward function \n",
    "In previous attemps, we noticed that the agent has a tendencie to do nothing. As Such, we will add a Neutrality penality as to make sure the agent does not do anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd878140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_reward(historical_info: dict):\n",
    "    # Position: The position held *during* the last completed step (t).\n",
    "    position = historical_info[\"position\", -1] \n",
    "    \n",
    "    # Prices: Use the Close price of the completed bar (t) and the Close price of the bar before it (t-1).\n",
    "    # This represents the fractional return of the asset over the last hour.\n",
    "    current_close = historical_info[\"data_close\", -1] \n",
    "    previous_close = historical_info[\"data_close\", -2] \n",
    "    \n",
    "    # 1. Calculate the asset's fractional return during the step\n",
    "    # Note: Using .pct_change() logic is generally more stable than (C-P)/P\n",
    "    # return is the return of bar t.\n",
    "    asset_return = (current_close - previous_close) / previous_close\n",
    "    \n",
    "    # 2. Calculate Portfolio PnL for this step\n",
    "    # PnL = (Asset Return) * (Held Position)\n",
    "    pnl = asset_return * position\n",
    "\n",
    "    # 3. Define and Apply the Neutrality Penalty\n",
    "    NEUTRAL_PENALTY = -0.000001\n",
    "    \n",
    "    reward = pnl * 100 # scrale the reward because hourly PnL are ussualy really low (ex : 0.0005)\n",
    "    if position == 0:\n",
    "        # Penalize for holding cash (or no position)\n",
    "        reward += NEUTRAL_PENALTY\n",
    "        \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6e342f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Creation of Three Environments ---\n",
    "\n",
    "POSITIONS = [-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1]\n",
    "WINDOW_SIZE = 1\n",
    "TRADING_FEES =  0.01/100\n",
    "BORROW_INTEREST_RATE = 0.0003/100\n",
    "\n",
    "# Environment for TRAINING\n",
    "env_train = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD_Train\",\n",
    "        df = df_train, \n",
    "        windows=WINDOW_SIZE,\n",
    "        positions = POSITIONS,\n",
    "        trading_fees = TRADING_FEES, \n",
    "        borrow_interest_rate= BORROW_INTEREST_RATE,\n",
    "        reward_function=custom_reward\n",
    "    )\n",
    "\n",
    "# Environment for FINAL TEST (used only once)\n",
    "env_eval = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD_Eval\",\n",
    "        df = df_eval, \n",
    "        windows=WINDOW_SIZE,\n",
    "        positions = POSITIONS,\n",
    "        trading_fees = TRADING_FEES, \n",
    "        borrow_interest_rate= BORROW_INTEREST_RATE,\n",
    "        reward_function=custom_reward\n",
    "\n",
    "    )\n",
    "\n",
    "# Environment for Optuna optimization\n",
    "env_eval_optu = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD_Eval\",\n",
    "        df = df_eval_optu, \n",
    "        windows=WINDOW_SIZE,\n",
    "        positions = POSITIONS,\n",
    "        trading_fees = TRADING_FEES, \n",
    "        borrow_interest_rate= BORROW_INTEREST_RATE,\n",
    "        reward_function=custom_reward\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c0dbde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env, num_episodes=20, max_steps=None, render=False, csv_path=\"evaluation_results.csv\", renderer_logs_dir=\"render_logs\"):\n",
    "    \"\"\"\n",
    "    Evaluate the agent on the environment for a number of episodes.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Ensure render dir exists\n",
    "    if render:\n",
    "        Path(renderer_logs_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        step = 0\n",
    "        reward_total = 0.0\n",
    "        while not done and not truncated:\n",
    "            action = agent.choose_action_eval(obs)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            reward_total += reward\n",
    "            step += 1\n",
    "            if (max_steps is not None) and (step >= max_steps):\n",
    "                break\n",
    "\n",
    "        metrics = env.get_metrics()\n",
    "        port_ret = float(metrics[\"Portfolio Return\"].strip('%')) / 100.0\n",
    "        market_ret = float(metrics[\"Market Return\"].strip('%')) / 100.0\n",
    "\n",
    "        results.append({\n",
    "            \"episode\": ep + 1,\n",
    "            \"portfolio_return\": port_ret,\n",
    "            \"market_return\": market_ret,\n",
    "            \"excess_return\": port_ret - market_ret,\n",
    "            \"steps\": step,\n",
    "            \"total_reward\": reward_total,\n",
    "        })\n",
    "        \n",
    "        if render:\n",
    "            print(f\"Eval Episode {ep+1}: Total Reward: {reward_total:.2f}, Portfolio Return: {port_ret:.2%}, Market Return: {market_ret:.2%}, Excess Return: {(port_ret - market_ret):.2%}, Steps: {step}\")\n",
    "            time.sleep(1)\n",
    "            env.save_for_render(dir=renderer_logs_dir)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Ensure the directory for the CSV exists\n",
    "    Path(csv_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_results.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved evaluation results to {csv_path}\")\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae1fae",
   "metadata": {},
   "source": [
    "## 3. ðŸŽ² Baseline: The Random Agent\n",
    "\n",
    "Before we build a complex DRL agent, we must establish a baseline. If our \"smart\" agent can't beat an agent that takes random actions, it has learned nothing.\n",
    "\n",
    "The `RandomAgent` simply chooses a random action (a position from -1 to 1) from the environment's action space at every step. We will evaluate this agent on the **test set** to see what score we need to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa1b7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "    def choose_action_eval(self, state):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a031775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : -3.63%   |   Portfolio Return : -9.85%   |   \n",
      "Eval Episode 1: Total Reward: -4.06, Portfolio Return: -9.85%, Market Return: -3.63%, Excess Return: -6.22%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  2.83%   |   \n",
      "Eval Episode 2: Total Reward: 8.47, Portfolio Return: 2.83%, Market Return: -3.63%, Excess Return: 6.46%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -14.87%   |   \n",
      "Eval Episode 3: Total Reward: -10.13, Portfolio Return: -14.87%, Market Return: -3.63%, Excess Return: -11.24%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  0.60%   |   \n",
      "Eval Episode 4: Total Reward: 6.21, Portfolio Return: 0.60%, Market Return: -3.63%, Excess Return: 4.23%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.97%   |   \n",
      "Eval Episode 5: Total Reward: -3.37, Portfolio Return: -8.97%, Market Return: -3.63%, Excess Return: -5.34%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -11.74%   |   \n",
      "Eval Episode 6: Total Reward: -6.40, Portfolio Return: -11.74%, Market Return: -3.63%, Excess Return: -8.11%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -3.74%   |   \n",
      "Eval Episode 7: Total Reward: 2.59, Portfolio Return: -3.74%, Market Return: -3.63%, Excess Return: -0.11%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  7.19%   |   \n",
      "Eval Episode 8: Total Reward: 12.67, Portfolio Return: 7.19%, Market Return: -3.63%, Excess Return: 10.82%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -1.01%   |   \n",
      "Eval Episode 9: Total Reward: 5.03, Portfolio Return: -1.01%, Market Return: -3.63%, Excess Return: 2.62%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -16.62%   |   \n",
      "Eval Episode 10: Total Reward: -12.14, Portfolio Return: -16.62%, Market Return: -3.63%, Excess Return: -12.99%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -2.87%   |   \n",
      "Eval Episode 11: Total Reward: 2.73, Portfolio Return: -2.87%, Market Return: -3.63%, Excess Return: 0.76%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  0.32%   |   \n",
      "Eval Episode 12: Total Reward: 6.33, Portfolio Return: 0.32%, Market Return: -3.63%, Excess Return: 3.95%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -12.14%   |   \n",
      "Eval Episode 13: Total Reward: -6.81, Portfolio Return: -12.14%, Market Return: -3.63%, Excess Return: -8.51%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.79%   |   \n",
      "Eval Episode 14: Total Reward: 10.06, Portfolio Return: 3.79%, Market Return: -3.63%, Excess Return: 7.42%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -6.25%   |   \n",
      "Eval Episode 15: Total Reward: -0.46, Portfolio Return: -6.25%, Market Return: -3.63%, Excess Return: -2.62%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -4.76%   |   \n",
      "Eval Episode 16: Total Reward: 1.18, Portfolio Return: -4.76%, Market Return: -3.63%, Excess Return: -1.13%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -9.49%   |   \n",
      "Eval Episode 17: Total Reward: -3.96, Portfolio Return: -9.49%, Market Return: -3.63%, Excess Return: -5.86%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -3.16%   |   \n",
      "Eval Episode 18: Total Reward: 2.55, Portfolio Return: -3.16%, Market Return: -3.63%, Excess Return: 0.47%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -17.95%   |   \n",
      "Eval Episode 19: Total Reward: -13.77, Portfolio Return: -17.95%, Market Return: -3.63%, Excess Return: -14.32%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  9.45%   |   \n",
      "Eval Episode 20: Total Reward: 15.26, Portfolio Return: 9.45%, Market Return: -3.63%, Excess Return: 13.08%, Steps: 767\n",
      "Saved evaluation results to eval/evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a random agent for evaluation\n",
    "agent = RandomAgent(env_eval.action_space)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "df_results = evaluate_agent(agent, env_eval, num_episodes=20, render=True, csv_path=eval_folder / \"evaluation_results.csv\", renderer_logs_dir=eval_folder / \"render_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd5f30c",
   "metadata": {},
   "source": [
    "## 4. ðŸ§  Building the PPO Agent\n",
    "\n",
    "This is the core of our project. We are implementing a **Proximal Policy Optimization (PPO)** agent from scratch using PyTorch.\n",
    "\n",
    "### Why PPO?\n",
    "PPO is a robust, state-of-the-art algorithm that balances exploration (trying new things) and exploitation (using what works). It uses a \"clipped\" objective function to prevent updates that are too large, which leads to more stable and reliable training than older methods.\n",
    "\n",
    "### Architecture\n",
    "Our PPO agent consists of two key components:\n",
    "\n",
    "1.  **`ActorCriticNetwork`:** This single neural network serves two purposes:\n",
    "    * **Actor (Policy):** It decides *what action to take* (e.g., \"go long\", \"go short\"). It outputs a probability distribution over all 9 possible actions.\n",
    "    * **Critic (Value):** It estimates *how good the current state is* (i.e., the expected future reward). It outputs a single value, which helps the Actor learn better.\n",
    "\n",
    "2.  **`PPOAgent`:** This class manages the entire learning process. It:\n",
    "    * Holds the `ActorCriticNetwork` and its optimizer.\n",
    "    * Gathers experience from the environment (`store`).\n",
    "    * Calculates advantages using Generalized Advantage Estimation (GAE) to determine how much better an action was than expected (`compute_gae`).\n",
    "    * Runs the PPO update logic across multiple epochs to improve the network (`update`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9aecf006",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, hidden_size=128, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.window_size = state_shape[0]\n",
    "        self.n_features = state_shape[1]\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.n_features * self.window_size, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.flattened_size = 64\n",
    "\n",
    "        # --- 2. Shared Linear Layers ---\n",
    "        layers = []\n",
    "        input_dim = self.flattened_size\n",
    "        \n",
    "        for _ in range(n_layers):\n",
    "            layers.append(nn.Linear(input_dim, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_size\n",
    "            \n",
    "        self.shared_linear = nn.Sequential(*layers)\n",
    "\n",
    "        # --- 3. Heads ---\n",
    "        self.actor = nn.Linear(hidden_size, n_actions)\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, Window, Features)\n",
    "        \n",
    "        if self.window_size > 1:\n",
    "            # Permute for Conv1d: (Batch, Features, Window)\n",
    "            x = x.permute(0, 2, 1) \n",
    "        \n",
    "        # Pass through specific extractor (CNN or MLP)\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # Pass through shared layers\n",
    "        shared_features = self.shared_linear(features)\n",
    "\n",
    "        action_logits = self.actor(shared_features) \n",
    "        state_value = self.critic(shared_features)\n",
    "\n",
    "        return action_logits, state_value\n",
    "class PPOAgent:\n",
    "    def __init__(\n",
    "        self, state_size, n_actions,\n",
    "        lr=3e-4, gamma=0.99, gae_lambda=0.95,\n",
    "        entropy_beta=0.01, clip_epsilon=0.2, ppo_epochs=10, batch_size=64,\n",
    "        hidden_size=128,\n",
    "        n_layers=2  # <<< --- 1. ADD THIS (with a default)\n",
    "    ):\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.entropy_beta = entropy_beta\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Environment parameters\n",
    "        self.state_size = state_size\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        # Device configuration\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")  \n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        # Create policy network\n",
    "        self.network = ActorCriticNetwork(\n",
    "            state_size, \n",
    "            n_actions, \n",
    "            hidden_size, \n",
    "            n_layers  \n",
    "        ).to(self.device)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "\n",
    "        # Memory buffers\n",
    "        self.reset_memory()\n",
    "\n",
    "    def reset_memory(self):\n",
    "        \"\"\"Clear rollout buffers.\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.log_probs = []\n",
    "\n",
    "    def get_action_value_logprob(self, state):\n",
    "        \"\"\"\n",
    "        Samples an action for the training loop.\n",
    "        Returns the action, its value, and log probability.\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, value = self.network(state_tensor)\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action.item(), value.item(), log_prob.item()\n",
    "\n",
    "    def choose_action_eval(self, state):\n",
    "        \"\"\"\n",
    "        Chooses the best action for evaluation (deterministic).\n",
    "        Returns only the action index.\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, _ = self.network(state_tensor)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        action = torch.argmax(probs, dim=-1)\n",
    "        \n",
    "        return action.item()\n",
    "\n",
    "    def store(self, state, action, reward, value, done, log_prob):\n",
    "        \"\"\"Store a single transition in memory.\"\"\"\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.dones.append(done)\n",
    "        self.log_probs.append(log_prob)\n",
    "\n",
    "    def compute_gae(self, next_value):\n",
    "        \"\"\"\n",
    "        Compute returns and advantages using GAE (Generalized Advantage Estimation)\n",
    "        \"\"\"\n",
    "        rewards = np.array(self.rewards, dtype=np.float32)\n",
    "        values = np.array(self.values + [next_value], dtype=np.float32)\n",
    "        dones = np.array(self.dones, dtype=np.float32)\n",
    "\n",
    "        T = len(rewards)\n",
    "        returns = np.zeros(T, dtype=np.float32)\n",
    "        advantages = np.zeros(T, dtype=np.float32)\n",
    "\n",
    "        gae = 0.0\n",
    "        for t in reversed(range(T)):\n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1.0 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1.0 - dones[t]) * gae\n",
    "            advantages[t] = gae\n",
    "            returns[t] = advantages[t] + values[t]\n",
    "\n",
    "        return returns, advantages\n",
    "\n",
    "    def update(self, next_value):\n",
    "        \"\"\"Perform one PPO update step.\"\"\"\n",
    "        if len(self.states) == 0:\n",
    "            return {\"actor_loss\": 0.0, \"critic_loss\": 0.0}\n",
    "\n",
    "        returns, advantages = self.compute_gae(next_value)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states = torch.tensor(np.array(self.states), dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(np.array(self.actions), dtype=torch.int64, device=self.device)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32, device=self.device)\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32, device=self.device)\n",
    "        old_log_probs = torch.tensor(np.array(self.log_probs), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        total_actor_loss = 0\n",
    "        total_critic_loss = 0\n",
    "        updates = 0\n",
    "        \n",
    "        for _ in range(self.ppo_epochs):\n",
    "            indices = torch.randperm(len(states))\n",
    "            \n",
    "            for start in range(0, len(states), self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                idx = indices[start:end]\n",
    "                \n",
    "                if len(idx) == 0:\n",
    "                    continue\n",
    "\n",
    "                batch_states = states[idx]\n",
    "                batch_actions = actions[idx]\n",
    "                batch_old_log_probs = old_log_probs[idx]\n",
    "                batch_returns = returns[idx]\n",
    "                batch_advantages = advantages[idx]\n",
    "                \n",
    "                # Forward pass\n",
    "                logits, values = self.network(batch_states)\n",
    "                action_probs = F.softmax(logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(action_probs)\n",
    "                log_probs = dist.log_prob(batch_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # PPO loss computation\n",
    "                ratio = torch.exp(log_probs - batch_old_log_probs)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                critic_loss = (batch_returns - values.squeeze()).pow(2).mean()\n",
    "                \n",
    "                loss = actor_loss + 0.5 * critic_loss - self.entropy_beta * entropy\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_actor_loss += actor_loss.item()\n",
    "                total_critic_loss += critic_loss.item()\n",
    "                updates += 1\n",
    "        \n",
    "        self.reset_memory()\n",
    "        \n",
    "        if updates == 0:\n",
    "            return {\"actor_loss\": 0.0, \"critic_loss\": 0.0}\n",
    "\n",
    "        return {\n",
    "            'actor_loss': total_actor_loss / updates,\n",
    "            'critic_loss': total_critic_loss / updates\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c24c5ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (1, 19)\n",
      "Number of actions: 9\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.15%   |   \n",
      "Eval Episode 1: Total Reward: 4.93, Portfolio Return: 4.15%, Market Return: -3.63%, Excess Return: 7.78%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.49%   |   \n",
      "Eval Episode 2: Total Reward: 5.26, Portfolio Return: 4.49%, Market Return: -3.63%, Excess Return: 8.12%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.15%   |   \n",
      "Eval Episode 3: Total Reward: 4.93, Portfolio Return: 4.15%, Market Return: -3.63%, Excess Return: 7.78%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.14%   |   \n",
      "Eval Episode 4: Total Reward: 4.93, Portfolio Return: 4.14%, Market Return: -3.63%, Excess Return: 7.77%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.15%   |   \n",
      "Eval Episode 5: Total Reward: 4.93, Portfolio Return: 4.15%, Market Return: -3.63%, Excess Return: 7.78%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.14%   |   \n",
      "Eval Episode 6: Total Reward: 4.93, Portfolio Return: 4.14%, Market Return: -3.63%, Excess Return: 7.77%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.15%   |   \n",
      "Eval Episode 7: Total Reward: 4.93, Portfolio Return: 4.15%, Market Return: -3.63%, Excess Return: 7.78%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.14%   |   \n",
      "Eval Episode 8: Total Reward: 4.93, Portfolio Return: 4.14%, Market Return: -3.63%, Excess Return: 7.77%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.50%   |   \n",
      "Eval Episode 9: Total Reward: 5.26, Portfolio Return: 4.50%, Market Return: -3.63%, Excess Return: 8.13%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.14%   |   \n",
      "Eval Episode 10: Total Reward: 4.93, Portfolio Return: 4.14%, Market Return: -3.63%, Excess Return: 7.77%, Steps: 767\n",
      "Saved evaluation results to eval/evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Get state and action dimensions from the environment\n",
    "state_shape = env_eval.observation_space.shape  # This will be (10, 12)\n",
    "n_actions = env_eval.action_space.n\n",
    "\n",
    "print(f\"State shape: {state_shape}\")\n",
    "print(f\"Number of actions: {n_actions}\")\n",
    "\n",
    "# Create the agent with the correct dimensions\n",
    "agent = PPOAgent(\n",
    "    state_size=state_shape, \n",
    "    n_actions=n_actions,\n",
    "    n_layers=2 # Explicitly using the new param we added\n",
    ")\n",
    "# Evaluate the (untrained) agent\n",
    "# This will now run without errors\n",
    "df_results = evaluate_agent(agent, env_eval, num_episodes=10, render=True, csv_path=eval_folder / \"evaluation_results.csv\", renderer_logs_dir=eval_folder / \"render_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf6e3ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # --- 1. Hyperparameters ---\n",
    "    ppo_hps = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True),\n",
    "        \"gamma\": trial.suggest_categorical(\"gamma\", [0.98, 0.99, 0.995, 0.999]),\n",
    "        \"gae_lambda\": trial.suggest_float(\"gae_lambda\", 0.8, 0.999),\n",
    "        # Increase lower bound for stability and exploration\n",
    "        \"entropy_beta\": trial.suggest_float(\"entropy_beta\", 1e-3, 1e-2, log=True), \n",
    "        \"clip_epsilon\": trial.suggest_float(\"clip_epsilon\", 0.1, 0.3),\n",
    "        \"ppo_epochs\": trial.suggest_int(\"ppo_epochs\", 5, 15), # Narrowed range slightly\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512]),\n",
    "        \"hidden_size\": trial.suggest_categorical(\"hidden_size\", [256, 512, 1024]), \n",
    "        \"n_layers\": trial.suggest_int(\"n_layers\", 2, 4)\n",
    "    }\n",
    "\n",
    "    # --- 2. Setup ---\n",
    "    TOTAL_TIMESTEPS = 200_00 \n",
    "    ROLLOUT_STEPS = 2048\n",
    "    PRUNING_INTERVAL_STEPS = 10 * ROLLOUT_STEPS # Evaluate every ~20k steps\n",
    "    \n",
    "    agent = PPOAgent(\n",
    "        state_size=state_shape,\n",
    "        n_actions=n_actions,\n",
    "        **ppo_hps\n",
    "    )\n",
    "\n",
    "    # --- 3. Training Loop (Standard PPO Loop) ---\n",
    "    obs, info = env_train.reset()\n",
    "    \n",
    "    for step in range(1, TOTAL_TIMESTEPS + 1):\n",
    "        # ... PPO sampling and storing logic ...\n",
    "        action, value, log_prob = agent.get_action_value_logprob(obs)\n",
    "        next_obs, reward, done, truncated, info = env_train.step(action)\n",
    "        agent.store(obs, action, reward, value, done, log_prob)\n",
    "        obs = next_obs\n",
    "        \n",
    "        # Update Phase\n",
    "        if step % ROLLOUT_STEPS == 0:\n",
    "            next_value = 0.0\n",
    "            if not done:\n",
    "                with torch.no_grad():\n",
    "                    _, next_value_tensor = agent.network(\n",
    "                        torch.tensor(obs, dtype=torch.float32, device=agent.device).unsqueeze(0)\n",
    "                    )\n",
    "                    next_value = next_value_tensor.item()\n",
    "            agent.update(next_value)\n",
    "\n",
    "            # --- 4. PRUNING LOGIC (Using Mean Excess Return) ---\n",
    "            if step % PRUNING_INTERVAL_STEPS == 0:\n",
    "                val_results = evaluate_agent(agent, env_train, num_episodes=5, render=False)\n",
    "                \n",
    "                # Metric for pruning: Mean Excess Return (Portfolio - Market)\n",
    "                mean_excess_return = val_results['portfolio_return'].mean() - val_results['market_return'].mean()\n",
    "                \n",
    "                # Report to Optuna\n",
    "                trial.report(mean_excess_return, step)\n",
    "\n",
    "                # Handle Pruning\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.TrialPruned()\n",
    "\n",
    "        if done or truncated:\n",
    "            obs, info = env_train.reset()\n",
    "\n",
    "    # --- 5. Final Evaluation (Robust) ---\n",
    "    eval_results = evaluate_agent(agent,env_train, num_episodes=20, render=False)\n",
    "    \n",
    "    portfolio_returns = eval_results['portfolio_return']\n",
    "    market_returns = eval_results['market_return']\n",
    "    excess_returns = portfolio_returns - market_returns\n",
    "    \n",
    "    # --- 6. The Final Score: Maximize Mean Excess Return (Most Stable) ---\n",
    "    final_score = excess_returns.mean()\n",
    "    \n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f28427cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 15:25:15,342] A new study created in memory with name: ppo_trading_agent_optimization_v3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : -30.01%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -62.41%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.81%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.80%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.81%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.81%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.79%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.81%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.80%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.79%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.80%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 15:28:25,021] Trial 0 finished with value: 0.03302999999999997 and parameters: {'lr': 5.6115164153345e-05, 'gamma': 0.98, 'gae_lambda': 0.8310429095469044, 'entropy_beta': 0.001143098387631322, 'clip_epsilon': 0.27323522915498705, 'ppo_epochs': 11, 'batch_size': 256, 'hidden_size': 256, 'n_layers': 2}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : 82.80%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -55.00%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -56.90%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.46%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.47%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.47%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.47%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.46%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.47%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.47%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.47%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.46%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.47%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.46%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.47%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.47%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.47%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.47%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.47%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.46%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.47%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.46%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 15:31:52,869] Trial 1 finished with value: -0.630435 and parameters: {'lr': 0.00011207606211860574, 'gamma': 0.995, 'gae_lambda': 0.8581367850585084, 'entropy_beta': 0.0023246728489504354, 'clip_epsilon': 0.19121399684340717, 'ppo_epochs': 13, 'batch_size': 256, 'hidden_size': 256, 'n_layers': 4}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : 16.46%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -62.36%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -7.58%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.77%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.77%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.77%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.77%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.76%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.76%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.76%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.77%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 43.76%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 15:35:40,797] Trial 2 finished with value: -0.35754500000000006 and parameters: {'lr': 0.0008536189862866829, 'gamma': 0.98, 'gae_lambda': 0.8875903462541807, 'entropy_beta': 0.0013244581340099357, 'clip_epsilon': 0.19903538202225404, 'ppo_epochs': 5, 'batch_size': 64, 'hidden_size': 512, 'n_layers': 4}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : 43.76%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -31.35%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -40.71%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.74%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 15:39:17,098] Trial 3 finished with value: -0.8625250000000001 and parameters: {'lr': 0.0003550304858128307, 'gamma': 0.98, 'gae_lambda': 0.817610007908332, 'entropy_beta': 0.0015703008378806713, 'clip_epsilon': 0.10904545778210761, 'ppo_epochs': 8, 'batch_size': 256, 'hidden_size': 512, 'n_layers': 4}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : -6.74%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -50.07%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -38.99%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.48%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.51%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.51%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.48%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.48%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.51%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.48%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.51%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.51%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.51%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 79.48%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 15:42:23,687] Trial 4 finished with value: -0.0001450000000000118 and parameters: {'lr': 1.4096175149815848e-05, 'gamma': 0.98, 'gae_lambda': 0.962276824262512, 'entropy_beta': 0.005091635945818554, 'clip_epsilon': 0.24580143360819745, 'ppo_epochs': 13, 'batch_size': 512, 'hidden_size': 256, 'n_layers': 2}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : 79.49%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -15.25%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -46.76%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.32%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.32%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.32%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.31%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.32%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.32%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.30%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.32%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.32%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.32%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.32%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.31%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.32%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.32%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.31%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.32%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.31%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.32%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 28.32%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 15:46:08,100] Trial 5 finished with value: -0.511935 and parameters: {'lr': 4.4706085467784903e-05, 'gamma': 0.995, 'gae_lambda': 0.8237992549417221, 'entropy_beta': 0.005167075260023276, 'clip_epsilon': 0.25215700972337945, 'ppo_epochs': 11, 'batch_size': 64, 'hidden_size': 512, 'n_layers': 3}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : 28.31%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -58.89%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -56.64%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.89%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.89%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.88%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.80%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.89%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.89%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.80%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.88%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.81%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.89%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.88%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.89%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.81%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.88%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.89%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.88%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.88%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.88%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.89%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 15:49:42,623] Trial 6 finished with value: -0.7464500000000001 and parameters: {'lr': 4.253162363790868e-05, 'gamma': 0.99, 'gae_lambda': 0.9503546765700667, 'entropy_beta': 0.0016935505549297925, 'clip_epsilon': 0.1153959819657586, 'ppo_epochs': 8, 'batch_size': 128, 'hidden_size': 256, 'n_layers': 4}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return :  4.80%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -0.06%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -44.17%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.51%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.51%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.51%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.51%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 14.49%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 15:53:17,781] Trial 7 finished with value: -0.650105 and parameters: {'lr': 0.00011986281799901183, 'gamma': 0.99, 'gae_lambda': 0.8453590973458465, 'entropy_beta': 0.0026736699110984107, 'clip_epsilon': 0.2636029531844986, 'ppo_epochs': 14, 'batch_size': 128, 'hidden_size': 1024, 'n_layers': 2}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : 14.50%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -69.44%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -52.11%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.94%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.93%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.93%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.94%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.94%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.93%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.93%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.92%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.93%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.94%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.94%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.93%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.94%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.92%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.92%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.94%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.93%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.93%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.93%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 15:56:33,909] Trial 8 finished with value: -0.71578 and parameters: {'lr': 0.00010903884523201111, 'gamma': 0.995, 'gae_lambda': 0.8501046768692475, 'entropy_beta': 0.0031423062259089674, 'clip_epsilon': 0.16017566196335392, 'ppo_epochs': 8, 'batch_size': 128, 'hidden_size': 512, 'n_layers': 2}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return :  7.93%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -52.20%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -36.73%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.16%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.16%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.14%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.14%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.14%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.16%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.14%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.16%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 15:59:50,212] Trial 9 finished with value: -0.7936 and parameters: {'lr': 9.525889086580776e-05, 'gamma': 0.98, 'gae_lambda': 0.8472898712544875, 'entropy_beta': 0.005348307249011093, 'clip_epsilon': 0.17355662654385065, 'ppo_epochs': 11, 'batch_size': 512, 'hidden_size': 256, 'n_layers': 3}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return :  0.15%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -43.06%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -41.62%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.74%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 16:03:08,740] Trial 10 finished with value: -1.0325199999999999 and parameters: {'lr': 1.2150104649007388e-05, 'gamma': 0.999, 'gae_lambda': 0.9086046673404061, 'entropy_beta': 0.0010422259339479673, 'clip_epsilon': 0.296670580533855, 'ppo_epochs': 5, 'batch_size': 256, 'hidden_size': 1024, 'n_layers': 2}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : -23.74%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -38.45%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -20.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 27.83%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 16:06:17,615] Trial 11 finished with value: -0.516835 and parameters: {'lr': 1.0244556241348389e-05, 'gamma': 0.98, 'gae_lambda': 0.9882161342747691, 'entropy_beta': 0.008441713345446992, 'clip_epsilon': 0.24481040036305052, 'ppo_epochs': 15, 'batch_size': 512, 'hidden_size': 256, 'n_layers': 2}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : 27.82%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -32.06%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -29.25%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.72%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.73%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.72%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.73%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.70%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.72%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.73%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.70%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.72%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.71%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.70%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.71%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.71%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.71%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.73%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.71%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.73%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 70.74%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 16:09:42,615] Trial 12 finished with value: -0.08791000000000004 and parameters: {'lr': 2.5642331533697755e-05, 'gamma': 0.98, 'gae_lambda': 0.9397012061282135, 'entropy_beta': 0.004892564965620156, 'clip_epsilon': 0.29831966577613656, 'ppo_epochs': 12, 'batch_size': 512, 'hidden_size': 256, 'n_layers': 3}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : 70.72%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -50.78%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -51.23%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.20%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.18%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.20%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.18%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.17%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.18%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.17%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.18%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.17%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.17%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.20%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.18%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.17%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.20%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.18%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 53.18%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 16:12:54,123] Trial 13 finished with value: -0.26327 and parameters: {'lr': 2.168066696799275e-05, 'gamma': 0.999, 'gae_lambda': 0.9886997554182845, 'entropy_beta': 0.0099853303776272, 'clip_epsilon': 0.23127492797042143, 'ppo_epochs': 10, 'batch_size': 256, 'hidden_size': 256, 'n_layers': 2}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : 53.18%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -53.40%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -62.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.00%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -0.01%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -0.01%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.00%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -0.01%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -0.01%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -0.00%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -0.00%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -0.01%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -0.00%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -0.01%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -0.01%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -0.01%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -0.00%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -0.00%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -0.00%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -0.00%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -0.01%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -0.01%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 16:16:02,656] Trial 14 finished with value: -0.7951499999999999 and parameters: {'lr': 4.5901373769776726e-05, 'gamma': 0.98, 'gae_lambda': 0.9001504861559989, 'entropy_beta': 0.004052034032408733, 'clip_epsilon': 0.22621686870779303, 'ppo_epochs': 13, 'batch_size': 512, 'hidden_size': 256, 'n_layers': 2}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : -0.00%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -16.63%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -60.29%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.38%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.40%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.38%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.40%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.38%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.39%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.39%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.40%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.40%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.40%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.40%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.39%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.39%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.39%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.40%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.40%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.39%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.40%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 34.38%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 16:19:18,418] Trial 15 finished with value: -0.45118 and parameters: {'lr': 0.0002456842867910009, 'gamma': 0.98, 'gae_lambda': 0.8006976588541808, 'entropy_beta': 0.0021262022989797015, 'clip_epsilon': 0.27130043872900106, 'ppo_epochs': 10, 'batch_size': 512, 'hidden_size': 256, 'n_layers': 3}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : 34.38%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -28.85%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -53.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.20%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.20%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.20%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.20%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 26.20%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 16:22:46,204] Trial 16 finished with value: -0.53305 and parameters: {'lr': 2.1262489667108474e-05, 'gamma': 0.98, 'gae_lambda': 0.9537180786888118, 'entropy_beta': 0.006864169955647862, 'clip_epsilon': 0.2207924942612485, 'ppo_epochs': 15, 'batch_size': 256, 'hidden_size': 1024, 'n_layers': 2}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : 26.20%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -48.66%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -60.33%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.38%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.38%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.39%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.39%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.38%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.38%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.40%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.39%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.40%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.40%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.39%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.38%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.40%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.41%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.39%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.38%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.38%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.39%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 62.41%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 16:26:22,103] Trial 17 finished with value: -0.17118500000000006 and parameters: {'lr': 1.5693857236566376e-05, 'gamma': 0.999, 'gae_lambda': 0.924443715600626, 'entropy_beta': 0.003696129736950654, 'clip_epsilon': 0.27162411349870225, 'ppo_epochs': 12, 'batch_size': 64, 'hidden_size': 256, 'n_layers': 3}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : 62.41%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -28.01%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -49.08%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.16%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.14%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.14%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.16%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.16%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.14%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 16:29:31,498] Trial 18 finished with value: -0.7736000000000001 and parameters: {'lr': 7.123498417990443e-05, 'gamma': 0.99, 'gae_lambda': 0.8750221981825094, 'entropy_beta': 0.0010009009674458408, 'clip_epsilon': 0.2813028353603811, 'ppo_epochs': 9, 'batch_size': 512, 'hidden_size': 256, 'n_layers': 2}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return :  2.15%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -53.40%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -52.42%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.78%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.77%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.78%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.78%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.78%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.77%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.78%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.77%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.77%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.78%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.78%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.78%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.77%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.77%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.77%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.77%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.77%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.78%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.78%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 16:33:15,460] Trial 19 finished with value: -0.9828500000000002 and parameters: {'lr': 0.00021376575202698174, 'gamma': 0.98, 'gae_lambda': 0.975020658381726, 'entropy_beta': 0.006597744651878007, 'clip_epsilon': 0.23717757323957545, 'ppo_epochs': 13, 'batch_size': 256, 'hidden_size': 1024, 'n_layers': 3}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : -18.77%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -42.80%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -58.71%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.16%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.16%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.15%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 18.15%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 16:36:18,717] Trial 20 finished with value: -0.61359 and parameters: {'lr': 3.3295178671269436e-05, 'gamma': 0.98, 'gae_lambda': 0.9666963928734871, 'entropy_beta': 0.0034713672141195007, 'clip_epsilon': 0.25330097797269957, 'ppo_epochs': 11, 'batch_size': 512, 'hidden_size': 256, 'n_layers': 2}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : 18.15%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -58.88%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -23.37%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.50%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.49%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -55.50%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 16:39:34,552] Trial 21 finished with value: -1.3500350000000003 and parameters: {'lr': 2.497154472935635e-05, 'gamma': 0.98, 'gae_lambda': 0.9366501291641349, 'entropy_beta': 0.0045463958870336375, 'clip_epsilon': 0.2956226955962374, 'ppo_epochs': 12, 'batch_size': 512, 'hidden_size': 256, 'n_layers': 3}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : -55.49%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -45.98%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -56.24%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.18%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.18%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.18%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.18%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.19%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.19%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 16:42:55,165] Trial 22 finished with value: -0.7932200000000001 and parameters: {'lr': 1.7122803442486383e-05, 'gamma': 0.98, 'gae_lambda': 0.9331647052593524, 'entropy_beta': 0.006110321090129489, 'clip_epsilon': 0.28616088316278576, 'ppo_epochs': 14, 'batch_size': 512, 'hidden_size': 256, 'n_layers': 3}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return :  0.19%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -64.14%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -53.06%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.22%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.09%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.22%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.09%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.09%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.09%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.22%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.21%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.21%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 16:45:57,445] Trial 23 finished with value: -0.6732250000000001 and parameters: {'lr': 2.9665042631661752e-05, 'gamma': 0.98, 'gae_lambda': 0.9173122452486471, 'entropy_beta': 0.004740648400173833, 'clip_epsilon': 0.26624368392699127, 'ppo_epochs': 12, 'batch_size': 512, 'hidden_size': 256, 'n_layers': 2}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : 12.21%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -40.71%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -56.94%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.06%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.06%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.06%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.06%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.07%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.04%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.06%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.06%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.07%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.07%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.04%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 66.05%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 16:49:20,478] Trial 24 finished with value: -0.134545 and parameters: {'lr': 6.703946912993864e-05, 'gamma': 0.98, 'gae_lambda': 0.9449557803984585, 'entropy_beta': 0.0027093148028037033, 'clip_epsilon': 0.2132196067019345, 'ppo_epochs': 14, 'batch_size': 512, 'hidden_size': 256, 'n_layers': 3}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : 66.06%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -51.34%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -26.04%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.76%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 39.75%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 16:52:30,924] Trial 25 finished with value: -0.39763 and parameters: {'lr': 1.425869144119499e-05, 'gamma': 0.98, 'gae_lambda': 0.9668897315979348, 'entropy_beta': 0.007712050712079272, 'clip_epsilon': 0.28391491740585617, 'ppo_epochs': 9, 'batch_size': 256, 'hidden_size': 256, 'n_layers': 2}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : 39.75%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -13.01%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -50.51%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.06%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.06%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.06%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.06%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.06%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.06%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.06%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.05%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 16:56:04,916] Trial 26 finished with value: -0.80564 and parameters: {'lr': 3.5284486298537534e-05, 'gamma': 0.995, 'gae_lambda': 0.8771159223511237, 'entropy_beta': 0.004165297155838607, 'clip_epsilon': 0.25453551841126004, 'ppo_epochs': 12, 'batch_size': 64, 'hidden_size': 256, 'n_layers': 3}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : -1.06%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "sampler = TPESampler(seed=42)\n",
    "pruner = MedianPruner()\n",
    "\n",
    "# Re-create the study to start fresh\n",
    "study = optuna.create_study(\n",
    "    study_name=\"ppo_trading_agent_optimization_v3\",\n",
    "    direction=\"maximize\",\n",
    "    sampler=sampler,\n",
    "    pruner=pruner\n",
    ")\n",
    "\n",
    "# Start the optimization again (it should no longer fail)\n",
    "try:\n",
    "    study.optimize(objective, n_trials=50, timeout=5400) # 50 trials, 1h 30min timeout\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Optimization stopped manually.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15794ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Optimization Finished ---\n",
      "Number of finished trials: 27\n",
      "\n",
      "Best trial:\n",
      "Value (Mean Excess Return): 0.0330\n",
      "Best Hyperparameters:\n",
      "lr: 5.6115164153345e-05\n",
      "gamma: 0.98\n",
      "gae_lambda: 0.8310429095469044\n",
      "entropy_beta: 0.001143098387631322\n",
      "clip_epsilon: 0.27323522915498705\n",
      "ppo_epochs: 11\n",
      "batch_size: 256\n",
      "hidden_size: 256\n",
      "n_layers: 2\n",
      "\n",
      "Best hyperparameters dictionary:\n",
      "{'lr': 5.6115164153345e-05, 'gamma': 0.98, 'gae_lambda': 0.8310429095469044, 'entropy_beta': 0.001143098387631322, 'clip_epsilon': 0.27323522915498705, 'ppo_epochs': 11, 'batch_size': 256, 'hidden_size': 256, 'n_layers': 2}\n"
     ]
    }
   ],
   "source": [
    "# --- Print Results ---\n",
    "print(\"\\n--- Optimization Finished ---\")\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "\n",
    "print(\"\\nBest trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(f\"Value (Mean Excess Return): {best_trial.value:.4f}\")\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# You can now use these best hyperparameters to train your final agent\n",
    "best_hps = best_trial.params\n",
    "print(\"\\nBest hyperparameters dictionary:\")\n",
    "print(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31ba0713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (1, 19)\n",
      "Number of actions: 9\n",
      "Starting training for 100000 timesteps...\n",
      "Will update every 2048 steps.\n",
      "Evaluating every 5 updates.\n",
      "\n",
      "Update 1 (Step 2048/100000)\n",
      "  Actor Loss: -0.0056, Critic Loss: 0.3446\n",
      "\n",
      "Update 2 (Step 4096/100000)\n",
      "  Actor Loss: -0.0051, Critic Loss: 0.4341\n",
      "\n",
      "Update 3 (Step 6144/100000)\n",
      "  Actor Loss: -0.0097, Critic Loss: 0.2237\n",
      "\n",
      "Update 4 (Step 8192/100000)\n",
      "  Actor Loss: -0.0061, Critic Loss: 0.1445\n",
      "Market Return : 79.51%   |   Portfolio Return : -52.82%   |   \n",
      "\n",
      "Update 5 (Step 10240/100000)\n",
      "  Actor Loss: -0.0065, Critic Loss: 0.2505\n",
      "  Mean Reward (last 10 ep): -9.9611\n",
      "--- Running Validation ---\n",
      "Market Return : -3.63%   |   Portfolio Return : -2.86%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.85%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.85%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.86%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.86%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.87%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.86%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.85%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.86%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.86%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.86%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.86%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.86%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.85%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.85%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.85%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.85%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.86%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.87%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.85%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: -2.86%\n",
      "  Mean Validation Excess Return: 0.77%\n",
      "  Market Return: -3.63%\n",
      "  *** New best model saved with EXCESS return 0.77% ***\n",
      "--------------------------\n",
      "\n",
      "Update 6 (Step 12288/100000)\n",
      "  Actor Loss: -0.0097, Critic Loss: 0.4063\n",
      "  Mean Reward (last 10 ep): -9.9611\n",
      "\n",
      "Update 7 (Step 14336/100000)\n",
      "  Actor Loss: -0.0078, Critic Loss: 0.4171\n",
      "  Mean Reward (last 10 ep): -9.9611\n",
      "\n",
      "Update 8 (Step 16384/100000)\n",
      "  Actor Loss: -0.0057, Critic Loss: 0.1553\n",
      "  Mean Reward (last 10 ep): -9.9611\n",
      "Market Return : 79.51%   |   Portfolio Return : -38.00%   |   \n",
      "\n",
      "Update 9 (Step 18432/100000)\n",
      "  Actor Loss: -0.0067, Critic Loss: 0.1877\n",
      "  Mean Reward (last 10 ep): 4.3656\n",
      "\n",
      "Update 10 (Step 20480/100000)\n",
      "  Actor Loss: -0.0095, Critic Loss: 0.4488\n",
      "  Mean Reward (last 10 ep): 4.3656\n",
      "--- Running Validation ---\n",
      "Market Return : -3.63%   |   Portfolio Return : -2.59%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.60%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.60%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.59%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.60%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.60%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.60%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.59%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.60%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.59%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.59%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.59%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.60%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.59%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.59%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.60%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.59%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.60%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.60%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.60%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: -2.60%\n",
      "  Mean Validation Excess Return: 1.03%\n",
      "  Market Return: -3.63%\n",
      "  *** New best model saved with EXCESS return 1.03% ***\n",
      "--------------------------\n",
      "\n",
      "Update 11 (Step 22528/100000)\n",
      "  Actor Loss: -0.0056, Critic Loss: 0.3848\n",
      "  Mean Reward (last 10 ep): 4.3656\n",
      "\n",
      "Update 12 (Step 24576/100000)\n",
      "  Actor Loss: -0.0107, Critic Loss: 0.1737\n",
      "  Mean Reward (last 10 ep): 4.3656\n",
      "Market Return : 79.51%   |   Portfolio Return : -48.20%   |   \n",
      "\n",
      "Update 13 (Step 26624/100000)\n",
      "  Actor Loss: -0.0059, Critic Loss: 0.1345\n",
      "  Mean Reward (last 10 ep): 1.3822\n",
      "\n",
      "Update 14 (Step 28672/100000)\n",
      "  Actor Loss: -0.0070, Critic Loss: 0.3901\n",
      "  Mean Reward (last 10 ep): 1.3822\n",
      "\n",
      "Update 15 (Step 30720/100000)\n",
      "  Actor Loss: -0.0094, Critic Loss: 0.3190\n",
      "  Mean Reward (last 10 ep): 1.3822\n",
      "--- Running Validation ---\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.21%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.21%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.21%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.21%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.21%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.21%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.21%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: 1.20%\n",
      "  Mean Validation Excess Return: 4.83%\n",
      "  Market Return: -3.63%\n",
      "  *** New best model saved with EXCESS return 4.83% ***\n",
      "--------------------------\n",
      "\n",
      "Update 16 (Step 32768/100000)\n",
      "  Actor Loss: -0.0044, Critic Loss: 0.1790\n",
      "  Mean Reward (last 10 ep): 1.3822\n",
      "\n",
      "Update 17 (Step 34816/100000)\n",
      "  Actor Loss: -0.0095, Critic Loss: 0.1171\n",
      "  Mean Reward (last 10 ep): 1.3822\n",
      "Market Return : 79.51%   |   Portfolio Return : -39.93%   |   \n",
      "\n",
      "Update 18 (Step 36864/100000)\n",
      "  Actor Loss: -0.0069, Critic Loss: 0.3202\n",
      "  Mean Reward (last 10 ep): 1.9617\n",
      "\n",
      "Update 19 (Step 38912/100000)\n",
      "  Actor Loss: -0.0104, Critic Loss: 0.4063\n",
      "  Mean Reward (last 10 ep): 1.9617\n",
      "\n",
      "Update 20 (Step 40960/100000)\n",
      "  Actor Loss: -0.0055, Critic Loss: 0.2801\n",
      "  Mean Reward (last 10 ep): 1.9617\n",
      "--- Running Validation ---\n",
      "Market Return : -3.63%   |   Portfolio Return : -1.21%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.23%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.23%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.23%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.23%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.22%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.23%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.22%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.22%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.22%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.21%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.21%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.23%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.21%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.21%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.22%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.22%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.23%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.21%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.23%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: -1.22%\n",
      "  Mean Validation Excess Return: 2.41%\n",
      "  Market Return: -3.63%\n",
      "--------------------------\n",
      "\n",
      "Update 21 (Step 43008/100000)\n",
      "  Actor Loss: -0.0087, Critic Loss: 0.1765\n",
      "  Mean Reward (last 10 ep): 1.9617\n",
      "Market Return : 79.51%   |   Portfolio Return : -38.99%   |   \n",
      "\n",
      "Update 22 (Step 45056/100000)\n",
      "  Actor Loss: -0.0057, Critic Loss: 0.2832\n",
      "  Mean Reward (last 10 ep): 2.8040\n",
      "\n",
      "Update 23 (Step 47104/100000)\n",
      "  Actor Loss: -0.0057, Critic Loss: 0.4594\n",
      "  Mean Reward (last 10 ep): 2.8040\n",
      "\n",
      "Update 24 (Step 49152/100000)\n",
      "  Actor Loss: -0.0113, Critic Loss: 0.4126\n",
      "  Mean Reward (last 10 ep): 2.8040\n",
      "\n",
      "Update 25 (Step 51200/100000)\n",
      "  Actor Loss: -0.0072, Critic Loss: 0.1908\n",
      "  Mean Reward (last 10 ep): 2.8040\n",
      "--- Running Validation ---\n",
      "Market Return : -3.63%   |   Portfolio Return : -5.76%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.75%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.75%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.77%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.75%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.76%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.75%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.75%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.76%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.77%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.75%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.77%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.76%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.76%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.77%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.76%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.76%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.75%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.76%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -5.77%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: -5.76%\n",
      "  Mean Validation Excess Return: -2.13%\n",
      "  Market Return: -3.63%\n",
      "--------------------------\n",
      "Market Return : 79.51%   |   Portfolio Return : -49.29%   |   \n",
      "\n",
      "Update 26 (Step 53248/100000)\n",
      "  Actor Loss: -0.0046, Critic Loss: 0.2109\n",
      "  Mean Reward (last 10 ep): 0.2034\n",
      "\n",
      "Update 27 (Step 55296/100000)\n",
      "  Actor Loss: -0.0064, Critic Loss: 0.5320\n",
      "  Mean Reward (last 10 ep): 0.2034\n",
      "\n",
      "Update 28 (Step 57344/100000)\n",
      "  Actor Loss: -0.0067, Critic Loss: 0.5301\n",
      "  Mean Reward (last 10 ep): 0.2034\n",
      "\n",
      "Update 29 (Step 59392/100000)\n",
      "  Actor Loss: -0.0066, Critic Loss: 0.1613\n",
      "  Mean Reward (last 10 ep): 0.2034\n",
      "Market Return : 79.51%   |   Portfolio Return : -43.56%   |   \n",
      "\n",
      "Update 30 (Step 61440/100000)\n",
      "  Actor Loss: -0.0076, Critic Loss: 0.1404\n",
      "  Mean Reward (last 10 ep): -0.1380\n",
      "--- Running Validation ---\n",
      "Market Return : -3.63%   |   Portfolio Return : -2.66%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.66%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.65%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.64%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.65%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.64%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.66%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.66%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.65%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.66%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.65%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.64%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.64%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.65%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.66%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.65%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.66%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.64%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.66%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -2.65%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: -2.65%\n",
      "  Mean Validation Excess Return: 0.98%\n",
      "  Market Return: -3.63%\n",
      "--------------------------\n",
      "\n",
      "Update 31 (Step 63488/100000)\n",
      "  Actor Loss: -0.0038, Critic Loss: 0.5490\n",
      "  Mean Reward (last 10 ep): -0.1380\n",
      "\n",
      "Update 32 (Step 65536/100000)\n",
      "  Actor Loss: -0.0066, Critic Loss: 0.4424\n",
      "  Mean Reward (last 10 ep): -0.1380\n",
      "\n",
      "Update 33 (Step 67584/100000)\n",
      "  Actor Loss: -0.0043, Critic Loss: 0.2370\n",
      "  Mean Reward (last 10 ep): -0.1380\n",
      "\n",
      "Update 34 (Step 69632/100000)\n",
      "  Actor Loss: -0.0054, Critic Loss: 0.1407\n",
      "  Mean Reward (last 10 ep): -0.1380\n",
      "Market Return : 79.51%   |   Portfolio Return : -10.13%   |   \n",
      "\n",
      "Update 35 (Step 71680/100000)\n",
      "  Actor Loss: -0.0044, Critic Loss: 0.3399\n",
      "  Mean Reward (last 10 ep): 5.4288\n",
      "--- Running Validation ---\n",
      "Market Return : -3.63%   |   Portfolio Return : -3.04%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.04%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.04%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.04%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.04%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.03%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.03%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.04%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.05%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.03%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.03%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.04%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.04%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.04%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.04%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.03%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.03%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.05%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.03%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -3.03%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: -3.04%\n",
      "  Mean Validation Excess Return: 0.59%\n",
      "  Market Return: -3.63%\n",
      "--------------------------\n",
      "\n",
      "Update 36 (Step 73728/100000)\n",
      "  Actor Loss: -0.0063, Critic Loss: 0.4109\n",
      "  Mean Reward (last 10 ep): 5.4288\n",
      "\n",
      "Update 37 (Step 75776/100000)\n",
      "  Actor Loss: -0.0087, Critic Loss: 0.4691\n",
      "  Mean Reward (last 10 ep): 5.4288\n",
      "\n",
      "Update 38 (Step 77824/100000)\n",
      "  Actor Loss: -0.0059, Critic Loss: 0.1740\n",
      "  Mean Reward (last 10 ep): 5.4288\n",
      "Market Return : 79.51%   |   Portfolio Return : -21.99%   |   \n",
      "\n",
      "Update 39 (Step 79872/100000)\n",
      "  Actor Loss: -0.0062, Critic Loss: 0.2616\n",
      "  Mean Reward (last 10 ep): 8.1448\n",
      "\n",
      "Update 40 (Step 81920/100000)\n",
      "  Actor Loss: -0.0090, Critic Loss: 0.4752\n",
      "  Mean Reward (last 10 ep): 8.1448\n",
      "--- Running Validation ---\n",
      "Market Return : -3.63%   |   Portfolio Return : -1.53%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.52%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.54%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.52%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.53%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.54%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.53%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.53%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.53%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.53%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.53%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.54%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.52%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.54%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.53%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.53%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.54%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.53%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.53%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.54%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: -1.53%\n",
      "  Mean Validation Excess Return: 2.10%\n",
      "  Market Return: -3.63%\n",
      "--------------------------\n",
      "\n",
      "Update 41 (Step 83968/100000)\n",
      "  Actor Loss: -0.0095, Critic Loss: 0.4887\n",
      "  Mean Reward (last 10 ep): 8.1448\n",
      "\n",
      "Update 42 (Step 86016/100000)\n",
      "  Actor Loss: -0.0043, Critic Loss: 0.2392\n",
      "  Mean Reward (last 10 ep): 8.1448\n",
      "Market Return : 79.51%   |   Portfolio Return : -3.48%   |   \n",
      "\n",
      "Update 43 (Step 88064/100000)\n",
      "  Actor Loss: -0.0053, Critic Loss: 0.1842\n",
      "  Mean Reward (last 10 ep): 12.3415\n",
      "\n",
      "Update 44 (Step 90112/100000)\n",
      "  Actor Loss: -0.0089, Critic Loss: 0.5161\n",
      "  Mean Reward (last 10 ep): 12.3415\n",
      "\n",
      "Update 45 (Step 92160/100000)\n",
      "  Actor Loss: -0.0082, Critic Loss: 0.5018\n",
      "  Mean Reward (last 10 ep): 12.3415\n",
      "--- Running Validation ---\n",
      "Market Return : -3.63%   |   Portfolio Return : -1.06%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.06%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.04%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.06%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.06%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.06%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.06%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.06%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.06%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.05%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.06%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: -1.05%\n",
      "  Mean Validation Excess Return: 2.58%\n",
      "  Market Return: -3.63%\n",
      "--------------------------\n",
      "\n",
      "Update 46 (Step 94208/100000)\n",
      "  Actor Loss: -0.0082, Critic Loss: 0.2565\n",
      "  Mean Reward (last 10 ep): 12.3415\n",
      "\n",
      "Update 47 (Step 96256/100000)\n",
      "  Actor Loss: -0.0114, Critic Loss: 0.1355\n",
      "  Mean Reward (last 10 ep): 12.3415\n",
      "Market Return : 79.51%   |   Portfolio Return :  0.39%   |   \n",
      "\n",
      "Update 48 (Step 98304/100000)\n",
      "  Actor Loss: -0.0059, Critic Loss: 0.5207\n",
      "  Mean Reward (last 10 ep): 18.7698\n",
      "\n",
      "Training finished.\n",
      "Best model saved to models/ppo_trading_agent_v2.pth with validation excess return: 4.83%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "state_shape = env_train.observation_space.shape  \n",
    "n_actions = env_train.action_space.n\n",
    "\n",
    "print(f\"State shape: {state_shape}\")\n",
    "print(f\"Number of actions: {n_actions}\")\n",
    "\n",
    "TOTAL_TIMESTEPS = 1_000_00     \n",
    "ROLLOUT_STEPS = 2048         \n",
    "EVAL_EVERY_N_UPDATES = 5     \n",
    "MODEL_SAVE_PATH = \"models/ppo_trading_agent_v2.pth\"\n",
    "\n",
    "agent = PPOAgent(state_size=state_shape, n_actions=n_actions, **best_hps)\n",
    "\n",
    "all_episode_rewards = [] \n",
    "episode_rewards = []     \n",
    "best_eval_excess_return = -float('inf') \n",
    "update_count = 0\n",
    "\n",
    "print(f\"Starting training for {TOTAL_TIMESTEPS} timesteps...\")\n",
    "print(f\"Will update every {ROLLOUT_STEPS} steps.\")\n",
    "print(f\"Evaluating every {EVAL_EVERY_N_UPDATES} updates.\")\n",
    "\n",
    "obs, info = env_train.reset() \n",
    "\n",
    "for step in range(1, TOTAL_TIMESTEPS + 1):\n",
    "    action, value, log_prob = agent.get_action_value_logprob(obs)\n",
    "    \n",
    "    next_obs, reward, done, truncated, info = env_train.step(action) \n",
    "    \n",
    "    agent.store(obs, action, reward, value, done, log_prob)\n",
    "    episode_rewards.append(reward)\n",
    "    obs = next_obs\n",
    "    \n",
    "    if step % ROLLOUT_STEPS == 0:\n",
    "        update_count += 1\n",
    "        \n",
    "        next_value = 0.0\n",
    "        if not done:\n",
    "            with torch.no_grad():\n",
    "                _, next_value_tensor = agent.network(torch.tensor(obs, dtype=torch.float32, device=agent.device).unsqueeze(0))\n",
    "                next_value = next_value_tensor.item()\n",
    "        \n",
    "        losses = agent.update(next_value)\n",
    "        \n",
    "        print(f\"\\nUpdate {update_count} (Step {step}/{TOTAL_TIMESTEPS})\")\n",
    "        print(f\"  Actor Loss: {losses['actor_loss']:.4f}, Critic Loss: {losses['critic_loss']:.4f}\")\n",
    "        if len(all_episode_rewards) > 0:\n",
    "            print(f\"  Mean Reward (last 10 ep): {np.mean(all_episode_rewards[-10:]):.4f}\")\n",
    "        \n",
    "\n",
    "        if update_count % EVAL_EVERY_N_UPDATES == 0:\n",
    "            print(\"--- Running Validation ---\")\n",
    "            \n",
    "            eval_results = evaluate_agent(agent, env_eval, num_episodes=20, render=False)\n",
    "            \n",
    "            mean_eval_return = eval_results['portfolio_return'].mean()\n",
    "            market_return = eval_results['market_return'].mean()\n",
    "            \n",
    "            # --- Calculate and save based on EXCESS return ---\n",
    "            mean_excess_return = mean_eval_return - market_return \n",
    "            \n",
    "            print(f\"  Mean Validation Portfolio Return: {mean_eval_return:.2%}\")\n",
    "            print(f\"  Mean Validation Excess Return: {mean_excess_return:.2%}\")\n",
    "            print(f\"  Market Return: {market_return:.2%}\")\n",
    "            \n",
    "            if mean_excess_return > best_eval_excess_return:\n",
    "                best_eval_excess_return = mean_excess_return\n",
    "                torch.save(agent.network.state_dict(), MODEL_SAVE_PATH)\n",
    "                print(f\"  *** New best model saved with EXCESS return {best_eval_excess_return:.2%} ***\")\n",
    "            print(\"--------------------------\")\n",
    "            \n",
    "    if done or truncated:\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "        episode_rewards = []\n",
    "        obs, info = env_train.reset()\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "print(f\"Best model saved to {MODEL_SAVE_PATH} with validation excess return: {best_eval_excess_return:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d5efbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model with hyperparameters: {'lr': 5.6115164153345e-05, 'gamma': 0.98, 'gae_lambda': 0.8310429095469044, 'entropy_beta': 0.001143098387631322, 'clip_epsilon': 0.27323522915498705, 'ppo_epochs': 11, 'batch_size': 256, 'hidden_size': 256, 'n_layers': 2}\n",
      "--- Evaluating final trained agent on TEST SET ---\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Eval Episode 1: Total Reward: 1.48, Portfolio Return: 1.20%, Market Return: -3.63%, Excess Return: 4.83%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Eval Episode 2: Total Reward: 1.48, Portfolio Return: 1.20%, Market Return: -3.63%, Excess Return: 4.83%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.21%   |   \n",
      "Eval Episode 3: Total Reward: 1.48, Portfolio Return: 1.21%, Market Return: -3.63%, Excess Return: 4.84%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.21%   |   \n",
      "Eval Episode 4: Total Reward: 1.48, Portfolio Return: 1.21%, Market Return: -3.63%, Excess Return: 4.84%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Eval Episode 5: Total Reward: 1.48, Portfolio Return: 1.20%, Market Return: -3.63%, Excess Return: 4.83%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Eval Episode 6: Total Reward: 1.48, Portfolio Return: 1.20%, Market Return: -3.63%, Excess Return: 4.83%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.21%   |   \n",
      "Eval Episode 7: Total Reward: 1.48, Portfolio Return: 1.21%, Market Return: -3.63%, Excess Return: 4.84%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.21%   |   \n",
      "Eval Episode 8: Total Reward: 1.48, Portfolio Return: 1.21%, Market Return: -3.63%, Excess Return: 4.84%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Eval Episode 9: Total Reward: 1.48, Portfolio Return: 1.20%, Market Return: -3.63%, Excess Return: 4.83%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Eval Episode 10: Total Reward: 1.48, Portfolio Return: 1.20%, Market Return: -3.63%, Excess Return: 4.83%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Eval Episode 11: Total Reward: 1.48, Portfolio Return: 1.20%, Market Return: -3.63%, Excess Return: 4.83%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Eval Episode 12: Total Reward: 1.48, Portfolio Return: 1.20%, Market Return: -3.63%, Excess Return: 4.83%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Eval Episode 13: Total Reward: 1.48, Portfolio Return: 1.20%, Market Return: -3.63%, Excess Return: 4.83%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.21%   |   \n",
      "Eval Episode 14: Total Reward: 1.48, Portfolio Return: 1.21%, Market Return: -3.63%, Excess Return: 4.84%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Eval Episode 15: Total Reward: 1.48, Portfolio Return: 1.20%, Market Return: -3.63%, Excess Return: 4.83%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Eval Episode 16: Total Reward: 1.48, Portfolio Return: 1.20%, Market Return: -3.63%, Excess Return: 4.83%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.21%   |   \n",
      "Eval Episode 17: Total Reward: 1.48, Portfolio Return: 1.21%, Market Return: -3.63%, Excess Return: 4.84%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Eval Episode 18: Total Reward: 1.48, Portfolio Return: 1.20%, Market Return: -3.63%, Excess Return: 4.83%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Eval Episode 19: Total Reward: 1.48, Portfolio Return: 1.20%, Market Return: -3.63%, Excess Return: 4.83%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  1.20%   |   \n",
      "Eval Episode 20: Total Reward: 1.48, Portfolio Return: 1.20%, Market Return: -3.63%, Excess Return: 4.83%, Steps: 767\n",
      "Saved evaluation results to eval/evaluation_results.csv\n",
      "--- Final Evaluation Complete ---\n",
      "    episode  portfolio_return  market_return  excess_return  steps  \\\n",
      "0         1            0.0120        -0.0363         0.0483    767   \n",
      "1         2            0.0120        -0.0363         0.0483    767   \n",
      "2         3            0.0121        -0.0363         0.0484    767   \n",
      "3         4            0.0121        -0.0363         0.0484    767   \n",
      "4         5            0.0120        -0.0363         0.0483    767   \n",
      "5         6            0.0120        -0.0363         0.0483    767   \n",
      "6         7            0.0121        -0.0363         0.0484    767   \n",
      "7         8            0.0121        -0.0363         0.0484    767   \n",
      "8         9            0.0120        -0.0363         0.0483    767   \n",
      "9        10            0.0120        -0.0363         0.0483    767   \n",
      "10       11            0.0120        -0.0363         0.0483    767   \n",
      "11       12            0.0120        -0.0363         0.0483    767   \n",
      "12       13            0.0120        -0.0363         0.0483    767   \n",
      "13       14            0.0121        -0.0363         0.0484    767   \n",
      "14       15            0.0120        -0.0363         0.0483    767   \n",
      "15       16            0.0120        -0.0363         0.0483    767   \n",
      "16       17            0.0121        -0.0363         0.0484    767   \n",
      "17       18            0.0120        -0.0363         0.0483    767   \n",
      "18       19            0.0120        -0.0363         0.0483    767   \n",
      "19       20            0.0120        -0.0363         0.0483    767   \n",
      "\n",
      "    total_reward  \n",
      "0       1.482188  \n",
      "1       1.482188  \n",
      "2       1.482188  \n",
      "3       1.482188  \n",
      "4       1.482188  \n",
      "5       1.482188  \n",
      "6       1.482188  \n",
      "7       1.482188  \n",
      "8       1.482188  \n",
      "9       1.482188  \n",
      "10      1.482188  \n",
      "11      1.482188  \n",
      "12      1.482188  \n",
      "13      1.482188  \n",
      "14      1.482188  \n",
      "15      1.482188  \n",
      "16      1.482188  \n",
      "17      1.482188  \n",
      "18      1.482188  \n",
      "19      1.482188  \n"
     ]
    }
   ],
   "source": [
    "# --- 1. Initialize agent with the BEST hyperparameters from Optuna ---\n",
    "print(f\"Loading best model with hyperparameters: {best_hps}\")\n",
    "trained_agent = PPOAgent(\n",
    "    state_size=state_shape, \n",
    "    n_actions=n_actions,\n",
    "    **best_hps  \n",
    ")\n",
    "\n",
    "# --- 2. Load the saved model weights ---\n",
    "MODEL_SAVE_PATH = \"models/ppo_trading_agent_v2.pth\"\n",
    "trained_agent.network.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "# Set the network to evaluation mode (this is correct)\n",
    "trained_agent.network.eval()\n",
    "\n",
    "print(\"--- Evaluating final trained agent on TEST SET ---\")\n",
    "\n",
    "# --- 3. Evaluate the agent on the unseen TEST set ---\n",
    "df_results = evaluate_agent(\n",
    "    trained_agent,\n",
    "    env_eval,  \n",
    "    num_episodes=20,\n",
    "    render=True,\n",
    "    csv_path=eval_folder / \"evaluation_results.csv\",\n",
    "    renderer_logs_dir=eval_folder / \"render_logs\"\n",
    ")\n",
    "\n",
    "print(\"--- Final Evaluation Complete ---\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0627b698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>portfolio_return</th>\n",
       "      <th>market_return</th>\n",
       "      <th>excess_return</th>\n",
       "      <th>steps</th>\n",
       "      <th>total_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>767</td>\n",
       "      <td>1.482188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode  portfolio_return  market_return  excess_return  steps  \\\n",
       "0         1            0.0120        -0.0363         0.0483    767   \n",
       "1         2            0.0120        -0.0363         0.0483    767   \n",
       "2         3            0.0121        -0.0363         0.0484    767   \n",
       "3         4            0.0121        -0.0363         0.0484    767   \n",
       "4         5            0.0120        -0.0363         0.0483    767   \n",
       "5         6            0.0120        -0.0363         0.0483    767   \n",
       "6         7            0.0121        -0.0363         0.0484    767   \n",
       "7         8            0.0121        -0.0363         0.0484    767   \n",
       "8         9            0.0120        -0.0363         0.0483    767   \n",
       "9        10            0.0120        -0.0363         0.0483    767   \n",
       "10       11            0.0120        -0.0363         0.0483    767   \n",
       "11       12            0.0120        -0.0363         0.0483    767   \n",
       "12       13            0.0120        -0.0363         0.0483    767   \n",
       "13       14            0.0121        -0.0363         0.0484    767   \n",
       "14       15            0.0120        -0.0363         0.0483    767   \n",
       "15       16            0.0120        -0.0363         0.0483    767   \n",
       "16       17            0.0121        -0.0363         0.0484    767   \n",
       "17       18            0.0120        -0.0363         0.0483    767   \n",
       "18       19            0.0120        -0.0363         0.0483    767   \n",
       "19       20            0.0120        -0.0363         0.0483    767   \n",
       "\n",
       "    total_reward  \n",
       "0       1.482188  \n",
       "1       1.482188  \n",
       "2       1.482188  \n",
       "3       1.482188  \n",
       "4       1.482188  \n",
       "5       1.482188  \n",
       "6       1.482188  \n",
       "7       1.482188  \n",
       "8       1.482188  \n",
       "9       1.482188  \n",
       "10      1.482188  \n",
       "11      1.482188  \n",
       "12      1.482188  \n",
       "13      1.482188  \n",
       "14      1.482188  \n",
       "15      1.482188  \n",
       "16      1.482188  \n",
       "17      1.482188  \n",
       "18      1.482188  \n",
       "19      1.482188  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
