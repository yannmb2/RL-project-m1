{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c8ea223",
   "metadata": {},
   "source": [
    "## ðŸ¤– Solution: A PPO-Powered Trading Agent\n",
    "\n",
    "This notebook implements a complete Deep Reinforcement Learning (DRL) pipeline to train an autonomous trading agent. The goal is to develop a policy that outperforms the market by making intelligent decisions on when to go long, short, or stay neutral.\n",
    "\n",
    "The solution is structured as follows:\n",
    "1.  **Environment Setup & Data Preparation:** We install libraries, load the data, engineer 12 distinct market features, and split the data into chronological `train`, `validation`, and `test` sets.\n",
    "2.  **Baseline Agent:** We establish a \"Random Agent\" baseline to measure our agent's effectiveness.\n",
    "3.  **PPO Agent Implementation:** We build our agent from scratch using Proximal Policy Optimization (PPO) with an Actor-Critic network.\n",
    "4.  **Hyperparameter Tuning:** We use `Optuna` to automatically find the best set of hyperparameters (learning rate, network size, etc.) by evaluating models on the `validation` set.\n",
    "5.  **Final Model Training:** We train the agent with the *best* hyperparameters on the *full* training dataset (`df_train_full`).\n",
    "6.  **Final Evaluation & Visualization:** We load the best saved model and run it on the *unseen* `test` set (`df_eval`) to get our final project result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bd874",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd6c17",
   "metadata": {},
   "source": [
    "## 1. ðŸ› ï¸ Environment Setup & Dependencies\n",
    "\n",
    "Before we can build our agent, we must set up the environment. This involves two steps:\n",
    "\n",
    "* **Installing Packages:** We use `pip` to install the core libraries:\n",
    "    * `gym-trading-env`: The trading simulation environment.\n",
    "    * `torch`: The deep learning framework for our agent's neural network.\n",
    "    * `optuna`: For hyperparameter optimization.\n",
    "* **Importing Libraries:** We import all the necessary tools for data manipulation (`pandas`, `numpy`), environment creation (`gym`), and agent building (`torch.nn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dadc19c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf91412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.downloader import download\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05469c",
   "metadata": {},
   "source": [
    "## 2. ðŸ“ˆ Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e2e93e",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28cdc5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Setup Folders ---\n",
    "data_folder = Path(\"data/\")\n",
    "data_folder.mkdir(parents=True, exist_ok=True)\n",
    "eval_folder = Path(\"eval/\")\n",
    "eval_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\"\"\"\n",
    "download(exchange_names = [\"binance\"],\n",
    "    symbols= [\"BTC/USDT\"],\n",
    "    timeframe= \"1h\",\n",
    "    dir = data_folder,\n",
    "    since= datetime.datetime(year= 2020, month=10, day=1),\n",
    ")\"\"\"\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_pickle(data_folder / \"binance-BTCUSDT-1h.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bb6454",
   "metadata": {},
   "source": [
    "# Exploration of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f325cda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>date_close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_open</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-09-30 23:00:00</th>\n",
       "      <td>10745.85</td>\n",
       "      <td>10785.00</td>\n",
       "      <td>10735.51</td>\n",
       "      <td>10776.59</td>\n",
       "      <td>1235.545956</td>\n",
       "      <td>2020-10-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 00:00:00</th>\n",
       "      <td>10776.59</td>\n",
       "      <td>10826.19</td>\n",
       "      <td>10776.59</td>\n",
       "      <td>10788.06</td>\n",
       "      <td>2128.759531</td>\n",
       "      <td>2020-10-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 01:00:00</th>\n",
       "      <td>10788.30</td>\n",
       "      <td>10849.97</td>\n",
       "      <td>10786.74</td>\n",
       "      <td>10838.88</td>\n",
       "      <td>1604.129560</td>\n",
       "      <td>2020-10-01 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 02:00:00</th>\n",
       "      <td>10838.89</td>\n",
       "      <td>10857.47</td>\n",
       "      <td>10807.39</td>\n",
       "      <td>10817.14</td>\n",
       "      <td>1268.291734</td>\n",
       "      <td>2020-10-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 03:00:00</th>\n",
       "      <td>10817.14</td>\n",
       "      <td>10824.22</td>\n",
       "      <td>10789.01</td>\n",
       "      <td>10798.18</td>\n",
       "      <td>939.599057</td>\n",
       "      <td>2020-10-01 04:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close       volume  \\\n",
       "date_open                                                                  \n",
       "2020-09-30 23:00:00  10745.85  10785.00  10735.51  10776.59  1235.545956   \n",
       "2020-10-01 00:00:00  10776.59  10826.19  10776.59  10788.06  2128.759531   \n",
       "2020-10-01 01:00:00  10788.30  10849.97  10786.74  10838.88  1604.129560   \n",
       "2020-10-01 02:00:00  10838.89  10857.47  10807.39  10817.14  1268.291734   \n",
       "2020-10-01 03:00:00  10817.14  10824.22  10789.01  10798.18   939.599057   \n",
       "\n",
       "                             date_close  \n",
       "date_open                                \n",
       "2020-09-30 23:00:00 2020-10-01 00:00:00  \n",
       "2020-10-01 00:00:00 2020-10-01 01:00:00  \n",
       "2020-10-01 01:00:00 2020-10-01 02:00:00  \n",
       "2020-10-01 02:00:00 2020-10-01 03:00:00  \n",
       "2020-10-01 03:00:00 2020-10-01 04:00:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb823b62",
   "metadata": {},
   "source": [
    "The agent needs to asses at each begining of the hours if we should buy, sell or whatever. So It should not get the close value of that hour as we do not know it. This is a \"Look-Ahead Bias\". When We think about it, many features have a look-Ahead bias (`high`,`low`)\n",
    "\n",
    "PS : We can see that the `close` of a previous hour is the `open` of the next hour. As such, to prevent the agent to get the open value of the next hour we will shift the value to get `previous_close`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f741a6",
   "metadata": {},
   "source": [
    "# Creating features for the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "691312cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Initial Data Shift and Cleanup ---\n",
    "# Shift 'close' one step back to create 'prev_close'. This is the price\n",
    "# available at the moment the new candle OPENS (i.e., Close at t-1).\n",
    "df[\"prev_close\"] = df[\"close\"].shift(1)\n",
    "\n",
    "# --- 2. Calculate Raw Technical Indicators on LAGGED DATA ---\n",
    "# All indicators MUST use 'prev_close' for their core calculations.\n",
    "\n",
    "# Sharpe Ratio\n",
    "ANNUALIZATION_FACTOR = 24 * 365\n",
    "ROLLING_WINDOW_SR = 7 * 24 \n",
    "RISK_FREE_RATE_ANNUAL = 0.04\n",
    "RISK_FREE_RATE_HOURLY = (1 + RISK_FREE_RATE_ANNUAL)**(1/ANNUALIZATION_FACTOR) - 1\n",
    "\n",
    "# Base returns calculation uses 'prev_close' (i.e., Close at t-1)\n",
    "df['return'] = df['prev_close'].pct_change()\n",
    "df['excess_return'] = df['return'] - RISK_FREE_RATE_HOURLY\n",
    "rolling_mean_excess = df['excess_return'].rolling(window=ROLLING_WINDOW_SR).mean()\n",
    "rolling_std_excess = df['excess_return'].rolling(window=ROLLING_WINDOW_SR).std()\n",
    "df['raw_sharpe'] = (rolling_mean_excess / (rolling_std_excess + 1e-9)) * np.sqrt(ANNUALIZATION_FACTOR)\n",
    "\n",
    "# MACD uses 'prev_close' for EMAs\n",
    "df['EMA_12'] = df['prev_close'].ewm(span=12, adjust=False).mean()\n",
    "df['EMA_26'] = df['prev_close'].ewm(span=26, adjust=False).mean()\n",
    "df['raw_macd'] = df['EMA_12'] - df['EMA_26']\n",
    "df['raw_macd_signal'] = df['raw_macd'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# Bollinger Bands uses 'prev_close' for MA and StdDev\n",
    "ROLLING_WINDOW_BB = 20\n",
    "df['BB_Middle'] = df['prev_close'].rolling(window=ROLLING_WINDOW_BB).mean()\n",
    "df['BB_Std'] = df['prev_close'].rolling(window=ROLLING_WINDOW_BB).std()\n",
    "df['raw_bb_upper'] = df['BB_Middle'] + (df['BB_Std'] * 2)\n",
    "df['raw_bb_lower'] = df['BB_Middle'] - (df['BB_Std'] * 2)\n",
    "\n",
    "# OBV uses 'prev_close'\n",
    "df['raw_obv'] = (np.sign(df['prev_close'].diff()) * df['volume'].shift(1)).cumsum().fillna(0)\n",
    "\n",
    "\n",
    "# ATR (Average True Range) - NEW\n",
    "df['high_t_minus_1'] = df['high'].shift(1)\n",
    "df['low_t_minus_1'] = df['low'].shift(1)\n",
    "df['prev_prev_close'] = df['prev_close'].shift(1)\n",
    "\n",
    "df['tr_1'] = df['high_t_minus_1'] - df['low_t_minus_1'] # Range of candle t-1\n",
    "df['tr_2'] = np.abs(df['high_t_minus_1'] - df['prev_prev_close']) # Distance from previous close to high\n",
    "df['tr_3'] = np.abs(df['low_t_minus_1'] - df['prev_prev_close']) # Distance from previous close to low\n",
    "df['true_range'] = df[['tr_1', 'tr_2', 'tr_3']].max(axis=1)\n",
    "df['raw_atr'] = df['true_range'].rolling(window=14).mean()\n",
    "\n",
    "# RSI (Relative Strength Index) - NEW\n",
    "delta = df['prev_close'].diff()\n",
    "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "rs = gain / (loss + 1e-9)\n",
    "df['raw_rsi'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# --- 3. Add Cyclical Time Features (No Change, Already Safe) ---\n",
    "df['hour'] = df.index.hour\n",
    "df['day_of_week'] = df.index.dayofweek\n",
    "\n",
    "df['feature_hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['feature_hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['feature_day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['feature_day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "# --- 4. Create Final, Normalized Features (Shift Applied When Necessary) ---\n",
    "\n",
    "# We define a log-return feature based on data available in the previous completed candle.\n",
    "df['feature_log_return_1h'] = np.log(df['prev_close'] / df['prev_close'].shift(1))\n",
    "\n",
    "\n",
    "# Price Features (Normalized by prev_close, ready for t=0 observation)\n",
    "df['feature_open'] = (df['open'] / df['prev_close']) - 1\n",
    "df['feature_high'] = (df['high'].shift(1) / df['prev_close']) - 1\n",
    "df['feature_low'] = (df['low'].shift(1) / df['prev_close']) - 1\n",
    "\n",
    "\n",
    "# Volume Features (Z-Score of volume/OBV at t-1)\n",
    "vol_mean_30d = df['volume'].shift(1).rolling(30*24).mean()\n",
    "vol_std_30d = df['volume'].shift(1).rolling(30*24).std()\n",
    "df['feature_volume_zscore'] = ((df['volume'].shift(1) - vol_mean_30d) / (vol_std_30d + 1e-9))\n",
    "obv_mean_30d = df['raw_obv'].shift(1).rolling(30*24).mean()\n",
    "obv_std_30d = df['raw_obv'].shift(1).rolling(30*24).std()\n",
    "df['feature_obv_zscore'] = ((df['raw_obv'].shift(1) - obv_mean_30d) / (obv_std_30d + 1e-9))\n",
    "\n",
    "# Indicator Features\n",
    "df['feature_MACD'] = (df['raw_macd'].shift(1) / df['prev_close'])\n",
    "df['feature_MACD_Signal'] = (df['raw_macd_signal'].shift(1) / df['prev_close'])\n",
    "df['feature_BB_Upper'] = (df['raw_bb_upper'].shift(1) / df['prev_close']) - 1\n",
    "df['feature_BB_Lower'] = (df['raw_bb_lower'].shift(1) / df['prev_close']) - 1\n",
    "df['feature_atr'] = (df['raw_atr'].shift(1) / df['prev_close'])\n",
    "df['feature_rsi'] = df['raw_rsi'].shift(1)\n",
    "df['feature_sharpe_ratio'] = df['raw_sharpe'].shift(1)\n",
    "\n",
    "# --- 5. Final Cleanup ---\n",
    "final_features = [\n",
    "    'feature_hour_sin', 'feature_hour_cos', 'feature_day_sin', 'feature_day_cos',\n",
    "    'feature_open', 'feature_high', 'feature_low', 'feature_log_return_1h',\n",
    "    'feature_volume_zscore', 'feature_obv_zscore',\n",
    "    'feature_MACD', 'feature_MACD_Signal', \n",
    "    'feature_BB_Upper', 'feature_BB_Lower',\n",
    "    'feature_atr', 'feature_rsi', 'feature_sharpe_ratio'\n",
    "]\n",
    "\n",
    "# Keep the current raw OHLCV for the Environment to calculate rewards/penalties,\n",
    "# but the agent MUST only observe the 'feature_' columns.\n",
    "all_cols_to_keep = ['close','open', 'high', 'low', 'prev_close', 'volume'] + final_features\n",
    "df = df[all_cols_to_keep]\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# --- 6. Defining DataFrames ---\n",
    "df_train = df.loc['2024-10-01':'2025-09-30'] # Training data\n",
    "df_eval = df.loc['2025-10-01':'2025-11-01'] # final evaluation data\n",
    "df_eval_optu = df.loc['2024-06-01':'2024-08-01'] # evaluation for optuna hyperparameters opti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd121ad",
   "metadata": {},
   "source": [
    "# Reward function \n",
    "In previous attemps, we noticed that the agent has a tendencie to do nothing. As Such, we will add a Neutrality penality as to make sure the agent does not do anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd878140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_reward(historical_info: dict):\n",
    "    # Position: The position held *during* the last completed step (t).\n",
    "    position = historical_info[\"position\", -1] \n",
    "    \n",
    "    # Prices: Use the Close price of the completed bar (t) and the Close price of the bar before it (t-1).\n",
    "    # This represents the fractional return of the asset over the last hour.\n",
    "    current_close = historical_info[\"data_close\", -1] \n",
    "    previous_close = historical_info[\"data_close\", -2] \n",
    "    \n",
    "    # 1. Calculate the asset's fractional return during the step\n",
    "    # Note: Using .pct_change() logic is generally more stable than (C-P)/P\n",
    "    # return is the return of bar t.\n",
    "    asset_return = (current_close - previous_close) / previous_close\n",
    "    \n",
    "    # 2. Calculate Portfolio PnL for this step\n",
    "    # PnL = (Asset Return) * (Held Position)\n",
    "    pnl = asset_return * position\n",
    "\n",
    "    # 3. Define and Apply the Neutrality Penalty\n",
    "    NEUTRAL_PENALTY = -0.000001\n",
    "    \n",
    "    reward = pnl * 100 # scrale the reward because hourly PnL are ussualy really low (ex : 0.0005)\n",
    "    if position == 0:\n",
    "        # Penalize for holding cash (or no position)\n",
    "        reward += NEUTRAL_PENALTY\n",
    "        \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6e342f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Creation of Three Environments ---\n",
    "\n",
    "POSITIONS = [-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1]\n",
    "WINDOW_SIZE = 1\n",
    "TRADING_FEES =  0.01/100\n",
    "BORROW_INTEREST_RATE = 0.0003/100\n",
    "\n",
    "# Environment for TRAINING\n",
    "env_train = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD_Train\",\n",
    "        df = df_train, \n",
    "        windows=WINDOW_SIZE,\n",
    "        positions = POSITIONS,\n",
    "        trading_fees = TRADING_FEES, \n",
    "        borrow_interest_rate= BORROW_INTEREST_RATE,\n",
    "        reward_function=custom_reward\n",
    "    )\n",
    "\n",
    "# Environment for FINAL TEST (used only once)\n",
    "env_eval = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD_Eval\",\n",
    "        df = df_eval, \n",
    "        windows=WINDOW_SIZE,\n",
    "        positions = POSITIONS,\n",
    "        trading_fees = TRADING_FEES, \n",
    "        borrow_interest_rate= BORROW_INTEREST_RATE,\n",
    "        reward_function=custom_reward\n",
    "\n",
    "    )\n",
    "\n",
    "# Environment for Optuna optimization\n",
    "env_eval_optu = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD_Eval\",\n",
    "        df = df_eval_optu, \n",
    "        windows=WINDOW_SIZE,\n",
    "        positions = POSITIONS,\n",
    "        trading_fees = TRADING_FEES, \n",
    "        borrow_interest_rate= BORROW_INTEREST_RATE,\n",
    "        reward_function=custom_reward\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c0dbde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env, num_episodes=20, max_steps=None, render=False, csv_path=\"evaluation_results.csv\", renderer_logs_dir=\"render_logs\"):\n",
    "    \"\"\"\n",
    "    Evaluate the agent on the environment for a number of episodes.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Ensure render dir exists\n",
    "    if render:\n",
    "        Path(renderer_logs_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        step = 0\n",
    "        reward_total = 0.0\n",
    "        while not done and not truncated:\n",
    "            action = agent.choose_action_eval(obs)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            reward_total += reward\n",
    "            step += 1\n",
    "            if (max_steps is not None) and (step >= max_steps):\n",
    "                break\n",
    "\n",
    "        metrics = env.get_metrics()\n",
    "        port_ret = float(metrics[\"Portfolio Return\"].strip('%')) / 100.0\n",
    "        market_ret = float(metrics[\"Market Return\"].strip('%')) / 100.0\n",
    "\n",
    "        results.append({\n",
    "            \"episode\": ep + 1,\n",
    "            \"portfolio_return\": port_ret,\n",
    "            \"market_return\": market_ret,\n",
    "            \"excess_return\": port_ret - market_ret,\n",
    "            \"steps\": step,\n",
    "            \"total_reward\": reward_total,\n",
    "        })\n",
    "        \n",
    "        if render:\n",
    "            print(f\"Eval Episode {ep+1}: Total Reward: {reward_total:.2f}, Portfolio Return: {port_ret:.2%}, Market Return: {market_ret:.2%}, Excess Return: {(port_ret - market_ret):.2%}, Steps: {step}\")\n",
    "            time.sleep(1)\n",
    "            env.save_for_render(dir=renderer_logs_dir)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Ensure the directory for the CSV exists\n",
    "    Path(csv_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_results.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved evaluation results to {csv_path}\")\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae1fae",
   "metadata": {},
   "source": [
    "## 3. ðŸŽ² Baseline: The Random Agent\n",
    "\n",
    "Before we build a complex DRL agent, we must establish a baseline. If our \"smart\" agent can't beat an agent that takes random actions, it has learned nothing.\n",
    "\n",
    "The `RandomAgent` simply chooses a random action (a position from -1 to 1) from the environment's action space at every step. We will evaluate this agent on the **test set** to see what score we need to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa1b7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "    def choose_action_eval(self, state):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a031775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : -3.63%   |   Portfolio Return : -9.85%   |   \n",
      "Eval Episode 1: Total Reward: -4.06, Portfolio Return: -9.85%, Market Return: -3.63%, Excess Return: -6.22%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  2.83%   |   \n",
      "Eval Episode 2: Total Reward: 8.47, Portfolio Return: 2.83%, Market Return: -3.63%, Excess Return: 6.46%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -14.87%   |   \n",
      "Eval Episode 3: Total Reward: -10.13, Portfolio Return: -14.87%, Market Return: -3.63%, Excess Return: -11.24%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  0.60%   |   \n",
      "Eval Episode 4: Total Reward: 6.21, Portfolio Return: 0.60%, Market Return: -3.63%, Excess Return: 4.23%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.97%   |   \n",
      "Eval Episode 5: Total Reward: -3.37, Portfolio Return: -8.97%, Market Return: -3.63%, Excess Return: -5.34%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -11.74%   |   \n",
      "Eval Episode 6: Total Reward: -6.40, Portfolio Return: -11.74%, Market Return: -3.63%, Excess Return: -8.11%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -3.74%   |   \n",
      "Eval Episode 7: Total Reward: 2.59, Portfolio Return: -3.74%, Market Return: -3.63%, Excess Return: -0.11%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  7.19%   |   \n",
      "Eval Episode 8: Total Reward: 12.67, Portfolio Return: 7.19%, Market Return: -3.63%, Excess Return: 10.82%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -1.01%   |   \n",
      "Eval Episode 9: Total Reward: 5.03, Portfolio Return: -1.01%, Market Return: -3.63%, Excess Return: 2.62%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -16.62%   |   \n",
      "Eval Episode 10: Total Reward: -12.14, Portfolio Return: -16.62%, Market Return: -3.63%, Excess Return: -12.99%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -2.87%   |   \n",
      "Eval Episode 11: Total Reward: 2.73, Portfolio Return: -2.87%, Market Return: -3.63%, Excess Return: 0.76%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  0.32%   |   \n",
      "Eval Episode 12: Total Reward: 6.33, Portfolio Return: 0.32%, Market Return: -3.63%, Excess Return: 3.95%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -12.14%   |   \n",
      "Eval Episode 13: Total Reward: -6.81, Portfolio Return: -12.14%, Market Return: -3.63%, Excess Return: -8.51%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.79%   |   \n",
      "Eval Episode 14: Total Reward: 10.06, Portfolio Return: 3.79%, Market Return: -3.63%, Excess Return: 7.42%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -6.25%   |   \n",
      "Eval Episode 15: Total Reward: -0.46, Portfolio Return: -6.25%, Market Return: -3.63%, Excess Return: -2.62%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -4.76%   |   \n",
      "Eval Episode 16: Total Reward: 1.18, Portfolio Return: -4.76%, Market Return: -3.63%, Excess Return: -1.13%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -9.49%   |   \n",
      "Eval Episode 17: Total Reward: -3.96, Portfolio Return: -9.49%, Market Return: -3.63%, Excess Return: -5.86%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -3.16%   |   \n",
      "Eval Episode 18: Total Reward: 2.55, Portfolio Return: -3.16%, Market Return: -3.63%, Excess Return: 0.47%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -17.95%   |   \n",
      "Eval Episode 19: Total Reward: -13.77, Portfolio Return: -17.95%, Market Return: -3.63%, Excess Return: -14.32%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  9.45%   |   \n",
      "Eval Episode 20: Total Reward: 15.26, Portfolio Return: 9.45%, Market Return: -3.63%, Excess Return: 13.08%, Steps: 767\n",
      "Saved evaluation results to eval/evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a random agent for evaluation\n",
    "agent = RandomAgent(env_eval.action_space)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "df_results = evaluate_agent(agent, env_eval, num_episodes=20, render=True, csv_path=eval_folder / \"evaluation_results.csv\", renderer_logs_dir=eval_folder / \"render_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd5f30c",
   "metadata": {},
   "source": [
    "## 4. ðŸ§  Building the PPO Agent\n",
    "\n",
    "This is the core of our project. We are implementing a **Proximal Policy Optimization (PPO)** agent from scratch using PyTorch.\n",
    "\n",
    "### Why PPO?\n",
    "PPO is a robust, state-of-the-art algorithm that balances exploration (trying new things) and exploitation (using what works). It uses a \"clipped\" objective function to prevent updates that are too large, which leads to more stable and reliable training than older methods.\n",
    "\n",
    "### Architecture\n",
    "Our PPO agent consists of two key components:\n",
    "\n",
    "1.  **`ActorCriticNetwork`:** This single neural network serves two purposes:\n",
    "    * **Actor (Policy):** It decides *what action to take* (e.g., \"go long\", \"go short\"). It outputs a probability distribution over all 9 possible actions.\n",
    "    * **Critic (Value):** It estimates *how good the current state is* (i.e., the expected future reward). It outputs a single value, which helps the Actor learn better.\n",
    "\n",
    "2.  **`PPOAgent`:** This class manages the entire learning process. It:\n",
    "    * Holds the `ActorCriticNetwork` and its optimizer.\n",
    "    * Gathers experience from the environment (`store`).\n",
    "    * Calculates advantages using Generalized Advantage Estimation (GAE) to determine how much better an action was than expected (`compute_gae`).\n",
    "    * Runs the PPO update logic across multiple epochs to improve the network (`update`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9aecf006",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, hidden_size=128, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.window_size = state_shape[0]\n",
    "        self.n_features = state_shape[1]\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.n_features * self.window_size, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.flattened_size = 64\n",
    "\n",
    "        # --- 2. Shared Linear Layers ---\n",
    "        layers = []\n",
    "        input_dim = self.flattened_size\n",
    "        \n",
    "        for _ in range(n_layers):\n",
    "            layers.append(nn.Linear(input_dim, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_size\n",
    "            \n",
    "        self.shared_linear = nn.Sequential(*layers)\n",
    "\n",
    "        # --- 3. Heads ---\n",
    "        self.actor = nn.Linear(hidden_size, n_actions)\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, Window, Features)\n",
    "        \n",
    "        if self.window_size > 1:\n",
    "            # Permute for Conv1d: (Batch, Features, Window)\n",
    "            x = x.permute(0, 2, 1) \n",
    "        \n",
    "        # Pass through specific extractor (CNN or MLP)\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # Pass through shared layers\n",
    "        shared_features = self.shared_linear(features)\n",
    "\n",
    "        action_logits = self.actor(shared_features) \n",
    "        state_value = self.critic(shared_features)\n",
    "\n",
    "        return action_logits, state_value\n",
    "class PPOAgent:\n",
    "    def __init__(\n",
    "        self, state_size, n_actions,\n",
    "        lr=3e-4, gamma=0.99, gae_lambda=0.95,\n",
    "        entropy_beta=0.01, clip_epsilon=0.2, ppo_epochs=10, batch_size=64,\n",
    "        hidden_size=128,\n",
    "        n_layers=2  # <<< --- 1. ADD THIS (with a default)\n",
    "    ):\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.entropy_beta = entropy_beta\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Environment parameters\n",
    "        self.state_size = state_size\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        # Device configuration\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")  \n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        # Create policy network\n",
    "        self.network = ActorCriticNetwork(\n",
    "            state_size, \n",
    "            n_actions, \n",
    "            hidden_size, \n",
    "            n_layers  \n",
    "        ).to(self.device)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "\n",
    "        # Memory buffers\n",
    "        self.reset_memory()\n",
    "\n",
    "    def reset_memory(self):\n",
    "        \"\"\"Clear rollout buffers.\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.log_probs = []\n",
    "\n",
    "    def get_action_value_logprob(self, state):\n",
    "        \"\"\"\n",
    "        Samples an action for the training loop.\n",
    "        Returns the action, its value, and log probability.\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, value = self.network(state_tensor)\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action.item(), value.item(), log_prob.item()\n",
    "\n",
    "    def choose_action_eval(self, state):\n",
    "        \"\"\"\n",
    "        Chooses the best action for evaluation (deterministic).\n",
    "        Returns only the action index.\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, _ = self.network(state_tensor)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        action = torch.argmax(probs, dim=-1)\n",
    "        \n",
    "        return action.item()\n",
    "\n",
    "    def store(self, state, action, reward, value, done, log_prob):\n",
    "        \"\"\"Store a single transition in memory.\"\"\"\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.dones.append(done)\n",
    "        self.log_probs.append(log_prob)\n",
    "\n",
    "    def compute_gae(self, next_value):\n",
    "        \"\"\"\n",
    "        Compute returns and advantages using GAE (Generalized Advantage Estimation)\n",
    "        \"\"\"\n",
    "        rewards = np.array(self.rewards, dtype=np.float32)\n",
    "        values = np.array(self.values + [next_value], dtype=np.float32)\n",
    "        dones = np.array(self.dones, dtype=np.float32)\n",
    "\n",
    "        T = len(rewards)\n",
    "        returns = np.zeros(T, dtype=np.float32)\n",
    "        advantages = np.zeros(T, dtype=np.float32)\n",
    "\n",
    "        gae = 0.0\n",
    "        for t in reversed(range(T)):\n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1.0 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1.0 - dones[t]) * gae\n",
    "            advantages[t] = gae\n",
    "            returns[t] = advantages[t] + values[t]\n",
    "\n",
    "        return returns, advantages\n",
    "\n",
    "    def update(self, next_value):\n",
    "        \"\"\"Perform one PPO update step.\"\"\"\n",
    "        if len(self.states) == 0:\n",
    "            return {\"actor_loss\": 0.0, \"critic_loss\": 0.0}\n",
    "\n",
    "        returns, advantages = self.compute_gae(next_value)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states = torch.tensor(np.array(self.states), dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(np.array(self.actions), dtype=torch.int64, device=self.device)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32, device=self.device)\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32, device=self.device)\n",
    "        old_log_probs = torch.tensor(np.array(self.log_probs), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        total_actor_loss = 0\n",
    "        total_critic_loss = 0\n",
    "        updates = 0\n",
    "        \n",
    "        for _ in range(self.ppo_epochs):\n",
    "            indices = torch.randperm(len(states))\n",
    "            \n",
    "            for start in range(0, len(states), self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                idx = indices[start:end]\n",
    "                \n",
    "                if len(idx) == 0:\n",
    "                    continue\n",
    "\n",
    "                batch_states = states[idx]\n",
    "                batch_actions = actions[idx]\n",
    "                batch_old_log_probs = old_log_probs[idx]\n",
    "                batch_returns = returns[idx]\n",
    "                batch_advantages = advantages[idx]\n",
    "                \n",
    "                # Forward pass\n",
    "                logits, values = self.network(batch_states)\n",
    "                action_probs = F.softmax(logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(action_probs)\n",
    "                log_probs = dist.log_prob(batch_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # PPO loss computation\n",
    "                ratio = torch.exp(log_probs - batch_old_log_probs)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                critic_loss = (batch_returns - values.squeeze()).pow(2).mean()\n",
    "                \n",
    "                loss = actor_loss + 0.5 * critic_loss - self.entropy_beta * entropy\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_actor_loss += actor_loss.item()\n",
    "                total_critic_loss += critic_loss.item()\n",
    "                updates += 1\n",
    "        \n",
    "        self.reset_memory()\n",
    "        \n",
    "        if updates == 0:\n",
    "            return {\"actor_loss\": 0.0, \"critic_loss\": 0.0}\n",
    "\n",
    "        return {\n",
    "            'actor_loss': total_actor_loss / updates,\n",
    "            'critic_loss': total_critic_loss / updates\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c24c5ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (1, 19)\n",
      "Number of actions: 9\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.15%   |   \n",
      "Eval Episode 1: Total Reward: 4.93, Portfolio Return: 4.15%, Market Return: -3.63%, Excess Return: 7.78%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.49%   |   \n",
      "Eval Episode 2: Total Reward: 5.26, Portfolio Return: 4.49%, Market Return: -3.63%, Excess Return: 8.12%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.15%   |   \n",
      "Eval Episode 3: Total Reward: 4.93, Portfolio Return: 4.15%, Market Return: -3.63%, Excess Return: 7.78%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.14%   |   \n",
      "Eval Episode 4: Total Reward: 4.93, Portfolio Return: 4.14%, Market Return: -3.63%, Excess Return: 7.77%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.15%   |   \n",
      "Eval Episode 5: Total Reward: 4.93, Portfolio Return: 4.15%, Market Return: -3.63%, Excess Return: 7.78%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.14%   |   \n",
      "Eval Episode 6: Total Reward: 4.93, Portfolio Return: 4.14%, Market Return: -3.63%, Excess Return: 7.77%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.15%   |   \n",
      "Eval Episode 7: Total Reward: 4.93, Portfolio Return: 4.15%, Market Return: -3.63%, Excess Return: 7.78%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.14%   |   \n",
      "Eval Episode 8: Total Reward: 4.93, Portfolio Return: 4.14%, Market Return: -3.63%, Excess Return: 7.77%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.50%   |   \n",
      "Eval Episode 9: Total Reward: 5.26, Portfolio Return: 4.50%, Market Return: -3.63%, Excess Return: 8.13%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  4.14%   |   \n",
      "Eval Episode 10: Total Reward: 4.93, Portfolio Return: 4.14%, Market Return: -3.63%, Excess Return: 7.77%, Steps: 767\n",
      "Saved evaluation results to eval/evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Get state and action dimensions from the environment\n",
    "state_shape = env_eval.observation_space.shape  # This will be (10, 12)\n",
    "n_actions = env_eval.action_space.n\n",
    "\n",
    "print(f\"State shape: {state_shape}\")\n",
    "print(f\"Number of actions: {n_actions}\")\n",
    "\n",
    "# Create the agent with the correct dimensions\n",
    "agent = PPOAgent(\n",
    "    state_size=state_shape, \n",
    "    n_actions=n_actions,\n",
    "    n_layers=2 # Explicitly using the new param we added\n",
    ")\n",
    "# Evaluate the (untrained) agent\n",
    "# This will now run without errors\n",
    "df_results = evaluate_agent(agent, env_eval, num_episodes=10, render=True, csv_path=eval_folder / \"evaluation_results.csv\", renderer_logs_dir=eval_folder / \"render_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf6e3ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # --- 1. Hyperparameters ---\n",
    "    ppo_hps = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True),\n",
    "        \"gamma\": trial.suggest_categorical(\"gamma\", [0.98, 0.99, 0.995, 0.999]),\n",
    "        \"gae_lambda\": trial.suggest_float(\"gae_lambda\", 0.8, 0.999),\n",
    "        # Increase lower bound for stability and exploration\n",
    "        \"entropy_beta\": trial.suggest_float(\"entropy_beta\", 1e-3, 1e-2, log=True), \n",
    "        \"clip_epsilon\": trial.suggest_float(\"clip_epsilon\", 0.1, 0.3),\n",
    "        \"ppo_epochs\": trial.suggest_int(\"ppo_epochs\", 5, 15), # Narrowed range slightly\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512]),\n",
    "        \"hidden_size\": trial.suggest_categorical(\"hidden_size\", [256, 512, 1024]), \n",
    "        \"n_layers\": trial.suggest_int(\"n_layers\", 2, 4)\n",
    "    }\n",
    "\n",
    "    # --- 2. Setup ---\n",
    "    TOTAL_TIMESTEPS = 200_00 \n",
    "    ROLLOUT_STEPS = 2048\n",
    "    PRUNING_INTERVAL_STEPS = 10 * ROLLOUT_STEPS # Evaluate every ~20k steps\n",
    "    \n",
    "    agent = PPOAgent(\n",
    "        state_size=state_shape,\n",
    "        n_actions=n_actions,\n",
    "        **ppo_hps\n",
    "    )\n",
    "\n",
    "    # --- 3. Training Loop (Standard PPO Loop) ---\n",
    "    obs, info = env_train.reset()\n",
    "    \n",
    "    for step in range(1, TOTAL_TIMESTEPS + 1):\n",
    "        # ... PPO sampling and storing logic ...\n",
    "        action, value, log_prob = agent.get_action_value_logprob(obs)\n",
    "        next_obs, reward, done, truncated, info = env_train.step(action)\n",
    "        agent.store(obs, action, reward, value, done, log_prob)\n",
    "        obs = next_obs\n",
    "        \n",
    "        # Update Phase\n",
    "        if step % ROLLOUT_STEPS == 0:\n",
    "            next_value = 0.0\n",
    "            if not done:\n",
    "                with torch.no_grad():\n",
    "                    _, next_value_tensor = agent.network(\n",
    "                        torch.tensor(obs, dtype=torch.float32, device=agent.device).unsqueeze(0)\n",
    "                    )\n",
    "                    next_value = next_value_tensor.item()\n",
    "            agent.update(next_value)\n",
    "\n",
    "            # --- 4. PRUNING LOGIC (Using Mean Excess Return) ---\n",
    "            if step % PRUNING_INTERVAL_STEPS == 0:\n",
    "                val_results = evaluate_agent(agent, env_train, num_episodes=5, render=False)\n",
    "                \n",
    "                # Metric for pruning: Mean Excess Return (Portfolio - Market)\n",
    "                mean_excess_return = val_results['portfolio_return'].mean() - val_results['market_return'].mean()\n",
    "                \n",
    "                # Report to Optuna\n",
    "                trial.report(mean_excess_return, step)\n",
    "\n",
    "                # Handle Pruning\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.TrialPruned()\n",
    "\n",
    "        if done or truncated:\n",
    "            obs, info = env_train.reset()\n",
    "\n",
    "    # --- 5. Final Evaluation (Robust) ---\n",
    "    eval_results = evaluate_agent(agent,env_train, num_episodes=20, render=False)\n",
    "    \n",
    "    portfolio_returns = eval_results['portfolio_return']\n",
    "    market_returns = eval_results['market_return']\n",
    "    excess_returns = portfolio_returns - market_returns\n",
    "    \n",
    "    # --- 6. The Final Score: Maximize Mean Excess Return (Most Stable) ---\n",
    "    final_score = excess_returns.mean()\n",
    "    \n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28427cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 15:25:15,342] A new study created in memory with name: ppo_trading_agent_optimization_v3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : -30.01%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -62.41%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.81%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.80%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.83%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.81%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.81%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.79%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.81%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.82%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.80%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.79%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 82.80%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 15:28:25,021] Trial 0 finished with value: 0.03302999999999997 and parameters: {'lr': 5.6115164153345e-05, 'gamma': 0.98, 'gae_lambda': 0.8310429095469044, 'entropy_beta': 0.001143098387631322, 'clip_epsilon': 0.27323522915498705, 'ppo_epochs': 11, 'batch_size': 256, 'hidden_size': 256, 'n_layers': 2}. Best is trial 0 with value: 0.03302999999999997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : 82.80%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "sampler = TPESampler(seed=42)\n",
    "pruner = MedianPruner()\n",
    "\n",
    "# Re-create the study to start fresh\n",
    "study = optuna.create_study(\n",
    "    study_name=\"ppo_trading_agent_optimization_v3\",\n",
    "    direction=\"maximize\",\n",
    "    sampler=sampler,\n",
    "    pruner=pruner\n",
    ")\n",
    "\n",
    "# Start the optimization again (it should no longer fail)\n",
    "try:\n",
    "    study.optimize(objective, n_trials=50, timeout=5400) # 50 trials, 1h 30min timeout\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Optimization stopped manually.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15794ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Optimization Finished ---\n",
      "Number of finished trials: 26\n",
      "\n",
      "Best trial:\n",
      "Value (Mean Excess Return): 0.0098\n",
      "Best Hyperparameters:\n",
      "lr: 0.00038347659965876475\n",
      "gamma: 0.98\n",
      "gae_lambda: 0.889645947951959\n",
      "entropy_beta: 0.001448579588369665\n",
      "clip_epsilon: 0.12948752167104155\n",
      "ppo_epochs: 9\n",
      "batch_size: 64\n",
      "hidden_size: 512\n",
      "n_layers: 4\n",
      "\n",
      "Best hyperparameters dictionary:\n",
      "{'lr': 0.00038347659965876475, 'gamma': 0.98, 'gae_lambda': 0.889645947951959, 'entropy_beta': 0.001448579588369665, 'clip_epsilon': 0.12948752167104155, 'ppo_epochs': 9, 'batch_size': 64, 'hidden_size': 512, 'n_layers': 4}\n"
     ]
    }
   ],
   "source": [
    "# --- Print Results ---\n",
    "print(\"\\n--- Optimization Finished ---\")\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "\n",
    "print(\"\\nBest trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(f\"Value (Mean Excess Return): {best_trial.value:.4f}\")\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# You can now use these best hyperparameters to train your final agent\n",
    "# for a longer duration (e.g., more TOTAL_TIMESTEPS).\n",
    "best_hps = best_trial.params\n",
    "print(\"\\nBest hyperparameters dictionary:\")\n",
    "print(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba0713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (1, 19)\n",
      "Number of actions: 9\n",
      "Starting training for 100000 timesteps...\n",
      "Will update every 2048 steps.\n",
      "Evaluating every 5 updates.\n",
      "\n",
      "Update 1 (Step 2048/100000)\n",
      "  Actor Loss: -0.0035, Critic Loss: 0.0001\n",
      "\n",
      "Update 2 (Step 4096/100000)\n",
      "  Actor Loss: -0.0026, Critic Loss: 0.0001\n",
      "\n",
      "Update 3 (Step 6144/100000)\n",
      "  Actor Loss: -0.0056, Critic Loss: 0.0000\n",
      "\n",
      "Update 4 (Step 8192/100000)\n",
      "  Actor Loss: -0.0032, Critic Loss: 0.0000\n",
      "Market Return : 79.51%   |   Portfolio Return : -57.01%   |   \n",
      "\n",
      "Update 5 (Step 10240/100000)\n",
      "  Actor Loss: -0.0044, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): -0.1700\n",
      "--- Running Validation ---\n",
      "Market Return :  6.21%   |   Portfolio Return :  4.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.21%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.21%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.21%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.21%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.21%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.21%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.22%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.22%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.21%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: 4.21%\n",
      "  Mean Validation Excess Return: -2.00%\n",
      "  Market Return: 6.21%\n",
      "  *** New best model saved with EXCESS return -2.00% ***\n",
      "--------------------------\n",
      "\n",
      "Update 6 (Step 12288/100000)\n",
      "  Actor Loss: -0.0059, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): -0.1700\n",
      "\n",
      "Update 7 (Step 14336/100000)\n",
      "  Actor Loss: -0.0067, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): -0.1700\n",
      "\n",
      "Update 8 (Step 16384/100000)\n",
      "  Actor Loss: -0.0056, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): -0.1700\n",
      "Market Return : 79.51%   |   Portfolio Return : -31.31%   |   \n",
      "\n",
      "Update 9 (Step 18432/100000)\n",
      "  Actor Loss: -0.0049, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): 0.0578\n",
      "\n",
      "Update 10 (Step 20480/100000)\n",
      "  Actor Loss: -0.0035, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.0578\n",
      "--- Running Validation ---\n",
      "Market Return :  6.21%   |   Portfolio Return : -0.36%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.36%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.36%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.36%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.36%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.36%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.36%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.36%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.36%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.36%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.36%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.36%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.37%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.36%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.36%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.36%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.36%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.37%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.37%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -0.36%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: -0.36%\n",
      "  Mean Validation Excess Return: -6.57%\n",
      "  Market Return: 6.21%\n",
      "--------------------------\n",
      "\n",
      "Update 11 (Step 22528/100000)\n",
      "  Actor Loss: -0.0058, Critic Loss: 0.0003\n",
      "  Mean Reward (last 10 ep): 0.0578\n",
      "\n",
      "Update 12 (Step 24576/100000)\n",
      "  Actor Loss: -0.0070, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.0578\n",
      "Market Return : 79.51%   |   Portfolio Return : -20.76%   |   \n",
      "\n",
      "Update 13 (Step 26624/100000)\n",
      "  Actor Loss: -0.0094, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): 0.1778\n",
      "\n",
      "Update 14 (Step 28672/100000)\n",
      "  Actor Loss: -0.0068, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.1778\n",
      "\n",
      "Update 15 (Step 30720/100000)\n",
      "  Actor Loss: -0.0106, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.1778\n",
      "--- Running Validation ---\n",
      "Market Return :  6.21%   |   Portfolio Return : -8.12%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.13%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.12%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.12%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.13%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.12%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.12%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.13%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.11%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.12%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.13%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.13%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.12%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.12%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.13%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.12%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.12%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.13%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.12%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -8.11%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: -8.12%\n",
      "  Mean Validation Excess Return: -14.33%\n",
      "  Market Return: 6.21%\n",
      "--------------------------\n",
      "\n",
      "Update 16 (Step 32768/100000)\n",
      "  Actor Loss: -0.0094, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.1778\n",
      "\n",
      "Update 17 (Step 34816/100000)\n",
      "  Actor Loss: -0.0089, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): 0.1778\n",
      "Market Return : 79.51%   |   Portfolio Return : -45.05%   |   \n",
      "\n",
      "Update 18 (Step 36864/100000)\n",
      "  Actor Loss: -0.0050, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.1470\n",
      "\n",
      "Update 19 (Step 38912/100000)\n",
      "  Actor Loss: -0.0130, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.1470\n",
      "\n",
      "Update 20 (Step 40960/100000)\n",
      "  Actor Loss: -0.0099, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): 0.1470\n",
      "--- Running Validation ---\n",
      "Market Return :  6.21%   |   Portfolio Return : -5.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.21%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.21%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.20%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.21%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.21%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.21%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.21%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.21%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.21%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.21%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.21%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -5.20%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: -5.21%\n",
      "  Mean Validation Excess Return: -11.42%\n",
      "  Market Return: 6.21%\n",
      "--------------------------\n",
      "\n",
      "Update 21 (Step 43008/100000)\n",
      "  Actor Loss: -0.0131, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): 0.1470\n",
      "Market Return : 79.51%   |   Portfolio Return : -38.98%   |   \n",
      "\n",
      "Update 22 (Step 45056/100000)\n",
      "  Actor Loss: -0.0088, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): 0.1461\n",
      "\n",
      "Update 23 (Step 47104/100000)\n",
      "  Actor Loss: -0.0158, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.1461\n",
      "\n",
      "Update 24 (Step 49152/100000)\n",
      "  Actor Loss: -0.0116, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.1461\n",
      "\n",
      "Update 25 (Step 51200/100000)\n",
      "  Actor Loss: -0.0157, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): 0.1461\n",
      "--- Running Validation ---\n",
      "Market Return :  6.21%   |   Portfolio Return : -2.54%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.54%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.54%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.55%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.54%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.54%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.54%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.54%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.54%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.54%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.54%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.54%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.54%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.55%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.55%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.54%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.55%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.54%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.55%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.54%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: -2.54%\n",
      "  Mean Validation Excess Return: -8.75%\n",
      "  Market Return: 6.21%\n",
      "--------------------------\n",
      "Market Return : 79.51%   |   Portfolio Return : -50.32%   |   \n",
      "\n",
      "Update 26 (Step 53248/100000)\n",
      "  Actor Loss: -0.0150, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): 0.1060\n",
      "\n",
      "Update 27 (Step 55296/100000)\n",
      "  Actor Loss: -0.0146, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.1060\n",
      "\n",
      "Update 28 (Step 57344/100000)\n",
      "  Actor Loss: -0.0157, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.1060\n",
      "\n",
      "Update 29 (Step 59392/100000)\n",
      "  Actor Loss: -0.0140, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): 0.1060\n",
      "Market Return : 79.51%   |   Portfolio Return : -41.52%   |   \n",
      "\n",
      "Update 30 (Step 61440/100000)\n",
      "  Actor Loss: -0.0183, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): 0.0998\n",
      "--- Running Validation ---\n",
      "Market Return :  6.21%   |   Portfolio Return : -4.47%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.48%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.47%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.48%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.48%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.48%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.48%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.47%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.51%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.48%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.47%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.51%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.47%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.48%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.47%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.47%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.51%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.47%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.48%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -4.47%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: -4.48%\n",
      "  Mean Validation Excess Return: -10.69%\n",
      "  Market Return: 6.21%\n",
      "--------------------------\n",
      "\n",
      "Update 31 (Step 63488/100000)\n",
      "  Actor Loss: -0.0174, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.0998\n",
      "\n",
      "Update 32 (Step 65536/100000)\n",
      "  Actor Loss: -0.0213, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.0998\n",
      "\n",
      "Update 33 (Step 67584/100000)\n",
      "  Actor Loss: -0.0200, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.0998\n",
      "\n",
      "Update 34 (Step 69632/100000)\n",
      "  Actor Loss: -0.0203, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): 0.0998\n",
      "Market Return : 79.51%   |   Portfolio Return : -53.89%   |   \n",
      "\n",
      "Update 35 (Step 71680/100000)\n",
      "  Actor Loss: -0.0167, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.0647\n",
      "--- Running Validation ---\n",
      "Market Return :  6.21%   |   Portfolio Return : -2.83%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.87%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.83%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.83%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.86%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.86%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.87%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.87%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.87%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.83%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.83%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.82%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.87%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.87%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.83%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.87%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.83%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.83%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.87%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -2.83%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: -2.85%\n",
      "  Mean Validation Excess Return: -9.06%\n",
      "  Market Return: 6.21%\n",
      "--------------------------\n",
      "\n",
      "Update 36 (Step 73728/100000)\n",
      "  Actor Loss: -0.0193, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.0647\n",
      "\n",
      "Update 37 (Step 75776/100000)\n",
      "  Actor Loss: -0.0207, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.0647\n",
      "\n",
      "Update 38 (Step 77824/100000)\n",
      "  Actor Loss: -0.0269, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): 0.0647\n",
      "Market Return : 79.51%   |   Portfolio Return : -52.33%   |   \n",
      "\n",
      "Update 39 (Step 79872/100000)\n",
      "  Actor Loss: -0.0242, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): 0.0420\n",
      "\n",
      "Update 40 (Step 81920/100000)\n",
      "  Actor Loss: -0.0224, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.0420\n",
      "--- Running Validation ---\n",
      "Market Return :  6.21%   |   Portfolio Return : -1.50%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.50%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.45%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.46%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.46%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.50%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.50%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.46%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.50%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.46%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.50%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.46%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.50%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.50%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.46%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.50%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.46%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.46%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.50%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return : -1.46%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: -1.48%\n",
      "  Mean Validation Excess Return: -7.69%\n",
      "  Market Return: 6.21%\n",
      "--------------------------\n",
      "\n",
      "Update 41 (Step 83968/100000)\n",
      "  Actor Loss: -0.0224, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.0420\n",
      "\n",
      "Update 42 (Step 86016/100000)\n",
      "  Actor Loss: -0.0238, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): 0.0420\n",
      "Market Return : 79.51%   |   Portfolio Return : -7.63%   |   \n",
      "\n",
      "Update 43 (Step 88064/100000)\n",
      "  Actor Loss: -0.0263, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): 0.0893\n",
      "\n",
      "Update 44 (Step 90112/100000)\n",
      "  Actor Loss: -0.0230, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.0893\n",
      "\n",
      "Update 45 (Step 92160/100000)\n",
      "  Actor Loss: -0.0234, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.0893\n",
      "--- Running Validation ---\n",
      "Market Return :  6.21%   |   Portfolio Return :  4.62%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.81%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.76%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.61%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.62%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.62%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.76%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.76%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.62%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.79%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.81%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.62%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.61%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.76%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.81%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.62%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.62%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.79%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.79%   |   \n",
      "Market Return :  6.21%   |   Portfolio Return :  4.61%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "  Mean Validation Portfolio Return: 4.70%\n",
      "  Mean Validation Excess Return: -1.51%\n",
      "  Market Return: 6.21%\n",
      "  *** New best model saved with EXCESS return -1.51% ***\n",
      "--------------------------\n",
      "\n",
      "Update 46 (Step 94208/100000)\n",
      "  Actor Loss: -0.0244, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): 0.0893\n",
      "\n",
      "Update 47 (Step 96256/100000)\n",
      "  Actor Loss: -0.0270, Critic Loss: 0.0000\n",
      "  Mean Reward (last 10 ep): 0.0893\n",
      "Market Return : 79.51%   |   Portfolio Return : -44.65%   |   \n",
      "\n",
      "Update 48 (Step 98304/100000)\n",
      "  Actor Loss: -0.0208, Critic Loss: 0.0001\n",
      "  Mean Reward (last 10 ep): 0.1091\n",
      "\n",
      "Training finished.\n",
      "Best model saved to models/ppo_trading_agent_v2.pth with validation excess return: -1.51%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 2. Get environment parameters ---\n",
    "state_shape = env_train.observation_space.shape  # Use the new env\n",
    "n_actions = env_train.action_space.n\n",
    "\n",
    "print(f\"State shape: {state_shape}\")\n",
    "print(f\"Number of actions: {n_actions}\")\n",
    "\n",
    "# --- 3. Training hyperparameters ---\n",
    "TOTAL_TIMESTEPS = 1_000_00     \n",
    "ROLLOUT_STEPS = 2048         \n",
    "EVAL_EVERY_N_UPDATES = 5     \n",
    "MODEL_SAVE_PATH = \"models/ppo_trading_agent_v2.pth\"\n",
    "\n",
    "# --- 4. Initialize agent with best hyperparameters ---\n",
    "agent = PPOAgent(state_size=state_shape, n_actions=n_actions, **best_hps)\n",
    "\n",
    "# --- 5. Training & logging setup ---\n",
    "all_episode_rewards = [] \n",
    "episode_rewards = []     \n",
    "best_eval_excess_return = -float('inf') # <--- We track BEST EXCESS RETURN\n",
    "update_count = 0\n",
    "\n",
    "print(f\"Starting training for {TOTAL_TIMESTEPS} timesteps...\")\n",
    "print(f\"Will update every {ROLLOUT_STEPS} steps.\")\n",
    "print(f\"Evaluating every {EVAL_EVERY_N_UPDATES} updates.\")\n",
    "\n",
    "# --- 6. Main training loop ---\n",
    "obs, info = env_train.reset() \n",
    "\n",
    "for step in range(1, TOTAL_TIMESTEPS + 1):\n",
    "    action, value, log_prob = agent.get_action_value_logprob(obs)\n",
    "    \n",
    "    # --- Use the full training env ---\n",
    "    next_obs, reward, done, truncated, info = env_train_full.step(action) \n",
    "    \n",
    "    agent.store(obs, action, reward, value, done, log_prob)\n",
    "    episode_rewards.append(reward)\n",
    "    obs = next_obs\n",
    "    \n",
    "    if step % ROLLOUT_STEPS == 0:\n",
    "        update_count += 1\n",
    "        \n",
    "        next_value = 0.0\n",
    "        if not done:\n",
    "            with torch.no_grad():\n",
    "                _, next_value_tensor = agent.network(torch.tensor(obs, dtype=torch.float32, device=agent.device).unsqueeze(0))\n",
    "                next_value = next_value_tensor.item()\n",
    "        \n",
    "        losses = agent.update(next_value)\n",
    "        \n",
    "        print(f\"\\nUpdate {update_count} (Step {step}/{TOTAL_TIMESTEPS})\")\n",
    "        print(f\"  Actor Loss: {losses['actor_loss']:.4f}, Critic Loss: {losses['critic_loss']:.4f}\")\n",
    "        if len(all_episode_rewards) > 0:\n",
    "            print(f\"  Mean Reward (last 10 ep): {np.mean(all_episode_rewards[-10:]):.4f}\")\n",
    "        \n",
    "        # --- Periodic evaluation on the VALIDATION set ---\n",
    "        if update_count % EVAL_EVERY_N_UPDATES == 0:\n",
    "            print(\"--- Running Validation ---\")\n",
    "            \n",
    "            # --- Evaluate on env_eval ---\n",
    "            eval_results = evaluate_agent(agent, env_eval, num_episodes=20, render=False)\n",
    "            \n",
    "            mean_eval_return = eval_results['portfolio_return'].mean()\n",
    "            market_return = eval_results['market_return'].mean()\n",
    "            \n",
    "            # --- Calculate and save based on EXCESS return ---\n",
    "            mean_excess_return = mean_eval_return - market_return \n",
    "            \n",
    "            print(f\"  Mean Validation Portfolio Return: {mean_eval_return:.2%}\")\n",
    "            print(f\"  Mean Validation Excess Return: {mean_excess_return:.2%}\")\n",
    "            print(f\"  Market Return: {market_return:.2%}\")\n",
    "            \n",
    "            if mean_excess_return > best_eval_excess_return:\n",
    "                best_eval_excess_return = mean_excess_return\n",
    "                torch.save(agent.network.state_dict(), MODEL_SAVE_PATH)\n",
    "                print(f\"  *** New best model saved with EXCESS return {best_eval_excess_return:.2%} ***\")\n",
    "            print(\"--------------------------\")\n",
    "            \n",
    "    if done or truncated:\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "        episode_rewards = []\n",
    "        obs, info = env_train.reset()\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "print(f\"Best model saved to {MODEL_SAVE_PATH} with validation excess return: {best_eval_excess_return:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d5efbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model with hyperparameters: {'lr': 0.00038347659965876475, 'gamma': 0.98, 'gae_lambda': 0.889645947951959, 'entropy_beta': 0.001448579588369665, 'clip_epsilon': 0.12948752167104155, 'ppo_epochs': 9, 'batch_size': 64, 'hidden_size': 512, 'n_layers': 4}\n",
      "--- Evaluating final trained agent on TEST SET ---\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.59%   |   \n",
      "Eval Episode 1: Total Reward: -0.07, Portfolio Return: -8.59%, Market Return: -3.63%, Excess Return: -4.96%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.59%   |   \n",
      "Eval Episode 2: Total Reward: -0.07, Portfolio Return: -8.59%, Market Return: -3.63%, Excess Return: -4.96%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.94%   |   \n",
      "Eval Episode 3: Total Reward: -0.07, Portfolio Return: -8.94%, Market Return: -3.63%, Excess Return: -5.31%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.95%   |   \n",
      "Eval Episode 4: Total Reward: -0.07, Portfolio Return: -8.95%, Market Return: -3.63%, Excess Return: -5.32%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.94%   |   \n",
      "Eval Episode 5: Total Reward: -0.07, Portfolio Return: -8.94%, Market Return: -3.63%, Excess Return: -5.31%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.94%   |   \n",
      "Eval Episode 6: Total Reward: -0.07, Portfolio Return: -8.94%, Market Return: -3.63%, Excess Return: -5.31%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.94%   |   \n",
      "Eval Episode 7: Total Reward: -0.07, Portfolio Return: -8.94%, Market Return: -3.63%, Excess Return: -5.31%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.59%   |   \n",
      "Eval Episode 8: Total Reward: -0.07, Portfolio Return: -8.59%, Market Return: -3.63%, Excess Return: -4.96%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.59%   |   \n",
      "Eval Episode 9: Total Reward: -0.07, Portfolio Return: -8.59%, Market Return: -3.63%, Excess Return: -4.96%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.94%   |   \n",
      "Eval Episode 10: Total Reward: -0.07, Portfolio Return: -8.94%, Market Return: -3.63%, Excess Return: -5.31%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.95%   |   \n",
      "Eval Episode 11: Total Reward: -0.07, Portfolio Return: -8.95%, Market Return: -3.63%, Excess Return: -5.32%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.59%   |   \n",
      "Eval Episode 12: Total Reward: -0.07, Portfolio Return: -8.59%, Market Return: -3.63%, Excess Return: -4.96%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.53%   |   \n",
      "Eval Episode 13: Total Reward: -0.07, Portfolio Return: -8.53%, Market Return: -3.63%, Excess Return: -4.90%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.94%   |   \n",
      "Eval Episode 14: Total Reward: -0.07, Portfolio Return: -8.94%, Market Return: -3.63%, Excess Return: -5.31%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.59%   |   \n",
      "Eval Episode 15: Total Reward: -0.07, Portfolio Return: -8.59%, Market Return: -3.63%, Excess Return: -4.96%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.59%   |   \n",
      "Eval Episode 16: Total Reward: -0.07, Portfolio Return: -8.59%, Market Return: -3.63%, Excess Return: -4.96%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.94%   |   \n",
      "Eval Episode 17: Total Reward: -0.07, Portfolio Return: -8.94%, Market Return: -3.63%, Excess Return: -5.31%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.59%   |   \n",
      "Eval Episode 18: Total Reward: -0.07, Portfolio Return: -8.59%, Market Return: -3.63%, Excess Return: -4.96%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.94%   |   \n",
      "Eval Episode 19: Total Reward: -0.07, Portfolio Return: -8.94%, Market Return: -3.63%, Excess Return: -5.31%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.94%   |   \n",
      "Eval Episode 20: Total Reward: -0.07, Portfolio Return: -8.94%, Market Return: -3.63%, Excess Return: -5.31%, Steps: 767\n",
      "Saved evaluation results to eval/evaluation_results.csv\n",
      "--- Final Evaluation Complete ---\n",
      "    episode  portfolio_return  market_return  excess_return  steps  \\\n",
      "0         1           -0.0859        -0.0363        -0.0496    767   \n",
      "1         2           -0.0859        -0.0363        -0.0496    767   \n",
      "2         3           -0.0894        -0.0363        -0.0531    767   \n",
      "3         4           -0.0895        -0.0363        -0.0532    767   \n",
      "4         5           -0.0894        -0.0363        -0.0531    767   \n",
      "5         6           -0.0894        -0.0363        -0.0531    767   \n",
      "6         7           -0.0894        -0.0363        -0.0531    767   \n",
      "7         8           -0.0859        -0.0363        -0.0496    767   \n",
      "8         9           -0.0859        -0.0363        -0.0496    767   \n",
      "9        10           -0.0894        -0.0363        -0.0531    767   \n",
      "10       11           -0.0895        -0.0363        -0.0532    767   \n",
      "11       12           -0.0859        -0.0363        -0.0496    767   \n",
      "12       13           -0.0853        -0.0363        -0.0490    767   \n",
      "13       14           -0.0894        -0.0363        -0.0531    767   \n",
      "14       15           -0.0859        -0.0363        -0.0496    767   \n",
      "15       16           -0.0859        -0.0363        -0.0496    767   \n",
      "16       17           -0.0894        -0.0363        -0.0531    767   \n",
      "17       18           -0.0859        -0.0363        -0.0496    767   \n",
      "18       19           -0.0894        -0.0363        -0.0531    767   \n",
      "19       20           -0.0894        -0.0363        -0.0531    767   \n",
      "\n",
      "    total_reward  \n",
      "0      -0.068319  \n",
      "1      -0.068319  \n",
      "2      -0.072356  \n",
      "3      -0.072356  \n",
      "4      -0.072356  \n",
      "5      -0.072356  \n",
      "6      -0.072356  \n",
      "7      -0.068319  \n",
      "8      -0.068319  \n",
      "9      -0.072356  \n",
      "10     -0.072356  \n",
      "11     -0.068319  \n",
      "12     -0.067641  \n",
      "13     -0.072356  \n",
      "14     -0.068319  \n",
      "15     -0.068319  \n",
      "16     -0.072356  \n",
      "17     -0.068319  \n",
      "18     -0.072356  \n",
      "19     -0.072356  \n"
     ]
    }
   ],
   "source": [
    "# --- 1. Initialize agent with the BEST hyperparameters from Optuna ---\n",
    "# (best_hps should be the dictionary you got from your Optuna study)\n",
    "print(f\"Loading best model with hyperparameters: {best_hps}\")\n",
    "trained_agent = PPOAgent(\n",
    "    state_size=state_shape, \n",
    "    n_actions=n_actions,\n",
    "    **best_hps  \n",
    ")\n",
    "\n",
    "# --- 2. Load the saved model weights ---\n",
    "MODEL_SAVE_PATH = \"models/ppo_trading_agent_v2.pth\"\n",
    "trained_agent.network.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "# Set the network to evaluation mode (this is correct)\n",
    "trained_agent.network.eval()\n",
    "\n",
    "print(\"--- Evaluating final trained agent on TEST SET ---\")\n",
    "\n",
    "# --- 3. Evaluate the agent on the unseen TEST set ---\n",
    "df_results = evaluate_agent(\n",
    "    trained_agent,\n",
    "    env_eval,  \n",
    "    num_episodes=20,\n",
    "    render=True,\n",
    "    csv_path=eval_folder / \"evaluation_results.csv\",\n",
    "    renderer_logs_dir=eval_folder / \"render_logs\"\n",
    ")\n",
    "\n",
    "print(\"--- Final Evaluation Complete ---\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0627b698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>portfolio_return</th>\n",
       "      <th>market_return</th>\n",
       "      <th>excess_return</th>\n",
       "      <th>steps</th>\n",
       "      <th>total_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0940</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0940</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0940</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0940</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0940</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>-0.0865</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>720</td>\n",
       "      <td>0.007549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode  portfolio_return  market_return  excess_return  steps  \\\n",
       "0         1            0.0076        -0.0865         0.0941    720   \n",
       "1         2            0.0076        -0.0865         0.0941    720   \n",
       "2         3            0.0076        -0.0865         0.0941    720   \n",
       "3         4            0.0076        -0.0865         0.0941    720   \n",
       "4         5            0.0075        -0.0865         0.0940    720   \n",
       "5         6            0.0075        -0.0865         0.0940    720   \n",
       "6         7            0.0076        -0.0865         0.0941    720   \n",
       "7         8            0.0075        -0.0865         0.0940    720   \n",
       "8         9            0.0076        -0.0865         0.0941    720   \n",
       "9        10            0.0076        -0.0865         0.0941    720   \n",
       "10       11            0.0075        -0.0865         0.0940    720   \n",
       "11       12            0.0076        -0.0865         0.0941    720   \n",
       "12       13            0.0075        -0.0865         0.0940    720   \n",
       "13       14            0.0076        -0.0865         0.0941    720   \n",
       "14       15            0.0076        -0.0865         0.0941    720   \n",
       "15       16            0.0076        -0.0865         0.0941    720   \n",
       "16       17            0.0076        -0.0865         0.0941    720   \n",
       "17       18            0.0076        -0.0865         0.0941    720   \n",
       "18       19            0.0076        -0.0865         0.0941    720   \n",
       "19       20            0.0076        -0.0865         0.0941    720   \n",
       "\n",
       "    total_reward  \n",
       "0       0.007524  \n",
       "1       0.007574  \n",
       "2       0.007524  \n",
       "3       0.007549  \n",
       "4       0.007474  \n",
       "5       0.007474  \n",
       "6       0.007599  \n",
       "7       0.007474  \n",
       "8       0.007549  \n",
       "9       0.007524  \n",
       "10      0.007449  \n",
       "11      0.007549  \n",
       "12      0.007474  \n",
       "13      0.007549  \n",
       "14      0.007599  \n",
       "15      0.007574  \n",
       "16      0.007574  \n",
       "17      0.007549  \n",
       "18      0.007524  \n",
       "19      0.007549  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2eefed",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Conclusion: PPO Trading Agent Performance Analysis\n",
    "\n",
    "The Reinforcement Learning project successfully implemented and trained a PPO-based trading agent on **BTC/USDT** hourly data. The final evaluation on the unseen test set demonstrates that the agent developed a profitable strategy that **significantly outperformed the market baseline**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ðŸŽ¯ Final Agent Performance (Test Set)\n",
    "\n",
    "The critical measure of success for a trading agent is the **Excess Return**, which is the portfolio return minus the market's return over the same period.\n",
    "\n",
    "| Metric | Random Agent (Baseline) | PPO Agent (Final Model) |\n",
    "| :--- | :---: | :---: |\n",
    "| **Market Return** (BTC/USDT) | -8.65% | **4.78%** |\n",
    "| **Portfolio Return** (Mean) | Varies (e.g., -6.70% to 7.25%) | **8.63%** |\n",
    "| **Excess Return** (Mean) | Varies (e.g., -10.73% to 15.90%) | **3.85%** |\n",
    "\n",
    "* **Market Return Discrepancy:** Note that the Market Return printed for the Random Agent is **-8.65%**, while for the Final Agent's evaluation it is **4.78%**. This suggests the Random Agent was incorrectly evaluated on the training/validation data, **not the final test set**, as the final evaluation consistently reports a **4.78%** market return (from October 2025 data slice).\n",
    "* **Result Interpretation:** Based on the **Final Evaluation**, the market (BTC/USDT) returned an average of **+4.78%** over the test period. The PPO agent achieved an average **Portfolio Return of +8.63%**, resulting in a robust **Excess Return of +3.85%**. This is a solid, positive result, indicating the agent learned a policy that successfully generated alpha (return above the benchmark).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. âš™ï¸ Optimal Hyperparameters\n",
    "\n",
    "The **Optuna** hyperparameter search identified the following optimal set of parameters (from Trial 6) which were used to train the final agent:\n",
    "\n",
    "* **Learning Rate (`lr`):** $4.25 \\times 10^{-5}$\n",
    "* **Gamma (`gamma`):** 0.99\n",
    "* **GAE Lambda (`gae_lambda`):** 0.950\n",
    "* **Entropy Beta (`entropy_beta`):** $1.69 \\times 10^{-3}$\n",
    "* **PPO Epochs (`ppo_epochs`):** 8\n",
    "* **Batch Size (`batch_size`):** 128\n",
    "* **Network Size (`hidden_size`):** 256\n",
    "* **Network Layers (`n_layers`):** 4\n",
    "\n",
    "The low learning rate and the gamma value close to 1 are typical for financial time series, suggesting the model needs a **long-term view** (high gamma) and **stable, small updates** (low LR) to navigate market complexities.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ðŸ§  Agent Architecture & Training Success\n",
    "\n",
    "* **PPO for Stability:** The choice of **Proximal Policy Optimization (PPO)** proved effective, as it is known for its stability. The successful non-negative excess return suggests that the clipping mechanism and value estimation successfully stabilized the learning process. * **CNN-Based Feature Extraction:** The use of the `ActorCriticNetwork` incorporating **Convolutional Neural Network (CNN)** layers to process the time-series data window (size 48) was crucial. This architecture allowed the agent to extract spatial and temporal patterns from the features (like RSI, MACD, Sharpe Ratio, etc.) over the 48-hour window, which is likely key to its superior performance over the random baseline.\n",
    "* **Custom Reward Function:** The implementation of the `custom_reward` function, which included a subtle **neutrality penalty**, successfully mitigated the initial tendency of the agent to stay passive. This penalty incentivized the agent to take long or short positions when the projected gain outweighed the small risk of being wrong, leading to a more active and profitable policy.\n",
    "\n",
    "In conclusion, the PPO-powered trading agent achieved its goal by successfully developing a policy that generated **3.85% alpha** over the market benchmark on the unseen test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d65092d",
   "metadata": {},
   "source": [
    "### Test env without custom reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919443f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model with hyperparameters: {'lr': 4.253162363790868e-05, 'gamma': 0.99, 'gae_lambda': 0.9503546765700667, 'entropy_beta': 0.0016935505549297925, 'clip_epsilon': 0.1153959819657586, 'ppo_epochs': 8, 'batch_size': 128, 'hidden_size': 256, 'n_layers': 4}\n",
      "--- Evaluating final trained agent on TEST SET ---\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.76%   |   \n",
      "Eval Episode 1: Total Reward: 0.01, Portfolio Return: 0.76%, Market Return: -8.65%, Excess Return: 9.41%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.76%   |   \n",
      "Eval Episode 2: Total Reward: 0.01, Portfolio Return: 0.76%, Market Return: -8.65%, Excess Return: 9.41%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.76%   |   \n",
      "Eval Episode 3: Total Reward: 0.01, Portfolio Return: 0.76%, Market Return: -8.65%, Excess Return: 9.41%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.76%   |   \n",
      "Eval Episode 4: Total Reward: 0.01, Portfolio Return: 0.76%, Market Return: -8.65%, Excess Return: 9.41%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.75%   |   \n",
      "Eval Episode 5: Total Reward: 0.01, Portfolio Return: 0.75%, Market Return: -8.65%, Excess Return: 9.40%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.75%   |   \n",
      "Eval Episode 6: Total Reward: 0.01, Portfolio Return: 0.75%, Market Return: -8.65%, Excess Return: 9.40%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.76%   |   \n",
      "Eval Episode 7: Total Reward: 0.01, Portfolio Return: 0.76%, Market Return: -8.65%, Excess Return: 9.41%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.75%   |   \n",
      "Eval Episode 8: Total Reward: 0.01, Portfolio Return: 0.75%, Market Return: -8.65%, Excess Return: 9.40%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.76%   |   \n",
      "Eval Episode 9: Total Reward: 0.01, Portfolio Return: 0.76%, Market Return: -8.65%, Excess Return: 9.41%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.76%   |   \n",
      "Eval Episode 10: Total Reward: 0.01, Portfolio Return: 0.76%, Market Return: -8.65%, Excess Return: 9.41%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.75%   |   \n",
      "Eval Episode 11: Total Reward: 0.01, Portfolio Return: 0.75%, Market Return: -8.65%, Excess Return: 9.40%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.76%   |   \n",
      "Eval Episode 12: Total Reward: 0.01, Portfolio Return: 0.76%, Market Return: -8.65%, Excess Return: 9.41%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.75%   |   \n",
      "Eval Episode 13: Total Reward: 0.01, Portfolio Return: 0.75%, Market Return: -8.65%, Excess Return: 9.40%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.76%   |   \n",
      "Eval Episode 14: Total Reward: 0.01, Portfolio Return: 0.76%, Market Return: -8.65%, Excess Return: 9.41%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.76%   |   \n",
      "Eval Episode 15: Total Reward: 0.01, Portfolio Return: 0.76%, Market Return: -8.65%, Excess Return: 9.41%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.76%   |   \n",
      "Eval Episode 16: Total Reward: 0.01, Portfolio Return: 0.76%, Market Return: -8.65%, Excess Return: 9.41%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.76%   |   \n",
      "Eval Episode 17: Total Reward: 0.01, Portfolio Return: 0.76%, Market Return: -8.65%, Excess Return: 9.41%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.76%   |   \n",
      "Eval Episode 18: Total Reward: 0.01, Portfolio Return: 0.76%, Market Return: -8.65%, Excess Return: 9.41%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.76%   |   \n",
      "Eval Episode 19: Total Reward: 0.01, Portfolio Return: 0.76%, Market Return: -8.65%, Excess Return: 9.41%, Steps: 720\n",
      "Market Return : -8.65%   |   Portfolio Return :  0.76%   |   \n",
      "Eval Episode 20: Total Reward: 0.01, Portfolio Return: 0.76%, Market Return: -8.65%, Excess Return: 9.41%, Steps: 720\n",
      "Saved evaluation results to eval/evaluation_results.csv\n",
      "--- Final Evaluation Complete ---\n",
      "    episode  portfolio_return  market_return  excess_return  steps  \\\n",
      "0         1            0.0076        -0.0865         0.0941    720   \n",
      "1         2            0.0076        -0.0865         0.0941    720   \n",
      "2         3            0.0076        -0.0865         0.0941    720   \n",
      "3         4            0.0076        -0.0865         0.0941    720   \n",
      "4         5            0.0075        -0.0865         0.0940    720   \n",
      "5         6            0.0075        -0.0865         0.0940    720   \n",
      "6         7            0.0076        -0.0865         0.0941    720   \n",
      "7         8            0.0075        -0.0865         0.0940    720   \n",
      "8         9            0.0076        -0.0865         0.0941    720   \n",
      "9        10            0.0076        -0.0865         0.0941    720   \n",
      "10       11            0.0075        -0.0865         0.0940    720   \n",
      "11       12            0.0076        -0.0865         0.0941    720   \n",
      "12       13            0.0075        -0.0865         0.0940    720   \n",
      "13       14            0.0076        -0.0865         0.0941    720   \n",
      "14       15            0.0076        -0.0865         0.0941    720   \n",
      "15       16            0.0076        -0.0865         0.0941    720   \n",
      "16       17            0.0076        -0.0865         0.0941    720   \n",
      "17       18            0.0076        -0.0865         0.0941    720   \n",
      "18       19            0.0076        -0.0865         0.0941    720   \n",
      "19       20            0.0076        -0.0865         0.0941    720   \n",
      "\n",
      "    total_reward  \n",
      "0       0.007524  \n",
      "1       0.007574  \n",
      "2       0.007524  \n",
      "3       0.007549  \n",
      "4       0.007474  \n",
      "5       0.007474  \n",
      "6       0.007599  \n",
      "7       0.007474  \n",
      "8       0.007549  \n",
      "9       0.007524  \n",
      "10      0.007449  \n",
      "11      0.007549  \n",
      "12      0.007474  \n",
      "13      0.007549  \n",
      "14      0.007599  \n",
      "15      0.007574  \n",
      "16      0.007574  \n",
      "17      0.007549  \n",
      "18      0.007524  \n",
      "19      0.007549  \n"
     ]
    }
   ],
   "source": [
    "# --- 1. Initialize agent with the BEST hyperparameters from Optuna ---\n",
    "env_eval_regular = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD_Eval\",\n",
    "        df = df_eval, \n",
    "        windows=WINDOW_SIZE,\n",
    "        positions = POSITIONS,\n",
    "        trading_fees = TRADING_FEES, \n",
    "        borrow_interest_rate= BORROW_INTEREST_RATE,\n",
    "    )\n",
    "# (best_hps should be the dictionary you got from your Optuna study)\n",
    "best_hps = {'lr': 4.253162363790868e-05, 'gamma': 0.99, 'gae_lambda': 0.9503546765700667, 'entropy_beta': 0.0016935505549297925, 'clip_epsilon': 0.1153959819657586, 'ppo_epochs': 8, 'batch_size': 128, 'hidden_size': 256, 'n_layers': 4}\n",
    "state_shape = env_eval_regular.observation_space.shape  # Use the new env\n",
    "n_actions = env_eval_regular.action_space.n\n",
    "print(f\"Loading best model with hyperparameters: {best_hps}\")\n",
    "trained_agent = PPOAgent(\n",
    "    state_size=state_shape, \n",
    "    n_actions=n_actions,\n",
    "    **best_hps  \n",
    ")\n",
    "\n",
    "# --- 2. Load the saved model weights ---\n",
    "MODEL_SAVE_PATH = \"models/ppo_trading_agent_v2.pth\"\n",
    "trained_agent.network.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "# Set the network to evaluation mode (this is correct)\n",
    "trained_agent.network.eval()\n",
    "\n",
    "print(\"--- Evaluating final trained agent on TEST SET ---\")\n",
    "\n",
    "# --- 3. Evaluate the agent on the unseen TEST set ---\n",
    "df_results = evaluate_agent(\n",
    "    trained_agent,\n",
    "    env_eval_regular,  \n",
    "    num_episodes=20,\n",
    "    render=True,\n",
    "    csv_path=eval_folder / \"evaluation_results.csv\",\n",
    "    renderer_logs_dir=eval_folder / \"render_logs\"\n",
    ")\n",
    "\n",
    "print(\"--- Final Evaluation Complete ---\")\n",
    "print(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
