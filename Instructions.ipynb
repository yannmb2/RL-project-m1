{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b9217a",
   "metadata": {},
   "source": [
    "Intermediate Deep Learning - Deep Reinforcement Learning\n",
    "# Project: Deep Reinforcement Learning in Trading Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8ea223",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this project, you will implement and evaluate a deep reinforcement learning (DRL) agent in a trading environment. The goal is to train an agent that can make profitable trading decisions based on historical market data.\n",
    "\n",
    "You are encouraged to experiment with different DRL algorithms, architectures, and hyperparameters to optimize the agent's performance.\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "You are required to prepare:\n",
    "\n",
    "- Code implementation of the DRL agent(s) and training process. Can be in the form of Jupyter Notebooks or Python scripts.\n",
    "- A report (4 pages max) summarizing your approach, results, and insights gained from the project, including:  \n",
    "    - Description of the DRL algorithm(s) used.\n",
    "    - Training process and hyperparameter choices.\n",
    "    - Challenges faced and how they were addressed.\n",
    "\n",
    "    Justify your design choices. You are also encouraged to include visualizations of the agent's performance over time, or compare strategies developed with DRL against financial benchmarks.\n",
    "- An evaluation results CSV file `evaluation_results.csv` generated by your agent after training using the provided evaluation function.\n",
    "- A presentation (15 minutes) to showcase your work, findings, and any interesting observations.\n",
    "\n",
    "*The documents are to be submitted in a zip file, before 16/11/2025 11:59PM, and the presentation is scheduled for next session.*\n",
    "\n",
    "### Environment\n",
    "We will use Gym Trading Env as our trading environment. (https://gym-trading-env.readthedocs.io/en/latest/)\n",
    "\n",
    "- This is a gymnasium-compatible environment designed to simulate trading (stocks or crypto) from historical market data.\n",
    "- Its goal is to provide a fast and customizable platform for training RL agents in a trading scenario.\n",
    "\n",
    "We will use BTC/USDT hour step historical data from Binance for training and evaluation. The agent will be evaluated on the period from 2025-10-01 to 2025-11-01.\n",
    "\n",
    "Following code blocks demonstrate how to set up the environment and evaluate your agent.\n",
    "\n",
    "### Grading Criteria\n",
    "- Implementation of the DRL agent and training process (40%)\n",
    "    - DRL algorithm correctly implemented (15%)\n",
    "    - Appropriate training procedure (15%)\n",
    "    - Effective use of hyperparameters (10%)\n",
    "    - 10 % bonus for innovative approaches or techniques\n",
    "- Performance of the agent based on evaluation metrics (30%)\n",
    "    - If the agent shows progress during training (10%)\n",
    "    - If the agent outperforms a random strategy during evaluation (10%)\n",
    "    - If the portfolio return exceeds market return (10%)\n",
    "    - 30%, 20%, 10% bonus for the top 3 agents respectively\n",
    "- Quality and clarity of the report (20%)\n",
    "    - Clear explanation of methods and results (10%)\n",
    "    - Justification of design choices (10%)\n",
    "- Presentation (20%)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bd874",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c1cd5",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd6c17",
   "metadata": {},
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dadc19c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python3\n",
      "Requirement already satisfied: gym-trading-env in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.3.5)\n",
      "Requirement already satisfied: pandas>=1.5.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gym-trading-env) (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gym-trading-env) (2.3.4)\n",
      "Requirement already satisfied: gymnasium>=0.28.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gym-trading-env) (1.2.2)\n",
      "Requirement already satisfied: flask==2.2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gym-trading-env) (2.2.5)\n",
      "Requirement already satisfied: pyecharts>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gym-trading-env) (2.0.9)\n",
      "Requirement already satisfied: ccxt==3.0.59 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gym-trading-env) (3.0.59)\n",
      "Requirement already satisfied: nest_asyncio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gym-trading-env) (1.6.0)\n",
      "Requirement already satisfied: setuptools>=60.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ccxt==3.0.59->gym-trading-env) (80.9.0)\n",
      "Requirement already satisfied: certifi>=2018.1.18 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ccxt==3.0.59->gym-trading-env) (2025.10.5)\n",
      "Requirement already satisfied: requests>=2.18.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ccxt==3.0.59->gym-trading-env) (2.32.5)\n",
      "Requirement already satisfied: cryptography>=2.6.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ccxt==3.0.59->gym-trading-env) (46.0.3)\n",
      "Requirement already satisfied: aiohttp>=3.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ccxt==3.0.59->gym-trading-env) (3.13.2)\n",
      "Requirement already satisfied: aiodns>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ccxt==3.0.59->gym-trading-env) (3.5.0)\n",
      "Requirement already satisfied: yarl>=1.7.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ccxt==3.0.59->gym-trading-env) (1.22.0)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from flask==2.2.5->gym-trading-env) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from flask==2.2.5->gym-trading-env) (3.1.6)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from flask==2.2.5->gym-trading-env) (2.2.0)\n",
      "Requirement already satisfied: click>=8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from flask==2.2.5->gym-trading-env) (8.3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gymnasium>=0.28.1->gym-trading-env) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gymnasium>=0.28.1->gym-trading-env) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gymnasium>=0.28.1->gym-trading-env) (0.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=1.5.3->gym-trading-env) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=1.5.3->gym-trading-env) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=1.5.3->gym-trading-env) (2025.2)\n",
      "Requirement already satisfied: prettytable in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyecharts>=2.0.2->gym-trading-env) (3.17.0)\n",
      "Requirement already satisfied: simplejson in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyecharts>=2.0.2->gym-trading-env) (3.20.2)\n",
      "Requirement already satisfied: pycares>=4.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiodns>=1.1.1->ccxt==3.0.59->gym-trading-env) (4.11.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp>=3.8->ccxt==3.0.59->gym-trading-env) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp>=3.8->ccxt==3.0.59->gym-trading-env) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp>=3.8->ccxt==3.0.59->gym-trading-env) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp>=3.8->ccxt==3.0.59->gym-trading-env) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp>=3.8->ccxt==3.0.59->gym-trading-env) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp>=3.8->ccxt==3.0.59->gym-trading-env) (0.4.1)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from cryptography>=2.6.1->ccxt==3.0.59->gym-trading-env) (2.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Jinja2>=3.0->flask==2.2.5->gym-trading-env) (3.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.3->gym-trading-env) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.18.4->ccxt==3.0.59->gym-trading-env) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.18.4->ccxt==3.0.59->gym-trading-env) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.18.4->ccxt==3.0.59->gym-trading-env) (2.5.0)\n",
      "Requirement already satisfied: wcwidth in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from prettytable->pyecharts>=2.0.2->gym-trading-env) (0.2.14)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from cffi>=2.0.0->cryptography>=2.6.1->ccxt==3.0.59->gym-trading-env) (2.23)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "!\"{sys.executable}\" -m pip install gym-trading-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf91412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.downloader import download\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05469c",
   "metadata": {},
   "source": [
    "### Prepare Data Set\n",
    "\n",
    "- Create a data folder to store historical data\n",
    "- Download historical data for BTC/USDT from Binance using the provided utility function.\n",
    "- Preprocess the data to create features. The features (plus two dynamic features: last position taken by the agent, and the current real position) are the state of the environment at each time step.  \n",
    "    *(You can add more features if you want to experiment with different state representations.)*\n",
    "- Select training and evaluation data based on the specified date ranges.  \n",
    "    *(You can modify the training range if you want to experiment with different time periods, however, keep in mind the evaluation period should always be after the training period.)*\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28cdc5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTC/USDT downloaded from binance and stored at data/binance-BTCUSDT-1h.pkl\n"
     ]
    }
   ],
   "source": [
    "data_folder = Path(\"data/\")\n",
    "data_folder.mkdir(parents=True, exist_ok=True)\n",
    "eval_folder = Path(\"eval/\")\n",
    "eval_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "download(exchange_names = [\"binance\"],\n",
    "    symbols= [\"BTC/USDT\"],\n",
    "    timeframe= \"1h\",\n",
    "    dir = data_folder,\n",
    "    since= datetime.datetime(year= 2020, month=10, day=1),\n",
    ")\n",
    "\n",
    "df = pd.read_pickle(data_folder / \"binance-BTCUSDT-1h.pkl\")\n",
    "\n",
    "# --- 1. Initial Data Shift and Cleanup ---\n",
    "# Shift 'close' one step back to create 'prev_close'. This is the price\n",
    "# available at the moment the new candle OPENS (i.e., Close at t-1).\n",
    "df[\"prev_close\"] = df[\"close\"].shift(1)\n",
    "\n",
    "# --- 2. Calculate Raw Technical Indicators on LAGGED DATA ---\n",
    "# All indicators MUST use 'prev_close' for their core calculations.\n",
    "\n",
    "# Sharpe Ratio\n",
    "ANNUALIZATION_FACTOR = 24 * 365\n",
    "ROLLING_WINDOW_SR = 7 * 24 \n",
    "RISK_FREE_RATE_ANNUAL = 0.04\n",
    "RISK_FREE_RATE_HOURLY = (1 + RISK_FREE_RATE_ANNUAL)**(1/ANNUALIZATION_FACTOR) - 1\n",
    "\n",
    "# Base returns calculation uses 'prev_close' (i.e., Close at t-1)\n",
    "df['return'] = df['prev_close'].pct_change()\n",
    "df['excess_return'] = df['return'] - RISK_FREE_RATE_HOURLY\n",
    "rolling_mean_excess = df['excess_return'].rolling(window=ROLLING_WINDOW_SR).mean()\n",
    "rolling_std_excess = df['excess_return'].rolling(window=ROLLING_WINDOW_SR).std()\n",
    "df['raw_sharpe'] = (rolling_mean_excess / (rolling_std_excess + 1e-9)) * np.sqrt(ANNUALIZATION_FACTOR)\n",
    "\n",
    "# MACD uses 'prev_close' for EMAs\n",
    "df['EMA_12'] = df['prev_close'].ewm(span=12, adjust=False).mean()\n",
    "df['EMA_26'] = df['prev_close'].ewm(span=26, adjust=False).mean()\n",
    "df['raw_macd'] = df['EMA_12'] - df['EMA_26']\n",
    "df['raw_macd_signal'] = df['raw_macd'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# Bollinger Bands uses 'prev_close' for MA and StdDev\n",
    "ROLLING_WINDOW_BB = 20\n",
    "df['BB_Middle'] = df['prev_close'].rolling(window=ROLLING_WINDOW_BB).mean()\n",
    "df['BB_Std'] = df['prev_close'].rolling(window=ROLLING_WINDOW_BB).std()\n",
    "df['raw_bb_upper'] = df['BB_Middle'] + (df['BB_Std'] * 2)\n",
    "df['raw_bb_lower'] = df['BB_Middle'] - (df['BB_Std'] * 2)\n",
    "\n",
    "# OBV uses 'prev_close'\n",
    "df['raw_obv'] = (np.sign(df['prev_close'].diff()) * df['volume'].shift(1)).cumsum().fillna(0)\n",
    "\n",
    "\n",
    "# ATR (Average True Range) - NEW\n",
    "df['high_t_minus_1'] = df['high'].shift(1)\n",
    "df['low_t_minus_1'] = df['low'].shift(1)\n",
    "df['prev_prev_close'] = df['prev_close'].shift(1)\n",
    "\n",
    "df['tr_1'] = df['high_t_minus_1'] - df['low_t_minus_1'] # Range of candle t-1\n",
    "df['tr_2'] = np.abs(df['high_t_minus_1'] - df['prev_prev_close']) # Distance from previous close to high\n",
    "df['tr_3'] = np.abs(df['low_t_minus_1'] - df['prev_prev_close']) # Distance from previous close to low\n",
    "df['true_range'] = df[['tr_1', 'tr_2', 'tr_3']].max(axis=1)\n",
    "df['raw_atr'] = df['true_range'].rolling(window=14).mean()\n",
    "\n",
    "# RSI (Relative Strength Index) - NEW\n",
    "delta = df['prev_close'].diff()\n",
    "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "rs = gain / (loss + 1e-9)\n",
    "df['raw_rsi'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# --- 3. Add Cyclical Time Features (No Change, Already Safe) ---\n",
    "df['hour'] = df.index.hour\n",
    "df['day_of_week'] = df.index.dayofweek\n",
    "\n",
    "df['feature_hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['feature_hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['feature_day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['feature_day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "# --- 4. Create Final, Normalized Features (Shift Applied When Necessary) ---\n",
    "\n",
    "# We define a log-return feature based on data available in the previous completed candle.\n",
    "df['feature_log_return_1h'] = np.log(df['prev_close'] / df['prev_close'].shift(1))\n",
    "\n",
    "\n",
    "# Price Features (Normalized by prev_close, ready for t=0 observation)\n",
    "df['feature_open'] = (df['open'] / df['prev_close']) - 1\n",
    "df['feature_high'] = (df['high'].shift(1) / df['prev_close']) - 1\n",
    "df['feature_low'] = (df['low'].shift(1) / df['prev_close']) - 1\n",
    "\n",
    "\n",
    "# Volume Features (Z-Score of volume/OBV at t-1)\n",
    "vol_mean_30d = df['volume'].shift(1).rolling(30*24).mean()\n",
    "vol_std_30d = df['volume'].shift(1).rolling(30*24).std()\n",
    "df['feature_volume_zscore'] = ((df['volume'].shift(1) - vol_mean_30d) / (vol_std_30d + 1e-9))\n",
    "obv_mean_30d = df['raw_obv'].shift(1).rolling(30*24).mean()\n",
    "obv_std_30d = df['raw_obv'].shift(1).rolling(30*24).std()\n",
    "df['feature_obv_zscore'] = ((df['raw_obv'].shift(1) - obv_mean_30d) / (obv_std_30d + 1e-9))\n",
    "\n",
    "# Indicator Features\n",
    "df['feature_MACD'] = (df['raw_macd'].shift(1) / df['prev_close'])\n",
    "df['feature_MACD_Signal'] = (df['raw_macd_signal'].shift(1) / df['prev_close'])\n",
    "df['feature_BB_Upper'] = (df['raw_bb_upper'].shift(1) / df['prev_close']) - 1\n",
    "df['feature_BB_Lower'] = (df['raw_bb_lower'].shift(1) / df['prev_close']) - 1\n",
    "df['feature_atr'] = (df['raw_atr'].shift(1) / df['prev_close'])\n",
    "df['feature_rsi'] = df['raw_rsi'].shift(1)\n",
    "df['feature_sharpe_ratio'] = df['raw_sharpe'].shift(1)\n",
    "\n",
    "# --- 5. Final Cleanup ---\n",
    "final_features = [\n",
    "    'feature_hour_sin', 'feature_hour_cos', 'feature_day_sin', 'feature_day_cos',\n",
    "    'feature_open', 'feature_high', 'feature_low', 'feature_log_return_1h',\n",
    "    'feature_volume_zscore', 'feature_obv_zscore',\n",
    "    'feature_MACD', 'feature_MACD_Signal', \n",
    "    'feature_BB_Upper', 'feature_BB_Lower',\n",
    "    'feature_atr', 'feature_rsi', 'feature_sharpe_ratio'\n",
    "]\n",
    "\n",
    "# Keep the current raw OHLCV for the Environment to calculate rewards/penalties,\n",
    "# but the agent MUST only observe the 'feature_' columns.\n",
    "all_cols_to_keep = ['close','open', 'high', 'low', 'prev_close', 'volume'] + final_features\n",
    "df = df[all_cols_to_keep]\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# --- 6. Defining DataFrames ---\n",
    "# (Environment setup remains correct as it depends on the index slicing)\n",
    "\n",
    "df_train = df.loc['2024-10-01':'2025-09-30'] # Training data\n",
    "df_eval = df.loc['2025-10-01':'2025-11-01'] # Evaluation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec95a73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>prev_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>feature_hour_sin</th>\n",
       "      <th>feature_hour_cos</th>\n",
       "      <th>feature_day_sin</th>\n",
       "      <th>feature_day_cos</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_log_return_1h</th>\n",
       "      <th>feature_volume_zscore</th>\n",
       "      <th>feature_obv_zscore</th>\n",
       "      <th>feature_MACD</th>\n",
       "      <th>feature_MACD_Signal</th>\n",
       "      <th>feature_BB_Upper</th>\n",
       "      <th>feature_BB_Lower</th>\n",
       "      <th>feature_atr</th>\n",
       "      <th>feature_rsi</th>\n",
       "      <th>feature_sharpe_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_open</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-10-01 00:00:00</th>\n",
       "      <td>63531.99</td>\n",
       "      <td>63327.60</td>\n",
       "      <td>63606.00</td>\n",
       "      <td>63006.70</td>\n",
       "      <td>63327.59</td>\n",
       "      <td>1336.93335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002435</td>\n",
       "      <td>0.577133</td>\n",
       "      <td>1.019271</td>\n",
       "      <td>-0.006590</td>\n",
       "      <td>-0.007075</td>\n",
       "      <td>0.022020</td>\n",
       "      <td>-0.003970</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>37.120012</td>\n",
       "      <td>0.706674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01 01:00:00</th>\n",
       "      <td>63458.00</td>\n",
       "      <td>63532.00</td>\n",
       "      <td>63639.86</td>\n",
       "      <td>63370.01</td>\n",
       "      <td>63531.99</td>\n",
       "      <td>1004.08763</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003222</td>\n",
       "      <td>0.345030</td>\n",
       "      <td>0.919524</td>\n",
       "      <td>-0.006591</td>\n",
       "      <td>-0.006960</td>\n",
       "      <td>0.017406</td>\n",
       "      <td>-0.007778</td>\n",
       "      <td>0.006431</td>\n",
       "      <td>45.902577</td>\n",
       "      <td>0.009728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01 02:00:00</th>\n",
       "      <td>63443.76</td>\n",
       "      <td>63458.00</td>\n",
       "      <td>63458.00</td>\n",
       "      <td>63180.00</td>\n",
       "      <td>63458.00</td>\n",
       "      <td>716.11822</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001165</td>\n",
       "      <td>-0.019572</td>\n",
       "      <td>1.003197</td>\n",
       "      <td>-0.006284</td>\n",
       "      <td>-0.006831</td>\n",
       "      <td>0.017210</td>\n",
       "      <td>-0.006673</td>\n",
       "      <td>0.006645</td>\n",
       "      <td>47.891809</td>\n",
       "      <td>1.640393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01 03:00:00</th>\n",
       "      <td>63723.48</td>\n",
       "      <td>63443.76</td>\n",
       "      <td>63744.00</td>\n",
       "      <td>63430.00</td>\n",
       "      <td>63443.76</td>\n",
       "      <td>822.21265</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000224</td>\n",
       "      <td>-0.333418</td>\n",
       "      <td>0.938037</td>\n",
       "      <td>-0.006061</td>\n",
       "      <td>-0.006678</td>\n",
       "      <td>0.015486</td>\n",
       "      <td>-0.006181</td>\n",
       "      <td>0.006556</td>\n",
       "      <td>43.060654</td>\n",
       "      <td>1.358962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01 04:00:00</th>\n",
       "      <td>63868.94</td>\n",
       "      <td>63723.47</td>\n",
       "      <td>63879.81</td>\n",
       "      <td>63652.06</td>\n",
       "      <td>63723.48</td>\n",
       "      <td>778.75286</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004399</td>\n",
       "      <td>-0.218125</td>\n",
       "      <td>0.891219</td>\n",
       "      <td>-0.005808</td>\n",
       "      <td>-0.006481</td>\n",
       "      <td>0.009475</td>\n",
       "      <td>-0.010421</td>\n",
       "      <td>0.006423</td>\n",
       "      <td>37.358784</td>\n",
       "      <td>0.856831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        close      open      high       low  prev_close  \\\n",
       "date_open                                                                 \n",
       "2024-10-01 00:00:00  63531.99  63327.60  63606.00  63006.70    63327.59   \n",
       "2024-10-01 01:00:00  63458.00  63532.00  63639.86  63370.01    63531.99   \n",
       "2024-10-01 02:00:00  63443.76  63458.00  63458.00  63180.00    63458.00   \n",
       "2024-10-01 03:00:00  63723.48  63443.76  63744.00  63430.00    63443.76   \n",
       "2024-10-01 04:00:00  63868.94  63723.47  63879.81  63652.06    63723.48   \n",
       "\n",
       "                         volume  feature_hour_sin  feature_hour_cos  \\\n",
       "date_open                                                             \n",
       "2024-10-01 00:00:00  1336.93335          0.000000          1.000000   \n",
       "2024-10-01 01:00:00  1004.08763          0.258819          0.965926   \n",
       "2024-10-01 02:00:00   716.11822          0.500000          0.866025   \n",
       "2024-10-01 03:00:00   822.21265          0.707107          0.707107   \n",
       "2024-10-01 04:00:00   778.75286          0.866025          0.500000   \n",
       "\n",
       "                     feature_day_sin  feature_day_cos  ...  \\\n",
       "date_open                                              ...   \n",
       "2024-10-01 00:00:00         0.781831          0.62349  ...   \n",
       "2024-10-01 01:00:00         0.781831          0.62349  ...   \n",
       "2024-10-01 02:00:00         0.781831          0.62349  ...   \n",
       "2024-10-01 03:00:00         0.781831          0.62349  ...   \n",
       "2024-10-01 04:00:00         0.781831          0.62349  ...   \n",
       "\n",
       "                     feature_log_return_1h  feature_volume_zscore  \\\n",
       "date_open                                                           \n",
       "2024-10-01 00:00:00              -0.002435               0.577133   \n",
       "2024-10-01 01:00:00               0.003222               0.345030   \n",
       "2024-10-01 02:00:00              -0.001165              -0.019572   \n",
       "2024-10-01 03:00:00              -0.000224              -0.333418   \n",
       "2024-10-01 04:00:00               0.004399              -0.218125   \n",
       "\n",
       "                     feature_obv_zscore  feature_MACD  feature_MACD_Signal  \\\n",
       "date_open                                                                    \n",
       "2024-10-01 00:00:00            1.019271     -0.006590            -0.007075   \n",
       "2024-10-01 01:00:00            0.919524     -0.006591            -0.006960   \n",
       "2024-10-01 02:00:00            1.003197     -0.006284            -0.006831   \n",
       "2024-10-01 03:00:00            0.938037     -0.006061            -0.006678   \n",
       "2024-10-01 04:00:00            0.891219     -0.005808            -0.006481   \n",
       "\n",
       "                     feature_BB_Upper  feature_BB_Lower  feature_atr  \\\n",
       "date_open                                                              \n",
       "2024-10-01 00:00:00          0.022020         -0.003970     0.006462   \n",
       "2024-10-01 01:00:00          0.017406         -0.007778     0.006431   \n",
       "2024-10-01 02:00:00          0.017210         -0.006673     0.006645   \n",
       "2024-10-01 03:00:00          0.015486         -0.006181     0.006556   \n",
       "2024-10-01 04:00:00          0.009475         -0.010421     0.006423   \n",
       "\n",
       "                     feature_rsi  feature_sharpe_ratio  \n",
       "date_open                                               \n",
       "2024-10-01 00:00:00    37.120012              0.706674  \n",
       "2024-10-01 01:00:00    45.902577              0.009728  \n",
       "2024-10-01 02:00:00    47.891809              1.640393  \n",
       "2024-10-01 03:00:00    43.060654              1.358962  \n",
       "2024-10-01 04:00:00    37.358784              0.856831  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38a89d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>prev_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>feature_hour_sin</th>\n",
       "      <th>feature_hour_cos</th>\n",
       "      <th>feature_day_sin</th>\n",
       "      <th>feature_day_cos</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_log_return_1h</th>\n",
       "      <th>feature_volume_zscore</th>\n",
       "      <th>feature_obv_zscore</th>\n",
       "      <th>feature_MACD</th>\n",
       "      <th>feature_MACD_Signal</th>\n",
       "      <th>feature_BB_Upper</th>\n",
       "      <th>feature_BB_Lower</th>\n",
       "      <th>feature_atr</th>\n",
       "      <th>feature_rsi</th>\n",
       "      <th>feature_sharpe_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_open</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-10-01 00:00:00</th>\n",
       "      <td>114239.53</td>\n",
       "      <td>114048.94</td>\n",
       "      <td>114308.00</td>\n",
       "      <td>113966.67</td>\n",
       "      <td>114048.93</td>\n",
       "      <td>434.59016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>-0.564066</td>\n",
       "      <td>-1.216313</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.005904</td>\n",
       "      <td>-0.013626</td>\n",
       "      <td>0.005157</td>\n",
       "      <td>55.073954</td>\n",
       "      <td>2.843994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 01:00:00</th>\n",
       "      <td>114549.99</td>\n",
       "      <td>114239.53</td>\n",
       "      <td>114550.00</td>\n",
       "      <td>114142.99</td>\n",
       "      <td>114239.53</td>\n",
       "      <td>597.25360</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>-0.190021</td>\n",
       "      <td>-1.176929</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.001652</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>-0.015077</td>\n",
       "      <td>0.004875</td>\n",
       "      <td>64.896929</td>\n",
       "      <td>3.274433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 02:00:00</th>\n",
       "      <td>114272.15</td>\n",
       "      <td>114549.99</td>\n",
       "      <td>114551.76</td>\n",
       "      <td>114272.15</td>\n",
       "      <td>114549.99</td>\n",
       "      <td>508.42422</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.174400</td>\n",
       "      <td>-1.115457</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>-0.017825</td>\n",
       "      <td>0.004807</td>\n",
       "      <td>65.297454</td>\n",
       "      <td>3.203920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 03:00:00</th>\n",
       "      <td>114176.92</td>\n",
       "      <td>114272.16</td>\n",
       "      <td>114530.48</td>\n",
       "      <td>114096.58</td>\n",
       "      <td>114272.15</td>\n",
       "      <td>502.30318</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002428</td>\n",
       "      <td>-0.023705</td>\n",
       "      <td>-1.032361</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.004594</td>\n",
       "      <td>-0.015799</td>\n",
       "      <td>0.004870</td>\n",
       "      <td>66.683406</td>\n",
       "      <td>3.335955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 04:00:00</th>\n",
       "      <td>114289.01</td>\n",
       "      <td>114176.93</td>\n",
       "      <td>114700.00</td>\n",
       "      <td>114151.00</td>\n",
       "      <td>114176.92</td>\n",
       "      <td>597.89328</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000834</td>\n",
       "      <td>-0.036563</td>\n",
       "      <td>-1.099294</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>0.005860</td>\n",
       "      <td>-0.015073</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>63.496290</td>\n",
       "      <td>3.446608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         close       open       high        low  prev_close  \\\n",
       "date_open                                                                     \n",
       "2025-10-01 00:00:00  114239.53  114048.94  114308.00  113966.67   114048.93   \n",
       "2025-10-01 01:00:00  114549.99  114239.53  114550.00  114142.99   114239.53   \n",
       "2025-10-01 02:00:00  114272.15  114549.99  114551.76  114272.15   114549.99   \n",
       "2025-10-01 03:00:00  114176.92  114272.16  114530.48  114096.58   114272.15   \n",
       "2025-10-01 04:00:00  114289.01  114176.93  114700.00  114151.00   114176.92   \n",
       "\n",
       "                        volume  feature_hour_sin  feature_hour_cos  \\\n",
       "date_open                                                            \n",
       "2025-10-01 00:00:00  434.59016          0.000000          1.000000   \n",
       "2025-10-01 01:00:00  597.25360          0.258819          0.965926   \n",
       "2025-10-01 02:00:00  508.42422          0.500000          0.866025   \n",
       "2025-10-01 03:00:00  502.30318          0.707107          0.707107   \n",
       "2025-10-01 04:00:00  597.89328          0.866025          0.500000   \n",
       "\n",
       "                     feature_day_sin  feature_day_cos  ...  \\\n",
       "date_open                                              ...   \n",
       "2025-10-01 00:00:00         0.974928        -0.222521  ...   \n",
       "2025-10-01 01:00:00         0.974928        -0.222521  ...   \n",
       "2025-10-01 02:00:00         0.974928        -0.222521  ...   \n",
       "2025-10-01 03:00:00         0.974928        -0.222521  ...   \n",
       "2025-10-01 04:00:00         0.974928        -0.222521  ...   \n",
       "\n",
       "                     feature_log_return_1h  feature_volume_zscore  \\\n",
       "date_open                                                           \n",
       "2025-10-01 00:00:00               0.000952              -0.564066   \n",
       "2025-10-01 01:00:00               0.001670              -0.190021   \n",
       "2025-10-01 02:00:00               0.002714               0.174400   \n",
       "2025-10-01 03:00:00              -0.002428              -0.023705   \n",
       "2025-10-01 04:00:00              -0.000834              -0.036563   \n",
       "\n",
       "                     feature_obv_zscore  feature_MACD  feature_MACD_Signal  \\\n",
       "date_open                                                                    \n",
       "2025-10-01 00:00:00           -1.216313      0.001966             0.001569   \n",
       "2025-10-01 01:00:00           -1.176929      0.001997             0.001652   \n",
       "2025-10-01 02:00:00           -1.115457      0.002129             0.001744   \n",
       "2025-10-01 03:00:00           -1.032361      0.002434             0.001885   \n",
       "2025-10-01 04:00:00           -1.099294      0.002449             0.001999   \n",
       "\n",
       "                     feature_BB_Upper  feature_BB_Lower  feature_atr  \\\n",
       "date_open                                                              \n",
       "2025-10-01 00:00:00          0.005904         -0.013626     0.005157   \n",
       "2025-10-01 01:00:00          0.003745         -0.015077     0.004875   \n",
       "2025-10-01 02:00:00          0.001232         -0.017825     0.004807   \n",
       "2025-10-01 03:00:00          0.004594         -0.015799     0.004870   \n",
       "2025-10-01 04:00:00          0.005860         -0.015073     0.004883   \n",
       "\n",
       "                     feature_rsi  feature_sharpe_ratio  \n",
       "date_open                                               \n",
       "2025-10-01 00:00:00    55.073954              2.843994  \n",
       "2025-10-01 01:00:00    64.896929              3.274433  \n",
       "2025-10-01 02:00:00    65.297454              3.203920  \n",
       "2025-10-01 03:00:00    66.683406              3.335955  \n",
       "2025-10-01 04:00:00    63.496290              3.446608  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e32eae8",
   "metadata": {},
   "source": [
    "### Setting up the Trading Environment\n",
    "\n",
    "We use the `df_train` DataFrame for training and `df_eval` DataFrame for evaluation.\n",
    "\n",
    "The `positions` parameter defines the discrete positions the agent can take, it is a list containing possible position values. A position value corresponds to the ratio of the portfolio valuation engaged in the position ( > 0 to bet on the rise, < 0 to bet on the decrease)\n",
    "\n",
    "- if `position < 0` : the agent is shorting the asset\n",
    "- if `position = 0` : the agent is out of the market\n",
    "- if `position > 0` : the agent is longing the asset\n",
    "- if `position = 1` : the agent is fully invested in the asset\n",
    "- if `position > 1` : the agent is using leverage to invest more than its portfolio valuation in the asset\n",
    "\n",
    "You are free to modify the `positions` list to experiment with different position options for the agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6e342f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIONS = [-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1]\n",
    "WINDOW_SIZE = 48\n",
    "\n",
    "env_train = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD\",\n",
    "        df = df_train, # Your dataset with your custom features\n",
    "        positions = POSITIONS,\n",
    "        trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "        borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "    )\n",
    "\n",
    "env_eval = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD\",\n",
    "        df = df_eval, # Your dataset with your custom features\n",
    "        positions = POSITIONS,\n",
    "        trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "        borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72635801",
   "metadata": {},
   "source": [
    "### Example of interacting with the Environment\n",
    "\n",
    "The interaction with the environment follows the standard Gymnasium API. At each time step, the agent selects an action (position index) based on the current observation, and the environment returns the next observation, reward, and done flag.\n",
    "\n",
    "The following code block demonstrates a simple interaction loop where the agent randomly selects actions.\n",
    "\n",
    "The episode is terminated when `done` or `truncated` is `True`:\n",
    "\n",
    "- When environment reaches the end of the dataset, `truncated` is set to `True`.\n",
    "- When agent's portfolio valuation drops below 0, `done` is set to `True`. (This means the agent has gone bankrupt, the situdation can happen when high leverage is used.)\n",
    "\n",
    "The reward at each time step is calculated based on the change in portfolio valuation, taking into account trading fees and borrow interest rates: $r_{t} = ln(\\frac{p_{t}}{p_{t-1}})\\text{ with }p_{t}\\text{ = portofolio valuation at timestep }t$. You can customize your own reward function if needed (see https://gym-trading-env.readthedocs.io/en/latest/customization.html#custom-reward-function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb04ddd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : -62.14%   |   \n"
     ]
    }
   ],
   "source": [
    "done, truncated = False, False\n",
    "observation, info = env_train.reset()\n",
    "while not done and not truncated:\n",
    "    # Pick a position by its index in your position list\n",
    "    position_index = env_train.action_space.sample()\n",
    "    observation, reward, done, truncated, info = env_train.step(position_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3568723f",
   "metadata": {},
   "source": [
    "### DRL Agent Training and Evaluation\n",
    "\n",
    "You will then implement your DRL agent, train it using the training environment, and evaluate its performance using the evaluation environment.\n",
    "\n",
    "Your agent should include a method `choose_action_eval` for selecting actions based on state during evaluation. It is called in the evaluation function.\n",
    "\n",
    "```python\n",
    "    def choose_action_eval(self, state):\n",
    "        # Implement action selection logic for evaluation\n",
    "        ...\n",
    "        return action_index\n",
    "```\n",
    "\n",
    "After training, you can evaluate your agent using the provided `evaluate_agent` function, which runs the agent in the evaluation environment for a specified number of episodes and records the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f9eb382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "    def choose_action_eval(self, state):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c0dbde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env, num_episodes=10, max_steps=None, render=False, csv_path=\"evaluation_results.csv\", renderer_logs_dir=\"render_logs\"):\n",
    "    \"\"\"\n",
    "    Evaluate the agent on the environment for a number of episodes.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    for ep in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        step = 0\n",
    "        reward_total = 0.0\n",
    "        while not done and not truncated:\n",
    "            action = agent.choose_action_eval(obs)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            reward_total += reward\n",
    "            step += 1\n",
    "            if (max_steps is not None) and (step >= max_steps):\n",
    "                break\n",
    "\n",
    "        # Get metrics from the environment\n",
    "        metrics = env.get_metrics()  \n",
    "        # Transform metrics\n",
    "        # Assume metrics contain keys \"Portfolio Return\" and \"Market Return\" as strings like \"45.24%\"\n",
    "        port_ret = float(metrics[\"Portfolio Return\"].strip('%')) / 100.0\n",
    "        market_ret = float(metrics[\"Market Return\"].strip('%')) / 100.0\n",
    "\n",
    "        results.append({\n",
    "            \"episode\": ep+1,\n",
    "            \"portfolio_return\": port_ret,\n",
    "            \"market_return\": market_ret,\n",
    "            \"excess_return\": port_ret - market_ret,\n",
    "            \"steps\": step,\n",
    "            \"total_reward\": reward_total,\n",
    "        })\n",
    "        if render:\n",
    "            print(f\"Eval Episode {ep+1}: Total Reward: {reward_total:.2f}, Portfolio Return: {port_ret:.2%}, Market Return: {market_ret:.2%}, Excess Return: {(port_ret - market_ret):.2%}, Steps: {step}\")\n",
    "            time.sleep(1)  # Pause between episodes in case the execution is too fast and files are not saved properly\n",
    "            env.save_for_render(dir = renderer_logs_dir)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    df_results.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved submission to {csv_path}\")\n",
    "\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a031775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : -3.63%   |   Portfolio Return : -6.32%   |   \n",
      "Eval Episode 1: Total Reward: -0.07, Portfolio Return: -6.32%, Market Return: -3.63%, Excess Return: -2.69%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -12.30%   |   \n",
      "Eval Episode 2: Total Reward: -0.13, Portfolio Return: -12.30%, Market Return: -3.63%, Excess Return: -8.67%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -9.65%   |   \n",
      "Eval Episode 3: Total Reward: -0.10, Portfolio Return: -9.65%, Market Return: -3.63%, Excess Return: -6.02%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -8.00%   |   \n",
      "Eval Episode 4: Total Reward: -0.08, Portfolio Return: -8.00%, Market Return: -3.63%, Excess Return: -4.37%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -2.76%   |   \n",
      "Eval Episode 5: Total Reward: -0.03, Portfolio Return: -2.76%, Market Return: -3.63%, Excess Return: 0.87%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -3.78%   |   \n",
      "Eval Episode 6: Total Reward: -0.04, Portfolio Return: -3.78%, Market Return: -3.63%, Excess Return: -0.15%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -4.01%   |   \n",
      "Eval Episode 7: Total Reward: -0.04, Portfolio Return: -4.01%, Market Return: -3.63%, Excess Return: -0.38%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.43%   |   \n",
      "Eval Episode 8: Total Reward: 0.03, Portfolio Return: 3.43%, Market Return: -3.63%, Excess Return: 7.06%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -12.75%   |   \n",
      "Eval Episode 9: Total Reward: -0.14, Portfolio Return: -12.75%, Market Return: -3.63%, Excess Return: -9.12%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -3.04%   |   \n",
      "Eval Episode 10: Total Reward: -0.03, Portfolio Return: -3.04%, Market Return: -3.63%, Excess Return: 0.59%, Steps: 767\n",
      "Saved submission to eval/evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a random agent for evaluation\n",
    "agent = RandomAgent(env_eval.action_space)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "df_results = evaluate_agent(agent, env_eval, num_episodes=10, render=True, csv_path=eval_folder / \"evaluation_results.csv\", renderer_logs_dir=eval_folder / \"render_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02cd61ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, hidden_size=128, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.window_size = state_shape[0]\n",
    "        self.n_features = state_shape[1]\n",
    "\n",
    "        # --- 1. Feature Extractor ---\n",
    "        # If we have a time sequence (Window > 1), use CNN\n",
    "        if self.window_size > 1:\n",
    "            self.feature_extractor = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=self.n_features, out_channels=32, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            self.flattened_size = 64 * self.window_size\n",
    "            \n",
    "        # If we have a snapshot (Window == 1), use a Linear Layer (MLP)\n",
    "        else:\n",
    "            self.feature_extractor = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(self.n_features * self.window_size, 64),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.flattened_size = 64\n",
    "\n",
    "        # --- 2. Shared Linear Layers ---\n",
    "        layers = []\n",
    "        input_dim = self.flattened_size\n",
    "        \n",
    "        for _ in range(n_layers):\n",
    "            layers.append(nn.Linear(input_dim, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_size\n",
    "            \n",
    "        self.shared_linear = nn.Sequential(*layers)\n",
    "\n",
    "        # --- 3. Heads ---\n",
    "        self.actor = nn.Linear(hidden_size, n_actions)\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, Window, Features)\n",
    "        \n",
    "        if self.window_size > 1:\n",
    "            # Permute for Conv1d: (Batch, Features, Window)\n",
    "            x = x.permute(0, 2, 1) \n",
    "        \n",
    "        # Pass through specific extractor (CNN or MLP)\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # Pass through shared layers\n",
    "        shared_features = self.shared_linear(features)\n",
    "\n",
    "        action_logits = self.actor(shared_features) \n",
    "        state_value = self.critic(shared_features)\n",
    "\n",
    "        return action_logits, state_value\n",
    "class PPOAgent:\n",
    "    def __init__(\n",
    "        self, state_size, n_actions,\n",
    "        lr=3e-4, gamma=0.99, gae_lambda=0.95,\n",
    "        entropy_beta=0.01, clip_epsilon=0.2, ppo_epochs=10, batch_size=64,\n",
    "        hidden_size=128,\n",
    "        n_layers=2  # <<< --- 1. ADD THIS (with a default)\n",
    "    ):\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.entropy_beta = entropy_beta\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Environment parameters\n",
    "        self.state_size = state_size\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        # Device configuration\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")  \n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        # Create policy network\n",
    "        self.network = ActorCriticNetwork(\n",
    "            state_size, \n",
    "            n_actions, \n",
    "            hidden_size, \n",
    "            n_layers  \n",
    "        ).to(self.device)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "\n",
    "        # Memory buffers\n",
    "        self.reset_memory()\n",
    "\n",
    "    def reset_memory(self):\n",
    "        \"\"\"Clear rollout buffers.\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.log_probs = []\n",
    "\n",
    "    def get_action_value_logprob(self, state):\n",
    "        \"\"\"\n",
    "        Samples an action for the training loop.\n",
    "        Returns the action, its value, and log probability.\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, value = self.network(state_tensor)\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action.item(), value.item(), log_prob.item()\n",
    "\n",
    "    def choose_action_eval(self, state):\n",
    "        \"\"\"\n",
    "        Chooses the best action for evaluation (deterministic).\n",
    "        Returns only the action index.\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, _ = self.network(state_tensor)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        action = torch.argmax(probs, dim=-1)\n",
    "        \n",
    "        return action.item()\n",
    "\n",
    "    def store(self, state, action, reward, value, done, log_prob):\n",
    "        \"\"\"Store a single transition in memory.\"\"\"\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.dones.append(done)\n",
    "        self.log_probs.append(log_prob)\n",
    "\n",
    "    def compute_gae(self, next_value):\n",
    "        \"\"\"\n",
    "        Compute returns and advantages using GAE (Generalized Advantage Estimation)\n",
    "        \"\"\"\n",
    "        rewards = np.array(self.rewards, dtype=np.float32)\n",
    "        values = np.array(self.values + [next_value], dtype=np.float32)\n",
    "        dones = np.array(self.dones, dtype=np.float32)\n",
    "\n",
    "        T = len(rewards)\n",
    "        returns = np.zeros(T, dtype=np.float32)\n",
    "        advantages = np.zeros(T, dtype=np.float32)\n",
    "\n",
    "        gae = 0.0\n",
    "        for t in reversed(range(T)):\n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1.0 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1.0 - dones[t]) * gae\n",
    "            advantages[t] = gae\n",
    "            returns[t] = advantages[t] + values[t]\n",
    "\n",
    "        return returns, advantages\n",
    "\n",
    "    def update(self, next_value):\n",
    "        \"\"\"Perform one PPO update step.\"\"\"\n",
    "        if len(self.states) == 0:\n",
    "            return {\"actor_loss\": 0.0, \"critic_loss\": 0.0}\n",
    "\n",
    "        returns, advantages = self.compute_gae(next_value)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states = torch.tensor(np.array(self.states), dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(np.array(self.actions), dtype=torch.int64, device=self.device)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32, device=self.device)\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32, device=self.device)\n",
    "        old_log_probs = torch.tensor(np.array(self.log_probs), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        total_actor_loss = 0\n",
    "        total_critic_loss = 0\n",
    "        updates = 0\n",
    "        \n",
    "        for _ in range(self.ppo_epochs):\n",
    "            indices = torch.randperm(len(states))\n",
    "            \n",
    "            for start in range(0, len(states), self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                idx = indices[start:end]\n",
    "                \n",
    "                if len(idx) == 0:\n",
    "                    continue\n",
    "\n",
    "                batch_states = states[idx]\n",
    "                batch_actions = actions[idx]\n",
    "                batch_old_log_probs = old_log_probs[idx]\n",
    "                batch_returns = returns[idx]\n",
    "                batch_advantages = advantages[idx]\n",
    "                \n",
    "                # Forward pass\n",
    "                logits, values = self.network(batch_states)\n",
    "                action_probs = F.softmax(logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(action_probs)\n",
    "                log_probs = dist.log_prob(batch_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # PPO loss computation\n",
    "                ratio = torch.exp(log_probs - batch_old_log_probs)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                critic_loss = (batch_returns - values.squeeze()).pow(2).mean()\n",
    "                \n",
    "                loss = actor_loss + 0.5 * critic_loss - self.entropy_beta * entropy\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_actor_loss += actor_loss.item()\n",
    "                total_critic_loss += critic_loss.item()\n",
    "                updates += 1\n",
    "        \n",
    "        self.reset_memory()\n",
    "        \n",
    "        if updates == 0:\n",
    "            return {\"actor_loss\": 0.0, \"critic_loss\": 0.0}\n",
    "\n",
    "        return {\n",
    "            'actor_loss': total_actor_loss / updates,\n",
    "            'critic_loss': total_critic_loss / updates\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7dd65a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model with hyperparameters: {'lr': 0.00038347659965876475, 'gamma': 0.98, 'gae_lambda': 0.889645947951959, 'entropy_beta': 0.001448579588369665, 'clip_epsilon': 0.12948752167104155, 'ppo_epochs': 9, 'batch_size': 64, 'hidden_size': 512, 'n_layers': 4}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m n_actions = env_eval.action_space.n\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading best model with hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_hps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m trained_agent = \u001b[43mPPOAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbest_hps\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# --- 2. Load the saved model weights ---\u001b[39;00m\n\u001b[32m     13\u001b[39m MODEL_SAVE_PATH = \u001b[33m\"\u001b[39m\u001b[33mmodels/ppo_trading_agent_v2.pth\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mPPOAgent.__init__\u001b[39m\u001b[34m(self, state_size, n_actions, lr, gamma, gae_lambda, entropy_beta, clip_epsilon, ppo_epochs, batch_size, hidden_size, n_layers)\u001b[39m\n\u001b[32m     86\u001b[39m     \u001b[38;5;28mself\u001b[39m.device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Create policy network\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28mself\u001b[39m.network = \u001b[43mActorCriticNetwork\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# Optimizer\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer = optim.Adam(\u001b[38;5;28mself\u001b[39m.network.parameters(), lr=lr)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mActorCriticNetwork.__init__\u001b[39m\u001b[34m(self, state_shape, n_actions, hidden_size, n_layers)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m      4\u001b[39m \u001b[38;5;28mself\u001b[39m.window_size = state_shape[\u001b[32m0\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28mself\u001b[39m.n_features = \u001b[43mstate_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# --- 1. Feature Extractor ---\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# If we have a time sequence (Window > 1), use CNN\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.window_size > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# (best_hps should be the dictionary you got from your Optuna study)\n",
    "best_hps = {'lr': 0.00038347659965876475, 'gamma': 0.98, 'gae_lambda': 0.889645947951959, 'entropy_beta': 0.001448579588369665, 'clip_epsilon': 0.12948752167104155, 'ppo_epochs': 9, 'batch_size': 64, 'hidden_size': 512, 'n_layers': 4}\n",
    "state_shape = env_eval.observation_space.shape  # Use the new env\n",
    "n_actions = env_eval.action_space.n\n",
    "print(f\"Loading best model with hyperparameters: {best_hps}\")\n",
    "trained_agent = PPOAgent(\n",
    "    state_size=state_shape, \n",
    "    n_actions=n_actions,\n",
    "    **best_hps  \n",
    ")\n",
    "\n",
    "# --- 2. Load the saved model weights ---\n",
    "MODEL_SAVE_PATH = \"models/ppo_trading_agent_v2.pth\"\n",
    "trained_agent.network.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "# Set the network to evaluation mode (this is correct)\n",
    "trained_agent.network.eval()\n",
    "\n",
    "print(\"--- Evaluating final trained agent on TEST SET ---\")\n",
    "\n",
    "# --- 3. Evaluate the agent on the unseen TEST set ---\n",
    "df_results = evaluate_agent(\n",
    "    trained_agent,\n",
    "    env_eval,  \n",
    "    num_episodes=20,\n",
    "    render=True,\n",
    "    csv_path=eval_folder / \"evaluation_results.csv\",\n",
    "    renderer_logs_dir=eval_folder / \"render_logs\"\n",
    ")\n",
    "\n",
    "print(\"--- Final Evaluation Complete ---\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4bf452",
   "metadata": {},
   "source": [
    "### Environment rendering\n",
    "\n",
    "Gym Trading Env supports rendering the environment to visualize the agent's trading actions over time. You can enable rendering during evaluation by setting the `render` parameter to `True` in the `evaluate_agent` function. The rendered logs will be saved in the specified directory for later review.\n",
    "\n",
    "To visualize the rendered logs, you can use the built-in rendering tools provided by Gym Trading Env as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb92a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_trading_env.renderer import Renderer\n",
    "renderer = Renderer(render_logs_dir=eval_folder/\"render_logs\")\n",
    "renderer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2148f1",
   "metadata": {},
   "source": [
    "Now you are ready to implement your DRL agent and start training! Good luck 💪"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
