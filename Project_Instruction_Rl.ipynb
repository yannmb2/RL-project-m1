{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b9217a",
   "metadata": {},
   "source": [
    "Intermediate Deep Learning - Deep Reinforcement Learning\n",
    "# Project: Deep Reinforcement Learning in Trading Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8ea223",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this project, you will implement and evaluate a deep reinforcement learning (DRL) agent in a trading environment. The goal is to train an agent that can make profitable trading decisions based on historical market data.\n",
    "\n",
    "You are encouraged to experiment with different DRL algorithms, architectures, and hyperparameters to optimize the agent's performance.\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "You are required to prepare:\n",
    "\n",
    "- Code implementation of the DRL agent(s) and training process. Can be in the form of Jupyter Notebooks or Python scripts.\n",
    "- A report (4 pages max) summarizing your approach, results, and insights gained from the project, including:  \n",
    "    - Description of the DRL algorithm(s) used.\n",
    "    - Training process and hyperparameter choices.\n",
    "    - Challenges faced and how they were addressed.\n",
    "\n",
    "    Justify your design choices. You are also encouraged to include visualizations of the agent's performance over time, or compare strategies developed with DRL against financial benchmarks.\n",
    "- An evaluation results CSV file `evaluation_results.csv` generated by your agent after training using the provided evaluation function.\n",
    "- A presentation (15 minutes) to showcase your work, findings, and any interesting observations.\n",
    "\n",
    "*The documents are to be submitted in a zip file, before 16/11/2025 11:59PM, and the presentation is scheduled for next session.*\n",
    "\n",
    "### Environment\n",
    "We will use Gym Trading Env as our trading environment. (https://gym-trading-env.readthedocs.io/en/latest/)\n",
    "\n",
    "- This is a gymnasium-compatible environment designed to simulate trading (stocks or crypto) from historical market data.\n",
    "- Its goal is to provide a fast and customizable platform for training RL agents in a trading scenario.\n",
    "\n",
    "We will use BTC/USDT hour step historical data from Binance for training and evaluation. The agent will be evaluated on the period from 2025-10-01 to 2025-11-01.\n",
    "\n",
    "Following code blocks demonstrate how to set up the environment and evaluate your agent.\n",
    "\n",
    "### Grading Criteria\n",
    "- Implementation of the DRL agent and training process (40%)\n",
    "    - DRL algorithm correctly implemented (15%)\n",
    "    - Appropriate training procedure (15%)\n",
    "    - Effective use of hyperparameters (10%)\n",
    "    - 10 % bonus for innovative approaches or techniques\n",
    "- Performance of the agent based on evaluation metrics (30%)\n",
    "    - If the agent shows progress during training (10%)\n",
    "    - If the agent outperforms a random strategy during evaluation (10%)\n",
    "    - If the portfolio return exceeds market return (10%)\n",
    "    - 30%, 20%, 10% bonus for the top 3 agents respectively\n",
    "- Quality and clarity of the report (20%)\n",
    "    - Clear explanation of methods and results (10%)\n",
    "    - Justification of design choices (10%)\n",
    "- Presentation (20%)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bd874",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c1cd5",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd6c17",
   "metadata": {},
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc19c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python3\n",
      "Requirement already satisfied: gym-trading-env in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.3.5)\n",
      "Requirement already satisfied: pandas>=1.5.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gym-trading-env) (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gym-trading-env) (2.3.4)\n",
      "Requirement already satisfied: gymnasium>=0.28.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gym-trading-env) (1.2.2)\n",
      "Requirement already satisfied: flask==2.2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gym-trading-env) (2.2.5)\n",
      "Requirement already satisfied: pyecharts>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gym-trading-env) (2.0.9)\n",
      "Requirement already satisfied: ccxt==3.0.59 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gym-trading-env) (3.0.59)\n",
      "Requirement already satisfied: nest_asyncio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gym-trading-env) (1.6.0)\n",
      "Requirement already satisfied: setuptools>=60.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ccxt==3.0.59->gym-trading-env) (80.9.0)\n",
      "Requirement already satisfied: certifi>=2018.1.18 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ccxt==3.0.59->gym-trading-env) (2025.10.5)\n",
      "Requirement already satisfied: requests>=2.18.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ccxt==3.0.59->gym-trading-env) (2.32.5)\n",
      "Requirement already satisfied: cryptography>=2.6.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ccxt==3.0.59->gym-trading-env) (46.0.3)\n",
      "Requirement already satisfied: aiohttp>=3.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ccxt==3.0.59->gym-trading-env) (3.13.2)\n",
      "Requirement already satisfied: aiodns>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ccxt==3.0.59->gym-trading-env) (3.5.0)\n",
      "Requirement already satisfied: yarl>=1.7.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ccxt==3.0.59->gym-trading-env) (1.22.0)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from flask==2.2.5->gym-trading-env) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from flask==2.2.5->gym-trading-env) (3.1.6)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from flask==2.2.5->gym-trading-env) (2.2.0)\n",
      "Requirement already satisfied: click>=8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from flask==2.2.5->gym-trading-env) (8.3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gymnasium>=0.28.1->gym-trading-env) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gymnasium>=0.28.1->gym-trading-env) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gymnasium>=0.28.1->gym-trading-env) (0.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=1.5.3->gym-trading-env) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=1.5.3->gym-trading-env) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=1.5.3->gym-trading-env) (2025.2)\n",
      "Requirement already satisfied: prettytable in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyecharts>=2.0.2->gym-trading-env) (3.17.0)\n",
      "Requirement already satisfied: simplejson in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyecharts>=2.0.2->gym-trading-env) (3.20.2)\n",
      "Requirement already satisfied: pycares>=4.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiodns>=1.1.1->ccxt==3.0.59->gym-trading-env) (4.11.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp>=3.8->ccxt==3.0.59->gym-trading-env) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp>=3.8->ccxt==3.0.59->gym-trading-env) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp>=3.8->ccxt==3.0.59->gym-trading-env) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp>=3.8->ccxt==3.0.59->gym-trading-env) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp>=3.8->ccxt==3.0.59->gym-trading-env) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp>=3.8->ccxt==3.0.59->gym-trading-env) (0.4.1)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from cryptography>=2.6.1->ccxt==3.0.59->gym-trading-env) (2.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Jinja2>=3.0->flask==2.2.5->gym-trading-env) (3.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.3->gym-trading-env) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.18.4->ccxt==3.0.59->gym-trading-env) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.18.4->ccxt==3.0.59->gym-trading-env) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.18.4->ccxt==3.0.59->gym-trading-env) (2.5.0)\n",
      "Requirement already satisfied: wcwidth in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from prettytable->pyecharts>=2.0.2->gym-trading-env) (0.2.14)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from cffi>=2.0.0->cryptography>=2.6.1->ccxt==3.0.59->gym-trading-env) (2.23)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "!\"{sys.executable}\" -m pip install gym-trading-env torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "bf91412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.downloader import download\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05469c",
   "metadata": {},
   "source": [
    "### Prepare Data Set\n",
    "\n",
    "- Create a data folder to store historical data\n",
    "- Download historical data for BTC/USDT from Binance using the provided utility function.\n",
    "- Preprocess the data to create features. The features (plus two dynamic features: last position taken by the agent, and the current real position) are the state of the environment at each time step.  \n",
    "    *(You can add more features if you want to experiment with different state representations.)*\n",
    "- Select training and evaluation data based on the specified date ranges.  \n",
    "    *(You can modify the training range if you want to experiment with different time periods, however, keep in mind the evaluation period should always be after the training period.)*\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "28cdc5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing complete.\n",
      "Training data shape: (8760, 16)\n",
      "Evaluation data shape: (768, 16)\n",
      "\n",
      "Final DataFrame columns:\n",
      "['open', 'high', 'low', 'close', 'volume', 'feature_close', 'feature_open', 'feature_high', 'feature_low', 'feature_volume', 'Rolling_Sharpe_Ratio', 'feature_MACD', 'feature_MACD_Signal', 'feature_BB_Upper', 'feature_BB_Lower', 'feature_OBV']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Setup Folders ---\n",
    "data_folder = Path(\"data/\")\n",
    "data_folder.mkdir(parents=True, exist_ok=True)\n",
    "eval_folder = Path(\"eval/\")\n",
    "eval_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\"\"\"\n",
    "download(exchange_names = [\"binance\"],\n",
    "    symbols= [\"BTC/USDT\"],\n",
    "    timeframe= \"1h\",\n",
    "    dir = data_folder,\n",
    "    since= datetime.datetime(year= 2020, month=10, day=1),\n",
    ")\"\"\"\n",
    "\n",
    "# Import your fresh data\n",
    "# Assuming 'binance-BTCUSDT-1h.pkl' exists from the download step\n",
    "df = pd.read_pickle(data_folder / \"binance-BTCUSDT-1h.pkl\")\n",
    "\n",
    "\"\"\" Preprocess the data to create features \"\"\"\n",
    "# Create the feature : ( close[t] - close[t-1] )/ close[t-1]\n",
    "df[\"feature_close\"] = df[\"close\"].pct_change()\n",
    "\n",
    "# Create the feature : open[t] / close[t]\n",
    "df[\"feature_open\"] = df[\"open\"]/df[\"close\"]\n",
    "\n",
    "# Create the feature : high[t] / close[t]\n",
    "df[\"feature_high\"] = df[\"high\"]/df[\"close\"]\n",
    "\n",
    "# Create the feature : low[t] / close[t]\n",
    "df[\"feature_low\"] = df[\"low\"]/df[\"close\"]\n",
    "\n",
    " # Create the feature : volume[t] / max(*volume[t-7*24:t+1])\n",
    "df[\"feature_volume\"] = df[\"volume\"] / df[\"volume\"].rolling(7*24).max()\n",
    "\n",
    "# --- New attributs ---\n",
    "# Sharp ratio\n",
    "ANNUALIZATION_FACTOR = 24 * 365 # 8760 hours in a year\n",
    "ROLLING_WINDOW = 7 * 24          # 168 hours (7 days)\n",
    "RISK_FREE_RATE_ANNUAL = 0.04     # Placeholder: 4.0% annual risk-free rate\n",
    "# Convert annual R_f to hourly R_f: (1 + R_f^ann)^(1/T) - 1\n",
    "RISK_FREE_RATE_HOURLY = (1 + RISK_FREE_RATE_ANNUAL)**(1/ANNUALIZATION_FACTOR) - 1\n",
    "df['Excess_Return'] = df['feature_close'] - RISK_FREE_RATE_HOURLY\n",
    "\n",
    "rolling_mean_excess = df['Excess_Return'].rolling(window=ROLLING_WINDOW).mean()\n",
    "rolling_std_excess = df['Excess_Return'].rolling(window=ROLLING_WINDOW).std()\n",
    "# Sharpe Ratio = (Rolling Mean / Rolling Std Dev) * sqrt(T)\n",
    "df['Rolling_Sharpe_Ratio'] = ( rolling_mean_excess / rolling_std_excess) * np.sqrt(ANNUALIZATION_FACTOR)\n",
    "\n",
    "# Moving Average Convergence Divergence (MACD)\n",
    "df['EMA_12'] = df['close'].ewm(span=12, adjust=False).mean()\n",
    "df['EMA_26'] = df['close'].ewm(span=26, adjust=False).mean()\n",
    "df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
    "df['MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "# Create features normalized by close price\n",
    "df['feature_MACD'] = df['MACD'] / df['close']\n",
    "df['feature_MACD_Signal'] = df['MACD_Signal'] / df['close']\n",
    "\n",
    "\n",
    "# Bollinger Bands\n",
    "\n",
    "ROLLING_WINDOW_BB = 20\n",
    "df['BB_Middle'] = df['close'].rolling(window=ROLLING_WINDOW_BB).mean()\n",
    "df['BB_Std'] = df['close'].rolling(window=ROLLING_WINDOW_BB).std()\n",
    "df['BB_Upper'] = df['BB_Middle'] + (df['BB_Std'] * 2)\n",
    "df['BB_Lower'] = df['BB_Middle'] - (df['BB_Std'] * 2)\n",
    "# Create features relative to the close price\n",
    "df['feature_BB_Upper'] = df['BB_Upper'] / df['close']\n",
    "df['feature_BB_Lower'] = df['BB_Lower'] / df['close']\n",
    "\n",
    "\n",
    "# On-Balance Volume (OBV)\n",
    "df['OBV'] = (np.sign(df['close'].diff()) * df['volume']).cumsum().fillna(0)\n",
    "# Normalize OBV (e.g., divide by a rolling max, similar to volume)\n",
    "df['feature_OBV'] = df['OBV'] / df['OBV'].rolling(7*24).max()\n",
    "\n",
    "\n",
    "# --- Final Cleanup ---\n",
    "# Add all intermediate calculation columns to this list to drop them\n",
    "cols_to_drop = [\n",
    "    \"Excess_Return\", \"EMA_12\", \"EMA_26\", \"MACD\", \"MACD_Signal\",\n",
    "    \"BB_Middle\", \"BB_Std\", \"BB_Upper\", \"BB_Lower\", \"OBV\",\"date_close\"\n",
    "]\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "df.dropna(inplace= True) # Clean again!\n",
    "\n",
    "# Your final feature set now includes:\n",
    "# \"feature_close\", \"feature_open\", \"feature_high\", \"feature_low\", \"feature_volume\",\n",
    "# \"Rolling_Sharpe_Ratio\", \"feature_MACD\", \"feature_MACD_Signal\",\n",
    "# \"feature_BB_Upper\", \"feature_BB_Lower\", \"feature_OBV\"\n",
    "\n",
    "# --- Data Splitting ---\n",
    "df_train = df.loc['2024-10-01':'2025-09-30'] # Training data\n",
    "df_eval = df.loc['2025-10-01':'2025-11-01'] # Evaluation data\n",
    "\n",
    "print(\"Data preprocessing complete.\")\n",
    "print(f\"Training data shape: {df_train.shape}\")\n",
    "print(f\"Evaluation data shape: {df_eval.shape}\")\n",
    "print(\"\\nFinal DataFrame columns:\")\n",
    "print(df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ec95a73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8760, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>feature_close</th>\n",
       "      <th>feature_open</th>\n",
       "      <th>feature_high</th>\n",
       "      <th>feature_low</th>\n",
       "      <th>feature_volume</th>\n",
       "      <th>Rolling_Sharpe_Ratio</th>\n",
       "      <th>feature_MACD</th>\n",
       "      <th>feature_MACD_Signal</th>\n",
       "      <th>feature_BB_Upper</th>\n",
       "      <th>feature_BB_Lower</th>\n",
       "      <th>feature_OBV</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_open</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-10-01 00:00:00</th>\n",
       "      <td>63327.60</td>\n",
       "      <td>63606.00</td>\n",
       "      <td>63006.70</td>\n",
       "      <td>63531.99</td>\n",
       "      <td>1336.93335</td>\n",
       "      <td>0.003228</td>\n",
       "      <td>0.996783</td>\n",
       "      <td>1.001165</td>\n",
       "      <td>0.991732</td>\n",
       "      <td>0.287876</td>\n",
       "      <td>1.640393</td>\n",
       "      <td>-0.006277</td>\n",
       "      <td>-0.006823</td>\n",
       "      <td>1.016025</td>\n",
       "      <td>0.992171</td>\n",
       "      <td>0.991183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01 01:00:00</th>\n",
       "      <td>63532.00</td>\n",
       "      <td>63639.86</td>\n",
       "      <td>63370.01</td>\n",
       "      <td>63458.00</td>\n",
       "      <td>1004.08763</td>\n",
       "      <td>-0.001165</td>\n",
       "      <td>1.001166</td>\n",
       "      <td>1.002866</td>\n",
       "      <td>0.998613</td>\n",
       "      <td>0.216205</td>\n",
       "      <td>1.358962</td>\n",
       "      <td>-0.006059</td>\n",
       "      <td>-0.006677</td>\n",
       "      <td>1.015258</td>\n",
       "      <td>0.993596</td>\n",
       "      <td>0.990114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01 02:00:00</th>\n",
       "      <td>63458.00</td>\n",
       "      <td>63458.00</td>\n",
       "      <td>63180.00</td>\n",
       "      <td>63443.76</td>\n",
       "      <td>716.11822</td>\n",
       "      <td>-0.000224</td>\n",
       "      <td>1.000224</td>\n",
       "      <td>1.000224</td>\n",
       "      <td>0.995843</td>\n",
       "      <td>0.154198</td>\n",
       "      <td>0.856832</td>\n",
       "      <td>-0.005833</td>\n",
       "      <td>-0.006509</td>\n",
       "      <td>1.013925</td>\n",
       "      <td>0.993942</td>\n",
       "      <td>0.989351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01 03:00:00</th>\n",
       "      <td>63443.76</td>\n",
       "      <td>63744.00</td>\n",
       "      <td>63430.00</td>\n",
       "      <td>63723.48</td>\n",
       "      <td>822.21265</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>0.995610</td>\n",
       "      <td>1.000322</td>\n",
       "      <td>0.995394</td>\n",
       "      <td>0.177043</td>\n",
       "      <td>1.563768</td>\n",
       "      <td>-0.005214</td>\n",
       "      <td>-0.006227</td>\n",
       "      <td>1.006489</td>\n",
       "      <td>0.991249</td>\n",
       "      <td>0.990227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01 04:00:00</th>\n",
       "      <td>63723.47</td>\n",
       "      <td>63879.81</td>\n",
       "      <td>63652.06</td>\n",
       "      <td>63868.94</td>\n",
       "      <td>778.75286</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>0.997722</td>\n",
       "      <td>1.000170</td>\n",
       "      <td>0.996604</td>\n",
       "      <td>0.167685</td>\n",
       "      <td>2.021614</td>\n",
       "      <td>-0.004497</td>\n",
       "      <td>-0.005870</td>\n",
       "      <td>1.002350</td>\n",
       "      <td>0.990145</td>\n",
       "      <td>0.991056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close      volume  \\\n",
       "date_open                                                                 \n",
       "2024-10-01 00:00:00  63327.60  63606.00  63006.70  63531.99  1336.93335   \n",
       "2024-10-01 01:00:00  63532.00  63639.86  63370.01  63458.00  1004.08763   \n",
       "2024-10-01 02:00:00  63458.00  63458.00  63180.00  63443.76   716.11822   \n",
       "2024-10-01 03:00:00  63443.76  63744.00  63430.00  63723.48   822.21265   \n",
       "2024-10-01 04:00:00  63723.47  63879.81  63652.06  63868.94   778.75286   \n",
       "\n",
       "                     feature_close  feature_open  feature_high  feature_low  \\\n",
       "date_open                                                                     \n",
       "2024-10-01 00:00:00       0.003228      0.996783      1.001165     0.991732   \n",
       "2024-10-01 01:00:00      -0.001165      1.001166      1.002866     0.998613   \n",
       "2024-10-01 02:00:00      -0.000224      1.000224      1.000224     0.995843   \n",
       "2024-10-01 03:00:00       0.004409      0.995610      1.000322     0.995394   \n",
       "2024-10-01 04:00:00       0.002283      0.997722      1.000170     0.996604   \n",
       "\n",
       "                     feature_volume  Rolling_Sharpe_Ratio  feature_MACD  \\\n",
       "date_open                                                                 \n",
       "2024-10-01 00:00:00        0.287876              1.640393     -0.006277   \n",
       "2024-10-01 01:00:00        0.216205              1.358962     -0.006059   \n",
       "2024-10-01 02:00:00        0.154198              0.856832     -0.005833   \n",
       "2024-10-01 03:00:00        0.177043              1.563768     -0.005214   \n",
       "2024-10-01 04:00:00        0.167685              2.021614     -0.004497   \n",
       "\n",
       "                     feature_MACD_Signal  feature_BB_Upper  feature_BB_Lower  \\\n",
       "date_open                                                                      \n",
       "2024-10-01 00:00:00            -0.006823          1.016025          0.992171   \n",
       "2024-10-01 01:00:00            -0.006677          1.015258          0.993596   \n",
       "2024-10-01 02:00:00            -0.006509          1.013925          0.993942   \n",
       "2024-10-01 03:00:00            -0.006227          1.006489          0.991249   \n",
       "2024-10-01 04:00:00            -0.005870          1.002350          0.990145   \n",
       "\n",
       "                     feature_OBV  \n",
       "date_open                         \n",
       "2024-10-01 00:00:00     0.991183  \n",
       "2024-10-01 01:00:00     0.990114  \n",
       "2024-10-01 02:00:00     0.989351  \n",
       "2024-10-01 03:00:00     0.990227  \n",
       "2024-10-01 04:00:00     0.991056  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "38a89d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>feature_close</th>\n",
       "      <th>feature_open</th>\n",
       "      <th>feature_high</th>\n",
       "      <th>feature_low</th>\n",
       "      <th>feature_volume</th>\n",
       "      <th>Rolling_Sharpe_Ratio</th>\n",
       "      <th>feature_MACD</th>\n",
       "      <th>feature_MACD_Signal</th>\n",
       "      <th>feature_BB_Upper</th>\n",
       "      <th>feature_BB_Lower</th>\n",
       "      <th>feature_OBV</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_open</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-10-01 00:00:00</th>\n",
       "      <td>114048.94</td>\n",
       "      <td>114308.00</td>\n",
       "      <td>113966.67</td>\n",
       "      <td>114239.53</td>\n",
       "      <td>434.59016</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.998332</td>\n",
       "      <td>1.000599</td>\n",
       "      <td>0.997612</td>\n",
       "      <td>0.149874</td>\n",
       "      <td>3.203921</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>1.003953</td>\n",
       "      <td>0.984844</td>\n",
       "      <td>0.996682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 01:00:00</th>\n",
       "      <td>114239.53</td>\n",
       "      <td>114550.00</td>\n",
       "      <td>114142.99</td>\n",
       "      <td>114549.99</td>\n",
       "      <td>597.25360</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>0.997290</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996447</td>\n",
       "      <td>0.205971</td>\n",
       "      <td>3.335956</td>\n",
       "      <td>0.002428</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>1.002157</td>\n",
       "      <td>0.981814</td>\n",
       "      <td>0.997216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 02:00:00</th>\n",
       "      <td>114549.99</td>\n",
       "      <td>114551.76</td>\n",
       "      <td>114272.15</td>\n",
       "      <td>114272.15</td>\n",
       "      <td>508.42422</td>\n",
       "      <td>-0.002425</td>\n",
       "      <td>1.002431</td>\n",
       "      <td>1.002447</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.175337</td>\n",
       "      <td>3.446609</td>\n",
       "      <td>0.002447</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>1.005022</td>\n",
       "      <td>0.984106</td>\n",
       "      <td>0.996761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 03:00:00</th>\n",
       "      <td>114272.16</td>\n",
       "      <td>114530.48</td>\n",
       "      <td>114096.58</td>\n",
       "      <td>114176.92</td>\n",
       "      <td>502.30318</td>\n",
       "      <td>-0.000833</td>\n",
       "      <td>1.000834</td>\n",
       "      <td>1.003097</td>\n",
       "      <td>0.999296</td>\n",
       "      <td>0.173226</td>\n",
       "      <td>4.003348</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>1.006270</td>\n",
       "      <td>0.984935</td>\n",
       "      <td>0.996312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-01 04:00:00</th>\n",
       "      <td>114176.93</td>\n",
       "      <td>114700.00</td>\n",
       "      <td>114151.00</td>\n",
       "      <td>114289.01</td>\n",
       "      <td>597.89328</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>0.999019</td>\n",
       "      <td>1.003596</td>\n",
       "      <td>0.998792</td>\n",
       "      <td>0.206191</td>\n",
       "      <td>3.423206</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>1.005869</td>\n",
       "      <td>0.984117</td>\n",
       "      <td>0.996847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          open       high        low      close     volume  \\\n",
       "date_open                                                                    \n",
       "2025-10-01 00:00:00  114048.94  114308.00  113966.67  114239.53  434.59016   \n",
       "2025-10-01 01:00:00  114239.53  114550.00  114142.99  114549.99  597.25360   \n",
       "2025-10-01 02:00:00  114549.99  114551.76  114272.15  114272.15  508.42422   \n",
       "2025-10-01 03:00:00  114272.16  114530.48  114096.58  114176.92  502.30318   \n",
       "2025-10-01 04:00:00  114176.93  114700.00  114151.00  114289.01  597.89328   \n",
       "\n",
       "                     feature_close  feature_open  feature_high  feature_low  \\\n",
       "date_open                                                                     \n",
       "2025-10-01 00:00:00       0.001671      0.998332      1.000599     0.997612   \n",
       "2025-10-01 01:00:00       0.002718      0.997290      1.000000     0.996447   \n",
       "2025-10-01 02:00:00      -0.002425      1.002431      1.002447     1.000000   \n",
       "2025-10-01 03:00:00      -0.000833      1.000834      1.003097     0.999296   \n",
       "2025-10-01 04:00:00       0.000982      0.999019      1.003596     0.998792   \n",
       "\n",
       "                     feature_volume  Rolling_Sharpe_Ratio  feature_MACD  \\\n",
       "date_open                                                                 \n",
       "2025-10-01 00:00:00        0.149874              3.203921      0.002135   \n",
       "2025-10-01 01:00:00        0.205971              3.335956      0.002428   \n",
       "2025-10-01 02:00:00        0.175337              3.446609      0.002447   \n",
       "2025-10-01 03:00:00        0.173226              4.003348      0.002365   \n",
       "2025-10-01 04:00:00        0.206191              3.423206      0.002348   \n",
       "\n",
       "                     feature_MACD_Signal  feature_BB_Upper  feature_BB_Lower  \\\n",
       "date_open                                                                      \n",
       "2025-10-01 00:00:00             0.001749          1.003953          0.984844   \n",
       "2025-10-01 01:00:00             0.001881          1.002157          0.981814   \n",
       "2025-10-01 02:00:00             0.001998          1.005022          0.984106   \n",
       "2025-10-01 03:00:00             0.002073          1.006270          0.984935   \n",
       "2025-10-01 04:00:00             0.002126          1.005869          0.984117   \n",
       "\n",
       "                     feature_OBV  \n",
       "date_open                         \n",
       "2025-10-01 00:00:00     0.996682  \n",
       "2025-10-01 01:00:00     0.997216  \n",
       "2025-10-01 02:00:00     0.996761  \n",
       "2025-10-01 03:00:00     0.996312  \n",
       "2025-10-01 04:00:00     0.996847  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_eval.shape)\n",
    "df_eval.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e32eae8",
   "metadata": {},
   "source": [
    "### Setting up the Trading Environment\n",
    "\n",
    "We use the `df_train` DataFrame for training and `df_eval` DataFrame for evaluation.\n",
    "\n",
    "The `positions` parameter defines the discrete positions the agent can take, it is a list containing possible position values. A position value corresponds to the ratio of the portfolio valuation engaged in the position ( > 0 to bet on the rise, < 0 to bet on the decrease)\n",
    "\n",
    "- if `position < 0` : the agent is shorting the asset\n",
    "- if `position = 0` : the agent is out of the market\n",
    "- if `position > 0` : the agent is longing the asset\n",
    "- if `position = 1` : the agent is fully invested in the asset\n",
    "- if `position > 1` : the agent is using leverage to invest more than its portfolio valuation in the asset\n",
    "\n",
    "You are free to modify the `positions` list to experiment with different position options for the agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a6e342f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIONS = [-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "env_train = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD\",\n",
    "        df = df_train, # Your dataset with your custom features\n",
    "        positions = POSITIONS,\n",
    "        trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "        borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "    )\n",
    "\n",
    "env_eval = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD\",\n",
    "        df = df_eval, # Your dataset with your custom features\n",
    "        positions = POSITIONS,\n",
    "        trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "        borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5c0dbde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env, num_episodes=20, max_steps=None, render=False, csv_path=\"evaluation_results.csv\", renderer_logs_dir=\"render_logs\"):\n",
    "    \"\"\"\n",
    "    Evaluate the agent on the environment for a number of episodes.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Ensure render dir exists\n",
    "    if render:\n",
    "        Path(renderer_logs_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        step = 0\n",
    "        reward_total = 0.0\n",
    "        while not done and not truncated:\n",
    "            action = agent.choose_action_eval(obs)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            reward_total += reward\n",
    "            step += 1\n",
    "            if (max_steps is not None) and (step >= max_steps):\n",
    "                break\n",
    "\n",
    "        metrics = env.get_metrics()\n",
    "        port_ret = float(metrics[\"Portfolio Return\"].strip('%')) / 100.0\n",
    "        market_ret = float(metrics[\"Market Return\"].strip('%')) / 100.0\n",
    "\n",
    "        results.append({\n",
    "            \"episode\": ep + 1,\n",
    "            \"portfolio_return\": port_ret,\n",
    "            \"market_return\": market_ret,\n",
    "            \"excess_return\": port_ret - market_ret,\n",
    "            \"steps\": step,\n",
    "            \"total_reward\": reward_total,\n",
    "        })\n",
    "        \n",
    "        if render:\n",
    "            print(f\"Eval Episode {ep+1}: Total Reward: {reward_total:.2f}, Portfolio Return: {port_ret:.2%}, Market Return: {market_ret:.2%}, Excess Return: {(port_ret - market_ret):.2%}, Steps: {step}\")\n",
    "            time.sleep(1)\n",
    "            env.save_for_render(dir=renderer_logs_dir)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Ensure the directory for the CSV exists\n",
    "    Path(csv_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_results.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved evaluation results to {csv_path}\")\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae1fae",
   "metadata": {},
   "source": [
    "# Random agent\n",
    "We are going to create and test a random agent. This agent will be a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "fa1b7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "    def choose_action_eval(self, state):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4a031775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : -3.63%   |   Portfolio Return : -13.70%   |   \n",
      "Eval Episode 1: Total Reward: -0.15, Portfolio Return: -13.70%, Market Return: -3.63%, Excess Return: -10.07%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  0.12%   |   \n",
      "Eval Episode 2: Total Reward: 0.00, Portfolio Return: 0.12%, Market Return: -3.63%, Excess Return: 3.75%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -7.59%   |   \n",
      "Eval Episode 3: Total Reward: -0.08, Portfolio Return: -7.59%, Market Return: -3.63%, Excess Return: -3.96%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -12.16%   |   \n",
      "Eval Episode 4: Total Reward: -0.13, Portfolio Return: -12.16%, Market Return: -3.63%, Excess Return: -8.53%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -7.04%   |   \n",
      "Eval Episode 5: Total Reward: -0.07, Portfolio Return: -7.04%, Market Return: -3.63%, Excess Return: -3.41%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -3.43%   |   \n",
      "Eval Episode 6: Total Reward: -0.03, Portfolio Return: -3.43%, Market Return: -3.63%, Excess Return: 0.20%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  0.26%   |   \n",
      "Eval Episode 7: Total Reward: 0.00, Portfolio Return: 0.26%, Market Return: -3.63%, Excess Return: 3.89%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -9.20%   |   \n",
      "Eval Episode 8: Total Reward: -0.10, Portfolio Return: -9.20%, Market Return: -3.63%, Excess Return: -5.57%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -22.22%   |   \n",
      "Eval Episode 9: Total Reward: -0.25, Portfolio Return: -22.22%, Market Return: -3.63%, Excess Return: -18.59%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -3.13%   |   \n",
      "Eval Episode 10: Total Reward: -0.03, Portfolio Return: -3.13%, Market Return: -3.63%, Excess Return: 0.50%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -14.43%   |   \n",
      "Eval Episode 11: Total Reward: -0.16, Portfolio Return: -14.43%, Market Return: -3.63%, Excess Return: -10.80%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -5.50%   |   \n",
      "Eval Episode 12: Total Reward: -0.06, Portfolio Return: -5.50%, Market Return: -3.63%, Excess Return: -1.87%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -1.06%   |   \n",
      "Eval Episode 13: Total Reward: -0.01, Portfolio Return: -1.06%, Market Return: -3.63%, Excess Return: 2.57%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -10.32%   |   \n",
      "Eval Episode 14: Total Reward: -0.11, Portfolio Return: -10.32%, Market Return: -3.63%, Excess Return: -6.69%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  5.18%   |   \n",
      "Eval Episode 15: Total Reward: 0.05, Portfolio Return: 5.18%, Market Return: -3.63%, Excess Return: 8.81%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -3.99%   |   \n",
      "Eval Episode 16: Total Reward: -0.04, Portfolio Return: -3.99%, Market Return: -3.63%, Excess Return: -0.36%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -7.71%   |   \n",
      "Eval Episode 17: Total Reward: -0.08, Portfolio Return: -7.71%, Market Return: -3.63%, Excess Return: -4.08%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  6.40%   |   \n",
      "Eval Episode 18: Total Reward: 0.06, Portfolio Return: 6.40%, Market Return: -3.63%, Excess Return: 10.03%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -2.25%   |   \n",
      "Eval Episode 19: Total Reward: -0.02, Portfolio Return: -2.25%, Market Return: -3.63%, Excess Return: 1.38%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return : -13.58%   |   \n",
      "Eval Episode 20: Total Reward: -0.15, Portfolio Return: -13.58%, Market Return: -3.63%, Excess Return: -9.95%, Steps: 767\n",
      "Saved evaluation results to eval/evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a random agent for evaluation\n",
    "agent = RandomAgent(env_eval.action_space)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "df_results = evaluate_agent(agent, env_eval, num_episodes=20, render=True, csv_path=eval_folder / \"evaluation_results.csv\", renderer_logs_dir=eval_folder / \"render_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd5f30c",
   "metadata": {},
   "source": [
    "# PPO agent\n",
    "We are going to use a PPO agent with the new features we added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9aecf006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, state_size, n_actions, hidden_size=128, n_layers=2): \n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(state_size, hidden_size))\n",
    "        layers.append(nn.Tanh())\n",
    "        \n",
    "        # Add (n_layers - 1) hidden layers\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.Tanh())\n",
    "            \n",
    "        # Create the sequential shared network\n",
    "        self.shared = nn.Sequential(*layers)\n",
    "        \n",
    "        # Actor and Critic heads\n",
    "        self.actor = nn.Linear(hidden_size, n_actions)\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.shared(x)\n",
    "        logits = self.actor(features)\n",
    "        action_probs = torch.softmax(logits, dim=-1)\n",
    "        state_value = self.critic(features)\n",
    "        return action_probs, state_value\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(\n",
    "        self, state_size, n_actions,\n",
    "        lr=3e-4, gamma=0.99, gae_lambda=0.95,\n",
    "        entropy_beta=0.01, clip_epsilon=0.2, ppo_epochs=10, batch_size=64,\n",
    "        hidden_size=128,\n",
    "        n_layers=2  # <<< --- 1. ADD THIS (with a default)\n",
    "    ):\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.entropy_beta = entropy_beta\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Environment parameters\n",
    "        self.state_size = state_size\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        # Device configuration\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")  \n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        # Create policy network\n",
    "        self.network = ActorCriticNetwork(\n",
    "            state_size, \n",
    "            n_actions, \n",
    "            hidden_size, \n",
    "            n_layers  \n",
    "        ).to(self.device)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "\n",
    "        # Memory buffers\n",
    "        self.reset_memory()\n",
    "\n",
    "    def reset_memory(self):\n",
    "        \"\"\"Clear rollout buffers.\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.log_probs = []\n",
    "\n",
    "    def get_action_value_logprob(self, state):\n",
    "        \"\"\"\n",
    "        Samples an action for the training loop.\n",
    "        Returns the action, its value, and log probability.\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs, value = self.network(state_tensor)\n",
    "\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action.item(), value.item(), log_prob.item()\n",
    "\n",
    "    def choose_action_eval(self, state):\n",
    "        \"\"\"\n",
    "        Chooses the best action for evaluation (deterministic).\n",
    "        Returns only the action index.\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probs, _ = self.network(state_tensor)\n",
    "        \n",
    "        action = torch.argmax(probs, dim=-1)\n",
    "        \n",
    "        return action.item()\n",
    "\n",
    "    def store(self, state, action, reward, value, done, log_prob):\n",
    "        \"\"\"Store a single transition in memory.\"\"\"\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.dones.append(done)\n",
    "        self.log_probs.append(log_prob)\n",
    "\n",
    "    def compute_gae(self, next_value):\n",
    "        \"\"\"\n",
    "        Compute returns and advantages using GAE (Generalized Advantage Estimation)\n",
    "        \"\"\"\n",
    "        rewards = np.array(self.rewards, dtype=np.float32)\n",
    "        values = np.array(self.values + [next_value], dtype=np.float32)\n",
    "        dones = np.array(self.dones, dtype=np.float32)\n",
    "\n",
    "        T = len(rewards)\n",
    "        returns = np.zeros(T, dtype=np.float32)\n",
    "        advantages = np.zeros(T, dtype=np.float32)\n",
    "\n",
    "        gae = 0.0\n",
    "        for t in reversed(range(T)):\n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1.0 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1.0 - dones[t]) * gae\n",
    "            advantages[t] = gae\n",
    "            returns[t] = advantages[t] + values[t]\n",
    "\n",
    "        return returns, advantages\n",
    "\n",
    "    def update(self, next_value):\n",
    "        \"\"\"Perform one PPO update step.\"\"\"\n",
    "        if len(self.states) == 0:\n",
    "            return {\"actor_loss\": 0.0, \"critic_loss\": 0.0}\n",
    "\n",
    "        returns, advantages = self.compute_gae(next_value)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states = torch.tensor(np.array(self.states), dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(np.array(self.actions), dtype=torch.int64, device=self.device)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32, device=self.device)\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32, device=self.device)\n",
    "        old_log_probs = torch.tensor(np.array(self.log_probs), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        total_actor_loss = 0\n",
    "        total_critic_loss = 0\n",
    "        updates = 0\n",
    "        \n",
    "        for _ in range(self.ppo_epochs):\n",
    "            indices = torch.randperm(len(states))\n",
    "            \n",
    "            for start in range(0, len(states), self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                idx = indices[start:end]\n",
    "                \n",
    "                if len(idx) == 0:\n",
    "                    continue\n",
    "\n",
    "                batch_states = states[idx]\n",
    "                batch_actions = actions[idx]\n",
    "                batch_old_log_probs = old_log_probs[idx]\n",
    "                batch_returns = returns[idx]\n",
    "                batch_advantages = advantages[idx]\n",
    "                \n",
    "                # Forward pass\n",
    "                action_probs, values = self.network(batch_states)\n",
    "                dist = torch.distributions.Categorical(action_probs)\n",
    "                log_probs = dist.log_prob(batch_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # PPO loss computation\n",
    "                ratio = torch.exp(log_probs - batch_old_log_probs)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                critic_loss = (batch_returns - values.squeeze()).pow(2).mean()\n",
    "                \n",
    "                loss = actor_loss + 0.5 * critic_loss - self.entropy_beta * entropy\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_actor_loss += actor_loss.item()\n",
    "                total_critic_loss += critic_loss.item()\n",
    "                updates += 1\n",
    "        \n",
    "        self.reset_memory()\n",
    "        \n",
    "        if updates == 0:\n",
    "            return {\"actor_loss\": 0.0, \"critic_loss\": 0.0}\n",
    "\n",
    "        return {\n",
    "            'actor_loss': total_actor_loss / updates,\n",
    "            'critic_loss': total_critic_loss / updates\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c24c5ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 12\n",
      "Number of actions: 9\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.61%   |   \n",
      "Eval Episode 1: Total Reward: 0.04, Portfolio Return: 3.61%, Market Return: -3.63%, Excess Return: 7.24%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.62%   |   \n",
      "Eval Episode 2: Total Reward: 0.04, Portfolio Return: 3.62%, Market Return: -3.63%, Excess Return: 7.25%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.61%   |   \n",
      "Eval Episode 3: Total Reward: 0.04, Portfolio Return: 3.61%, Market Return: -3.63%, Excess Return: 7.24%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.62%   |   \n",
      "Eval Episode 4: Total Reward: 0.04, Portfolio Return: 3.62%, Market Return: -3.63%, Excess Return: 7.25%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.62%   |   \n",
      "Eval Episode 5: Total Reward: 0.04, Portfolio Return: 3.62%, Market Return: -3.63%, Excess Return: 7.25%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.60%   |   \n",
      "Eval Episode 6: Total Reward: 0.04, Portfolio Return: 3.60%, Market Return: -3.63%, Excess Return: 7.23%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.61%   |   \n",
      "Eval Episode 7: Total Reward: 0.04, Portfolio Return: 3.61%, Market Return: -3.63%, Excess Return: 7.24%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.62%   |   \n",
      "Eval Episode 8: Total Reward: 0.04, Portfolio Return: 3.62%, Market Return: -3.63%, Excess Return: 7.25%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.62%   |   \n",
      "Eval Episode 9: Total Reward: 0.04, Portfolio Return: 3.62%, Market Return: -3.63%, Excess Return: 7.25%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.62%   |   \n",
      "Eval Episode 10: Total Reward: 0.04, Portfolio Return: 3.62%, Market Return: -3.63%, Excess Return: 7.25%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.60%   |   \n",
      "Eval Episode 11: Total Reward: 0.04, Portfolio Return: 3.60%, Market Return: -3.63%, Excess Return: 7.23%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.61%   |   \n",
      "Eval Episode 12: Total Reward: 0.04, Portfolio Return: 3.61%, Market Return: -3.63%, Excess Return: 7.24%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.61%   |   \n",
      "Eval Episode 13: Total Reward: 0.04, Portfolio Return: 3.61%, Market Return: -3.63%, Excess Return: 7.24%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.62%   |   \n",
      "Eval Episode 14: Total Reward: 0.04, Portfolio Return: 3.62%, Market Return: -3.63%, Excess Return: 7.25%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.61%   |   \n",
      "Eval Episode 15: Total Reward: 0.04, Portfolio Return: 3.61%, Market Return: -3.63%, Excess Return: 7.24%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.61%   |   \n",
      "Eval Episode 16: Total Reward: 0.04, Portfolio Return: 3.61%, Market Return: -3.63%, Excess Return: 7.24%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.61%   |   \n",
      "Eval Episode 17: Total Reward: 0.04, Portfolio Return: 3.61%, Market Return: -3.63%, Excess Return: 7.24%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.62%   |   \n",
      "Eval Episode 18: Total Reward: 0.04, Portfolio Return: 3.62%, Market Return: -3.63%, Excess Return: 7.25%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.60%   |   \n",
      "Eval Episode 19: Total Reward: 0.04, Portfolio Return: 3.60%, Market Return: -3.63%, Excess Return: 7.23%, Steps: 767\n",
      "Market Return : -3.63%   |   Portfolio Return :  3.62%   |   \n",
      "Eval Episode 20: Total Reward: 0.04, Portfolio Return: 3.62%, Market Return: -3.63%, Excess Return: 7.25%, Steps: 767\n",
      "Saved evaluation results to eval/evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Get state and action dimensions from the environment\n",
    "state_size = env_eval.observation_space.shape[0]\n",
    "n_actions = env_eval.action_space.n\n",
    "\n",
    "print(f\"State size: {state_size}\")\n",
    "print(f\"Number of actions: {n_actions}\")\n",
    "\n",
    "# Create the agent with the correct dimensions\n",
    "agent = PPOAgent(state_size=state_size, n_actions=n_actions)\n",
    "\n",
    "# Evaluate the (untrained) agent\n",
    "# This will now run without errors\n",
    "df_results = evaluate_agent(agent, env_eval, num_episodes=20, render=True, csv_path=eval_folder / \"evaluation_results.csv\", renderer_logs_dir=eval_folder / \"render_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b634b64f",
   "metadata": {},
   "source": [
    "We observe that the PPO agent outperformed the market. Let's optimize the hyperparameters now. \n",
    "\n",
    "# Hyperparameter optimization with Optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cf6e3ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Optuna Objective Function ---\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Defines the objective for Optuna to optimize.\n",
    "    A \"trial\" consists of training and evaluating an agent with a specific\n",
    "    set of hyperparameters.\n",
    "    \"\"\"\n",
    "    # --- 1. Suggest Hyperparameters ---\n",
    "    # We define the search space for each hyperparameter.\n",
    "    ppo_hps = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True),\n",
    "        \"gamma\": trial.suggest_categorical(\"gamma\", [0.98, 0.99, 0.995, 0.999]),\n",
    "        \"gae_lambda\": trial.suggest_float(\"gae_lambda\", 0.8, 0.999),\n",
    "        \"entropy_beta\": trial.suggest_float(\"entropy_beta\", 1e-5, 0.1, log=True),\n",
    "        \"clip_epsilon\": trial.suggest_float(\"clip_epsilon\", 0.1, 0.3),\n",
    "        \"ppo_epochs\": trial.suggest_int(\"ppo_epochs\", 2, 20),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256, 512, 1024]),\n",
    "        \"hidden_size\": trial.suggest_categorical(\"hidden_size\", [64, 128, 256, 512]),\n",
    "        \"n_layers\": trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    }\n",
    "\n",
    "    # --- 2. Training Hyperparameters ---\n",
    "    TOTAL_TIMESTEPS = 200_000  # Reduced for faster trials, increase for better results\n",
    "    ROLLOUT_STEPS = 2048\n",
    "    \n",
    "    # --- 3. Initialize Agent ---\n",
    "    agent = PPOAgent(\n",
    "        state_size=state_size,\n",
    "        n_actions=n_actions,\n",
    "        **ppo_hps\n",
    "    )\n",
    "\n",
    "    # --- 4. Training Loop ---\n",
    "    obs, info = env_train.reset()\n",
    "    for step in range(1, TOTAL_TIMESTEPS + 1):\n",
    "        action, value, log_prob = agent.get_action_value_logprob(obs)\n",
    "        next_obs, reward, done, truncated, info = env_train.step(action)\n",
    "        agent.store(obs, action, reward, value, done, log_prob)\n",
    "        obs = next_obs\n",
    "\n",
    "        # Update if rollout buffer is full\n",
    "        if step % ROLLOUT_STEPS == 0:\n",
    "            next_value = 0.0\n",
    "            if not done:\n",
    "                with torch.no_grad():\n",
    "                    _, next_value_tensor = agent.network(torch.tensor(obs, dtype=torch.float32, device=agent.device).unsqueeze(0))\n",
    "                    next_value = next_value_tensor.item()\n",
    "            \n",
    "            agent.update(next_value)\n",
    "\n",
    "        if done or truncated:\n",
    "            obs, info = env_train.reset()\n",
    "\n",
    "    # --- 5. Evaluate the Agent ---\n",
    "    # Use a smaller number of episodes for faster evaluation during HPO\n",
    "    eval_results = evaluate_agent(agent, env_eval, num_episodes=20, render=False)\n",
    "    mean_portfolio_return = eval_results['portfolio_return'].mean()\n",
    "\n",
    "    # --- 6. Report result to Optuna ---\n",
    "    # Optuna will use this value to determine the best hyperparameters\n",
    "    return mean_portfolio_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28427cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 21:12:44,670] A new study created in memory with name: ppo_trading_agent_optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 79.51%   |   Portfolio Return : -34.77%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -2.87%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -24.98%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -15.24%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -7.73%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -8.14%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -2.08%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.01%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -14.00%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -1.54%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -9.84%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.64%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -9.75%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -7.04%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 15.33%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -5.79%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -2.72%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -4.02%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  4.89%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -7.71%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -4.38%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  1.83%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -0.00%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -0.01%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -0.01%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -0.00%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  0.00%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -0.01%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -0.00%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -0.01%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -0.01%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -0.01%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -0.01%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -0.00%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  0.00%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -0.00%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -0.00%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -0.00%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return :  0.00%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -0.00%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -0.01%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 21:22:52,177] Trial 0 finished with value: -4e-05 and parameters: {'lr': 5.6115164153345e-05, 'gamma': 0.98, 'gae_lambda': 0.8310429095469044, 'entropy_beta': 1.7073967431528103e-05, 'clip_epsilon': 0.27323522915498705, 'ppo_epochs': 13, 'batch_size': 128, 'hidden_size': 256, 'n_layers': 1}. Best is trial 0 with value: -4e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : -3.63%   |   Portfolio Return : -0.00%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -25.40%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -65.92%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -43.32%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -54.76%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -34.06%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -40.51%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -25.16%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -28.99%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -4.52%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  2.05%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -6.52%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  0.36%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  6.40%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  9.17%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 11.38%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.69%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return :  7.13%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 11.18%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.62%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 10.27%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 12.89%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 23.09%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.81%   |   \n",
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 21:31:40,322] Trial 1 finished with value: -0.018195 and parameters: {'lr': 0.00016738085788752134, 'gamma': 0.999, 'gae_lambda': 0.9562500163172097, 'entropy_beta': 6.290644294586152e-05, 'clip_epsilon': 0.20284688768272233, 'ppo_epochs': 13, 'batch_size': 1024, 'hidden_size': 64, 'n_layers': 2}. Best is trial 0 with value: -4e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : -3.63%   |   Portfolio Return : -1.82%   |   \n",
      "Saved evaluation results to evaluation_results.csv\n",
      "Market Return : 79.51%   |   Portfolio Return : -46.98%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -24.01%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -59.91%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -59.37%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -17.04%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -42.74%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -34.35%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -49.80%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -31.85%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -21.90%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -5.43%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -18.11%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 47.59%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 16.17%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : -24.73%   |   \n",
      "Market Return : 79.51%   |   Portfolio Return : 42.23%   |   \n"
     ]
    }
   ],
   "source": [
    "sampler = TPESampler(seed=42)\n",
    "pruner = MedianPruner()\n",
    "\n",
    "# Create the study\n",
    "study = optuna.create_study(\n",
    "    study_name=\"ppo_trading_agent_optimization\",\n",
    "    direction=\"maximize\",  # We want to maximize the portfolio return\n",
    "    sampler=sampler,\n",
    "    pruner=pruner\n",
    ")\n",
    "\n",
    "# Start the optimization\n",
    "# n_trials is the number of different hyperparameter combinations to test.\n",
    "# Increase this for a more thorough search.\n",
    "try:\n",
    "    study.optimize(objective, n_trials=10, timeout=1800) # 25 trials, 30min timeout\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Optimization stopped manually.\")\n",
    "\n",
    "# --- Print Results ---\n",
    "print(\"\\n--- Optimization Finished ---\")\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "\n",
    "print(\"\\nBest trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(f\"Value (Mean Portfolio Return): {best_trial.value:.4f}\")\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# You can now use these best hyperparameters to train your final agent\n",
    "# for a longer duration (e.g., more TOTAL_TIMESTEPS).\n",
    "best_hps = best_trial.params\n",
    "print(\"\\nBest hyperparameters dictionary:\")\n",
    "print(best_hps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c39e9",
   "metadata": {},
   "source": [
    "Now that we have found optimimal hyperparameters. We need to train the agent with these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba0713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 12\n",
      "Number of actions: 9\n",
      "Starting training for 10000 timesteps...\n",
      "Will update every 2048 steps.\n",
      "Evaluating every 5 updates.\n",
      "\n",
      "Update 1 (Step 2048/10000)\n",
      "  Actor Loss: -0.0045, Critic Loss: 0.0032\n",
      "\n",
      "Update 2 (Step 4096/10000)\n",
      "  Actor Loss: -0.0007, Critic Loss: 0.0007\n",
      "\n",
      "Update 3 (Step 6144/10000)\n",
      "  Actor Loss: -0.0016, Critic Loss: 0.0002\n",
      "\n",
      "Update 4 (Step 8192/10000)\n",
      "  Actor Loss: -0.0007, Critic Loss: 0.0001\n",
      "Market Return : 79.51%   |   Portfolio Return : -41.41%   |   \n",
      "\n",
      "Training finished.\n",
      "Best model saved to models/ppo_trading_agent.pth with return -inf%\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Get environment parameters ---\n",
    "# Use the training env for setting up the agent\n",
    "state_size = env_train.observation_space.shape[0]\n",
    "n_actions = env_train.action_space.n\n",
    "\n",
    "print(f\"State size: {state_size}\")\n",
    "print(f\"Number of actions: {n_actions}\")\n",
    "\n",
    "# --- 2. Training hyperparameters ---\n",
    "# You will need to TUNE these. These are small values for a quick test.\n",
    "TOTAL_TIMESTEPS = 1_000_000     # Total steps to train for\n",
    "ROLLOUT_STEPS = 2048         # Steps to collect before each PPO update\n",
    "EVAL_EVERY_N_UPDATES = 5     # How often to run evaluation\n",
    "MODEL_SAVE_PATH = \"models/ppo_trading_agent.pth\"\n",
    "\n",
    "\n",
    "\n",
    "# --- 3. Initialize agent with best hyperparameters ---\n",
    "agent = PPOAgent(state_size=state_size, n_actions=n_actions, **best_hps)\n",
    "\n",
    "# --- 4. Training & logging setup ---\n",
    "all_episode_rewards = [] # Stores total reward for each completed episode\n",
    "episode_rewards = []     # Stores rewards for the *current* episode\n",
    "best_eval_return = -float('inf') # Track best performance\n",
    "update_count = 0\n",
    "\n",
    "print(f\"Starting training for {TOTAL_TIMESTEPS} timesteps...\")\n",
    "print(f\"Will update every {ROLLOUT_STEPS} steps.\")\n",
    "print(f\"Evaluating every {EVAL_EVERY_N_UPDATES} updates.\")\n",
    "\n",
    "# --- 5. Main training loop ---\n",
    "obs, info = env_train.reset()\n",
    "\n",
    "for step in range(1, TOTAL_TIMESTEPS + 1):\n",
    "    # 5a. Get action, value, and log_prob from the agent\n",
    "    action, value, log_prob = agent.get_action_value_logprob(obs)\n",
    "    \n",
    "    # 5b. Take action in the environment\n",
    "    next_obs, reward, done, truncated, info = env_train.step(action)\n",
    "    \n",
    "    # 5c. Store the transition\n",
    "    # We store 'done' (terminal state like bankruptcy), not 'truncated'\n",
    "    agent.store(obs, action, reward, value, done, log_prob)\n",
    "    episode_rewards.append(reward)\n",
    "    \n",
    "    # 5d. Update the current observation\n",
    "    obs = next_obs\n",
    "    \n",
    "    # 5e. Check if rollout is complete (time to update)\n",
    "    if step % ROLLOUT_STEPS == 0:\n",
    "        update_count += 1\n",
    "        \n",
    "        # 5f. Get the value of the *last* observation for GAE\n",
    "        # This is the \"next_value\" for the last transition in the buffer.\n",
    "        # We get this value UNLESS the last step was a *terminal* 'done'.\n",
    "        # If it was 'truncated', we still bootstrap.\n",
    "        next_value = 0.0\n",
    "        if not done:\n",
    "            with torch.no_grad():\n",
    "                _, next_value_tensor = agent.network(torch.tensor(obs, dtype=torch.float32, device=agent.device).unsqueeze(0))\n",
    "                next_value = next_value_tensor.item()\n",
    "        \n",
    "        # 5g. Perform PPO update\n",
    "        losses = agent.update(next_value)\n",
    "        \n",
    "        # 5h. Log progress\n",
    "        print(f\"\\nUpdate {update_count} (Step {step}/{TOTAL_TIMESTEPS})\")\n",
    "        print(f\"  Actor Loss: {losses['actor_loss']:.4f}, Critic Loss: {losses['critic_loss']:.4f}\")\n",
    "        if len(all_episode_rewards) > 0:\n",
    "            print(f\"  Mean Reward (last 10 ep): {np.mean(all_episode_rewards[-10:]):.4f}\")\n",
    "        \n",
    "        # 5i. Periodic evaluation\n",
    "        if update_count % EVAL_EVERY_N_UPDATES == 0:\n",
    "            print(\"--- Running Evaluation ---\")\n",
    "            eval_results = evaluate_agent(agent, env_eval, num_episodes=20, render=False) # 5 episodes, no render\n",
    "            mean_eval_return = eval_results['portfolio_return'].mean()\n",
    "            market_return = eval_results['market_return'].mean() # Market return is constant\n",
    "            \n",
    "            print(f\"  Mean Eval Portfolio Return: {mean_eval_return:.2%}\")\n",
    "            print(f\"  Market Return: {market_return:.2%}\")\n",
    "            \n",
    "            if mean_eval_return > best_eval_return:\n",
    "                best_eval_return = mean_eval_return\n",
    "                torch.save(agent.network.state_dict(), MODEL_SAVE_PATH)\n",
    "                print(f\"  *** New best model saved with return {best_eval_return:.2%} ***\")\n",
    "            print(\"--------------------------\")\n",
    "            \n",
    "    # 5j. Handle episode end (if 'done' or 'truncated')\n",
    "    if done or truncated:\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "        episode_rewards = []\n",
    "        obs, info = env_train.reset()\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "print(f\"Best model saved to {MODEL_SAVE_PATH} with return {best_eval_return:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d24d0cd",
   "metadata": {},
   "source": [
    "# Test best model on real "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5efbf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ActorCriticNetwork:\n\tsize mismatch for shared.0.weight: copying a param with shape torch.Size([512, 12]) from checkpoint, the shape in current model is torch.Size([128, 12]).\n\tsize mismatch for shared.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for shared.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for shared.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for actor.weight: copying a param with shape torch.Size([7, 512]) from checkpoint, the shape in current model is torch.Size([7, 128]).\n\tsize mismatch for critic.weight: copying a param with shape torch.Size([1, 512]) from checkpoint, the shape in current model is torch.Size([1, 128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m trained_agent = PPOAgent(state_size=state_size, n_actions=n_actions)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load the saved model weights\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrained_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_SAVE_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Set the network to evaluation mode (e.g., for dropout, batchnorm)\u001b[39;00m\n\u001b[32m      8\u001b[39m trained_agent.network.eval() \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for ActorCriticNetwork:\n\tsize mismatch for shared.0.weight: copying a param with shape torch.Size([512, 12]) from checkpoint, the shape in current model is torch.Size([128, 12]).\n\tsize mismatch for shared.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for shared.2.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for shared.2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for actor.weight: copying a param with shape torch.Size([7, 512]) from checkpoint, the shape in current model is torch.Size([7, 128]).\n\tsize mismatch for critic.weight: copying a param with shape torch.Size([1, 512]) from checkpoint, the shape in current model is torch.Size([1, 128])."
     ]
    }
   ],
   "source": [
    "# Create a new agent instance\n",
    "trained_agent = PPOAgent(state_size=state_size, n_actions=n_actions)\n",
    "\n",
    "# Load the saved model weights\n",
    "trained_agent.network.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "# Set the network to evaluation mode (e.g., for dropout, batchnorm)\n",
    "trained_agent.network.eval() \n",
    "\n",
    "print(\"Evaluating trained agent...\")\n",
    "\n",
    "# Evaluate the trained agent\n",
    "df_results = evaluate_agent(\n",
    "    trained_agent, \n",
    "    env_eval, \n",
    "    num_episodes=20, \n",
    "    render=True, \n",
    "    csv_path=eval_folder / \"evaluation_results.csv\", \n",
    "    renderer_logs_dir=eval_folder / \"render_logs\"\n",
    ")\n",
    "\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe41adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>portfolio_return</th>\n",
       "      <th>market_return</th>\n",
       "      <th>excess_return</th>\n",
       "      <th>steps</th>\n",
       "      <th>total_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0362</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0725</td>\n",
       "      <td>767</td>\n",
       "      <td>0.035558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>767</td>\n",
       "      <td>0.129163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.1378</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>767</td>\n",
       "      <td>0.129138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0362</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0725</td>\n",
       "      <td>767</td>\n",
       "      <td>0.035558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>767</td>\n",
       "      <td>0.129163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>767</td>\n",
       "      <td>0.129188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0362</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.0725</td>\n",
       "      <td>767</td>\n",
       "      <td>0.035558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.1378</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>767</td>\n",
       "      <td>0.129138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.1363</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>767</td>\n",
       "      <td>0.127746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>767</td>\n",
       "      <td>0.129188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode  portfolio_return  market_return  excess_return  steps  \\\n",
       "0        1            0.0362        -0.0363         0.0725    767   \n",
       "1        2            0.1379        -0.0363         0.1742    767   \n",
       "2        3            0.1378        -0.0363         0.1741    767   \n",
       "3        4            0.0362        -0.0363         0.0725    767   \n",
       "4        5            0.1379        -0.0363         0.1742    767   \n",
       "5        6            0.1379        -0.0363         0.1742    767   \n",
       "6        7            0.0362        -0.0363         0.0725    767   \n",
       "7        8            0.1378        -0.0363         0.1741    767   \n",
       "8        9            0.1363        -0.0363         0.1726    767   \n",
       "9       10            0.1379        -0.0363         0.1742    767   \n",
       "\n",
       "   total_reward  \n",
       "0      0.035558  \n",
       "1      0.129163  \n",
       "2      0.129138  \n",
       "3      0.035558  \n",
       "4      0.129163  \n",
       "5      0.129188  \n",
       "6      0.035558  \n",
       "7      0.129138  \n",
       "8      0.127746  \n",
       "9      0.129188  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b7c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'gym_trading_env.renderer'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [16/Nov/2025 16:09:09] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Nov/2025 16:09:09] \"GET /update_data/BTCUSD_2025-11-16_15-47-18.pkl HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Nov/2025 16:09:09] \"GET /metrics HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Nov/2025 16:09:09] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [16/Nov/2025 16:09:25] \"GET /update_data/BTCUSD_2025-11-16_15-47-17.pkl HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Nov/2025 16:09:25] \"GET /metrics HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Nov/2025 16:09:35] \"GET /update_data/BTCUSD_2025-11-16_15-47-15.pkl HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Nov/2025 16:09:35] \"GET /metrics HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Nov/2025 16:09:37] \"GET /update_data/BTCUSD_2025-11-16_15-47-08.pkl HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Nov/2025 16:09:37] \"GET /metrics HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Nov/2025 16:09:39] \"GET /update_data/BTCUSD_2025-11-16_15-38-06.pkl HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Nov/2025 16:09:39] \"GET /metrics HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Nov/2025 16:09:43] \"GET /update_data/BTCUSD_2025-11-16_15-46-20.pkl HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Nov/2025 16:09:43] \"GET /metrics HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Nov/2025 16:09:46] \"GET /update_data/BTCUSD_2025-11-16_15-47-12.pkl HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Nov/2025 16:09:46] \"GET /metrics HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Nov/2025 16:09:49] \"GET /update_data/BTCUSD_2025-11-16_15-47-18.pkl HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Nov/2025 16:09:49] \"GET /metrics HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from gym_trading_env.renderer import Renderer\n",
    "renderer = Renderer(render_logs_dir=eval_folder/\"render_logs\")\n",
    "renderer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad1d52b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
